{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal TSMNet demo notebook for inter-session/-subject source-free (SF) offline and online unsupervised domain adaptation (UDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moabb\\pipelines\\__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"C:\\\\Users\\\\s.velut\\\\Documents\\\\These\\\\Protheus_PHD\\\\Scripts\")\n",
    "sys.path.insert(0,\"C:\\\\Users\\\\s.velut\\\\Documents\\\\These\\\\moabb\\\\moabb\\\\datasets\")\n",
    "sys.path.insert(0,\"C:\\\\Users\\\\s.velut\\\\Documents\\\\These\\\\Protheus_PHD\")\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from moabb.datasets.bnci import BNCI2015001, BNCI2014001\n",
    "from moabb.paradigms import MotorImagery, CVEP\n",
    "\n",
    "# from library.utils.torch import StratifiedDomainDataLoader\n",
    "from spdnets.utils.data import StratifiedDomainDataLoader, DomainDataset\n",
    "from spdnets.models import TSMNet, SPDSMNet, SPDSMNet2, DSBNSPDBNNet, SPDSMNet_visu\n",
    "import spdnets.batchnorm as bn\n",
    "import spdnets.functionals as fn\n",
    "\n",
    "from spdnets.trainer import Trainer\n",
    "from spdnets.callbacks import MomentumBatchNormScheduler, EarlyStopping\n",
    "\n",
    "\n",
    "from pyriemann.estimation import XdawnCovariances\n",
    "import matplotlib.pyplot as plt\n",
    "from Scripts.utils import prepare_data,get_BVEP_data,balance,get_y_pred\n",
    "from _utils import make_preds_accumul_aggresive, make_preds_pvalue\n",
    "from castillos2023 import CasitllosCVEP100,CasitllosCVEP40,CasitllosBurstVEP100,CasitllosBurstVEP40\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network and training configuration\n",
    "cfg_org = dict(\n",
    "    epochs = 20,\n",
    "    batch_size_train = 64,\n",
    "    domains_per_batch = 4,\n",
    "    validation_size = 0.2,\n",
    "    evaluation = 'inter-subject', # or 'inter-subject'\n",
    "    dtype = torch.float32,\n",
    "    spd_device='cpu',\n",
    "    # parameters for the TSMNet model\n",
    "    mdl_kwargs = dict(\n",
    "        temporal_filters=2,\n",
    "        nclasses = 2,\n",
    "        spatial_filters=32,\n",
    "        subspacedims=32, \n",
    "        bnorm_dispersion=bn.BatchNormDispersion.SCALAR,\n",
    "        spd_device='cpu',\n",
    "        spd_dtype=torch.double,\n",
    "        domain_adaptation=True\n",
    "    )\n",
    ")\n",
    "\n",
    "cfg_spd = dict(\n",
    "    epochs = 20,\n",
    "    batch_size_train = 64,\n",
    "    domains_per_batch = 4,\n",
    "    validation_size = 0.2,\n",
    "    evaluation = 'inter-subject', # or 'inter-subject'\n",
    "    dtype = torch.double,\n",
    "    spd_device='cuda',\n",
    "    # parameters for the TSMNet model\n",
    "    mdl_kwargs = dict(\n",
    "        temporal_filters=2,\n",
    "        nclasses = 2,\n",
    "        spatial_filters=32,\n",
    "        bimap_dims = [32,28,14,7],\n",
    "        subspacedims=32, \n",
    "        bnorm_dispersion=bn.BatchNormDispersion.SCALAR,\n",
    "        spd_device='cuda',\n",
    "        spd_dtype=torch.double,\n",
    "        domain_adaptation=True\n",
    "    )\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load a MOABB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing the first None classes from all possible events.\n"
     ]
    }
   ],
   "source": [
    "# moabb_ds = BNCI2015001()\n",
    "# n_classes = 2\n",
    "# moabb_paradigm = MotorImagery(n_classes=n_classes, events=['right_hand', 'feet'], fmin=4, fmax=36, tmin=0.0, tmax=4.0, resample=256)\n",
    "\n",
    "moabb_ds = CasitllosBurstVEP100()\n",
    "n_classes = 2\n",
    "moabb_paradigm = CVEP()\n",
    "\n",
    "# moabb_ds = BNCI2014001()\n",
    "# n_classes = 4\n",
    "# moabb_paradigm = MotorImagery(n_classes=n_classes, events=[\"left_hand\", \"right_hand\", \"feet\", \"tongue\"], fmin=4, fmax=36, tmin=0.5, tmax=3.496, resample=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SF offline UDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfuda_offline(dataset : DomainDataset, model : TSMNet, cfg):\n",
    "    model.eval()\n",
    "    model.domainadapt_finetune(dataset.features.to(dtype=cfg['dtype'], device=device), dataset.labels.to(device=device), dataset.domains, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SF online UDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sfuda_online_init_other_spd(domain_bn : torch.nn.Module, model : TSMNet, cfg):\n",
    "\n",
    "    # initialize new mean with the grand average of the other domains\n",
    "    M = []\n",
    "    s = []\n",
    "    for _, val in model.spdbnorm_layers[0].batchnorm.items():\n",
    "        M += [val.running_mean_test]\n",
    "        s += [val.running_var_test]\n",
    "    M = torch.cat(M, dim=0)\n",
    "    s = torch.cat(s, dim=0)\n",
    "    prev_mean = fn.spd_mean_kracher_flow(M, dim=0, return_dist=False)\n",
    "    prev_var = fn.spd_mean_kracher_flow(s[...,None], dim=0, return_dist=False).squeeze(-1)\n",
    "\n",
    "    # because we use precomputed means advance the observation number\n",
    "    # so that we start with slower adaptation\n",
    "    domain_bn.adapt_observation = 5 if cfg['evaluation'] == 'inter-subject' else 15\n",
    "    domain_bn.running_mean_test.data = prev_mean.data.clone()\n",
    "    domain_bn.running_var_test = prev_var.clone()\n",
    "\n",
    "\n",
    "def sfuda_online_init_spd(target_domains : torch.LongTensor, model : TSMNet, cfg, strategy : str = 'other'):\n",
    "\n",
    "    assert target_domains.ndim == 1\n",
    "    # assert isinstance(model.spdbnorm, bn.BaseDomainBatchNorm)\n",
    "    for target_domain in target_domains:\n",
    "        for i in range(len(model.bimap_dims[1:])):\n",
    "            domain_key = model.spdbnorm_layers[i].domain_to_key(target_domain)\n",
    "            # add domain if not yet in the model\n",
    "            if domain_key not in model.spdbnorm_layers[i].batchnorm:\n",
    "                bncls = model.spdbnorm_layers[i].domain_bn_cls\n",
    "                domain_bn = bncls(\n",
    "                    shape=model.spdbnorm_layers[i].mean.shape,\n",
    "                    batchdim=model.spdbnorm_layers[i].batchdim, \n",
    "                    learn_mean=model.spdbnorm_layers[i].learn_mean,\n",
    "                    learn_std=model.spdbnorm_layers[i].learn_std,\n",
    "                    dispersion=model.spdbnorm_layers[i].dispersion,\n",
    "                    mean=model.spdbnorm_layers[i].mean,\n",
    "                    std=model.spdbnorm_layers[i].std,\n",
    "                    eta=model.spdbnorm_layers[i].eta,\n",
    "                    eta_test=model.spdbnorm_layers[i].eta_test\n",
    "                )\n",
    "            else:\n",
    "                domain_bn = model.spdbnorm_layers[i].batchnorm[domain_key]\n",
    "\n",
    "        if strategy == 'other':\n",
    "            _sfuda_online_init_other_spd(domain_bn, model, cfg)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        # change the BN mode to perform online adaptation (for each batch)\n",
    "        domain_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.ADAPT)\n",
    "    \n",
    "    for target_domain in target_domains:    \n",
    "        for i in range(len(model.bimap_dims[1:])):\n",
    "            if domain_key not in model.spdbnorm_layers[i].batchnorm:\n",
    "                model.spdbnorm_layers[i].batchnorm[domain_key] = domain_bn\n",
    "\n",
    "def sfuda_online_step(inputs : torch.Tensor, domains : torch.LongTensor, model : TSMNet, cfg):\n",
    "\n",
    "    model = model.to(device='cpu')\n",
    "    # the model needs to be in evaluation mode so that the batch norm statistics for testing will be adapted\n",
    "    model.eval()\n",
    "\n",
    "    activations = model(inputs.to(dtype=cfg['dtype'],device=cfg['spd_device']), domains)\n",
    "    # return class probabilities\n",
    "    return activations\n",
    "\n",
    "\n",
    "def sfuda_online_simulate_spd(dataset : DomainDataset, model : TSMNet, cfg, loss_fn):\n",
    "\n",
    "    # do adaptation if the model is configured to do domain adaptation\n",
    "    if not model.domain_adaptation_:\n",
    "        test_loss, score  = None, None\n",
    "    else:\n",
    "        \n",
    "        # extract the target domains from the dataset\n",
    "        target_domains = dataset.domains.unique()\n",
    "\n",
    "        sfuda_online_init_spd(target_domains, model, cfg)\n",
    "        \n",
    "        y_true = []\n",
    "        y_hat = []\n",
    "\n",
    "        test_loss = 0.\n",
    "        # feed each observation through the network\n",
    "        # to adapt and infer the target\n",
    "        for i, (features, y) in enumerate(dataset):\n",
    "            \n",
    "            inputs = features['inputs'][None,...]\n",
    "            domains = features['domains'][None,...]\n",
    "            y = y\n",
    "\n",
    "            pred = sfuda_online_step(inputs, domains, model, cfg)\n",
    "            test_loss += loss_fn(pred, y[None,...]).item()\n",
    "            y_true.append(y[None,...])\n",
    "            y_hat.append(pred.argmax(1)[None,...])\n",
    "            # p_class = torch.nn.functional.softmax(pred, dim=-1)\n",
    "\n",
    "        # compute the overall score \n",
    "        score = balanced_accuracy_score(torch.cat(y_true).detach().cpu().numpy(), torch.cat(y_hat).detach().cpu().numpy())\n",
    "\n",
    "        test_loss /= len(dataset)\n",
    "\n",
    "        # stop adaptation for the target domains\n",
    "        for target_domain in target_domains:\n",
    "            for i in range(len(model.bimap_dims[1:])):\n",
    "                trgt_bn = model.spdbnorm_layers[i].batchnorm[str(target_domain.item())]\n",
    "                trgt_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.BUFFER)  \n",
    "\n",
    "    return test_loss, score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfuda_online(dataset : DomainDataset, model : TSMNet, cfg, loss_fn):\n",
    "\n",
    "    model = model.to(device='cpu')\n",
    "    # the model needs to be in evaluation mode so that the batch norm statistics for testing will be adapted\n",
    "    model.eval()\n",
    "\n",
    "    # do adaptation if the model is configured to do domain adaptation\n",
    "    if not model.domain_adaptation_:\n",
    "        test_loss, score  = None, None\n",
    "    else:\n",
    "        \n",
    "        # extract the target domains from the dataset\n",
    "        target_domains = dataset.domains.unique()\n",
    "        # initialize new mean with the grand average of the other domains\n",
    "        M = []\n",
    "        s = []\n",
    "        for key, val in model.spdbnorm.batchnorm.items():\n",
    "            if key in [f'dom {t}' for t in target_domains]:\n",
    "                continue\n",
    "            M += [val.running_mean_test]\n",
    "            s += [val.running_var_test]\n",
    "        M = torch.cat(M, dim=0)\n",
    "        s = torch.cat(s, dim=0)\n",
    "        prev_mean = fn.spd_mean_kracher_flow(M, dim=0, return_dist=False)\n",
    "        prev_var = fn.spd_mean_kracher_flow(s[...,None], dim=0, return_dist=False).squeeze(-1)\n",
    "        \n",
    "        # assign the grand average mean and var to the target domains\n",
    "        for target_domain in target_domains:\n",
    "\n",
    "            trgt_bn = model.spdbnorm.batchnorm[str(target_domain.item())]\n",
    "            # because we use precomputed means advance the observation number\n",
    "            # so that we start with slower adaptation\n",
    "            trgt_bn.adapt_observation = 5 if cfg['evaluation'] == 'inter-subject' else 15\n",
    "            trgt_bn.running_mean_test.data = prev_mean.data.clone()\n",
    "            trgt_bn.running_var_test = prev_var.clone()\n",
    "\n",
    "            # change the BN mode to perform online adaptation (for each batch)\n",
    "            trgt_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.ADAPT)\n",
    "        \n",
    "        y_true = []\n",
    "        y_hat = []\n",
    "\n",
    "        test_loss = 0.\n",
    "        # feed each observation through the network\n",
    "        # to adapt and infer the target\n",
    "        for i, (features, y) in enumerate(dataset):\n",
    "            \n",
    "            inputs = features['inputs'][None,...].to(dtype=cfg['dtype'])\n",
    "            domains = features['domains'][None,...]\n",
    "            y = y\n",
    "            pred = model.forward(inputs=inputs, domains=domains)\n",
    "            test_loss += loss_fn(pred, y[None,...]).item()\n",
    "            y_true.append(y[None,...])\n",
    "            y_hat.append(pred.argmax(1)[None,...])\n",
    "\n",
    "        # compute the overall score \n",
    "        score = balanced_accuracy_score(torch.cat(y_true).detach().cpu().numpy(), torch.cat(y_hat).detach().cpu().numpy())\n",
    "\n",
    "        test_loss /= len(dataset)\n",
    "\n",
    "        # stop adaptation for the target domains\n",
    "        for target_domain in target_domains:\n",
    "            trgt_bn = model.spdbnorm.batchnorm[str(target_domain.item())]\n",
    "            trgt_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.BUFFER)  \n",
    "\n",
    "    return test_loss, score \n",
    "\n",
    "def sfuda_online_spd(dataset : DomainDataset, model : TSMNet, cfg, loss_fn):\n",
    "\n",
    "    model = model.to(device='cpu')\n",
    "    # the model needs to be in evaluation mode so that the batch norm statistics for testing will be adapted\n",
    "    model.eval()\n",
    "\n",
    "    # do adaptation if the model is configured to do domain adaptation\n",
    "    if not model.domain_adaptation_:\n",
    "        test_loss, score  = None, None\n",
    "    else:\n",
    "        \n",
    "        # extract the target domains from the dataset\n",
    "        target_domains = dataset.domains.unique()\n",
    "        # initialize new mean with the grand average of the other domains\n",
    "        M = []\n",
    "        s = []\n",
    "        for key, val in model.spdbnorm_layers[0].batchnorm.items():\n",
    "            if key in [f'dom {t}' for t in target_domains]:\n",
    "                continue\n",
    "            M += [val.running_mean_test]\n",
    "            s += [val.running_var_test]\n",
    "        M = torch.cat(M, dim=0)\n",
    "        s = torch.cat(s, dim=0)\n",
    "        prev_mean = fn.spd_mean_kracher_flow(M, dim=0, return_dist=False)\n",
    "        prev_var = fn.spd_mean_kracher_flow(s[...,None], dim=0, return_dist=False).squeeze(-1)\n",
    "        \n",
    "        # assign the grand average mean and var to the target domains\n",
    "        for target_domain in target_domains:\n",
    "            trgt_bn = model.spdbnorm_layers[0].batchnorm[str(target_domain.item())]\n",
    "            # because we use precomputed means advance the observation number\n",
    "            # so that we start with slower adaptation\n",
    "            trgt_bn.adapt_observation = 5 if cfg['evaluation'] == 'inter-subject' else 15\n",
    "            trgt_bn.running_mean_test.data = prev_mean.data.clone()\n",
    "            trgt_bn.running_var_test = prev_var.clone()\n",
    "\n",
    "            # change the BN mode to perform online adaptation (for each batch)\n",
    "            trgt_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.ADAPT)\n",
    "        \n",
    "        y_true = []\n",
    "        y_hat = []\n",
    "\n",
    "        test_loss = 0.\n",
    "        # feed each observation through the network\n",
    "        # to adapt and infer the target\n",
    "        for i, (features, y) in enumerate(dataset):\n",
    "            \n",
    "            inputs = features['inputs'][None,...].to(dtype=cfg['dtype'],device=cfg['spd_device'])\n",
    "            domains = features['domains'][None,...]\n",
    "            y = y\n",
    "            pred = model.forward(inputs=inputs, domains=domains)\n",
    "            test_loss += loss_fn(pred, y[None,...]).item()\n",
    "            y_true.append(y[None,...])\n",
    "            y_hat.append(pred.argmax(1)[None,...])\n",
    "\n",
    "        # compute the overall score \n",
    "        score = balanced_accuracy_score(torch.cat(y_true).detach().cpu().numpy(), torch.cat(y_hat).detach().cpu().numpy())\n",
    "\n",
    "        test_loss /= len(dataset)\n",
    "\n",
    "        # stop adaptation for the target domains\n",
    "        for target_domain in target_domains:\n",
    "            trgt_bn = model.spdbnorm_layers[0].batchnorm[str(target_domain.item())]\n",
    "            trgt_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.BUFFER)  \n",
    "\n",
    "    return test_loss, score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sfuda_online_init_other(domain_bn : torch.nn.Module, model : TSMNet, cfg):\n",
    "\n",
    "    # initialize new mean with the grand average of the other domains\n",
    "    M = []\n",
    "    s = []\n",
    "    for _, val in model.spdbnorm.batchnorm.items():\n",
    "        M += [val.running_mean_test]\n",
    "        s += [val.running_var_test]\n",
    "    M = torch.cat(M, dim=0)\n",
    "    s = torch.cat(s, dim=0)\n",
    "    prev_mean = fn.spd_mean_kracher_flow(M, dim=0, return_dist=False)\n",
    "    prev_var = fn.spd_mean_kracher_flow(s[...,None], dim=0, return_dist=False).squeeze(-1)\n",
    "\n",
    "    # because we use precomputed means advance the observation number\n",
    "    # so that we start with slower adaptation\n",
    "    domain_bn.adapt_observation = 5 if cfg['evaluation'] == 'inter-subject' else 15\n",
    "    domain_bn.running_mean_test.data = prev_mean.data.clone()\n",
    "    domain_bn.running_var_test = prev_var.clone()\n",
    "\n",
    "\n",
    "def sfuda_online_init(target_domains : torch.LongTensor, model : TSMNet, cfg, strategy : str = 'other'):\n",
    "\n",
    "    assert target_domains.ndim == 1\n",
    "    assert isinstance(model.spdbnorm, bn.BaseDomainBatchNorm)\n",
    "    for target_domain in target_domains:\n",
    "        domain_key = model.spdbnorm.domain_to_key(target_domain)\n",
    "        # add domain if not yet in the model\n",
    "        if domain_key not in model.spdbnorm.batchnorm:\n",
    "            bncls = model.spdbnorm.domain_bn_cls\n",
    "            domain_bn = bncls(\n",
    "                shape=model.spdbnorm.mean.shape,\n",
    "                batchdim=model.spdbnorm.batchdim, \n",
    "                learn_mean=model.spdbnorm.learn_mean,\n",
    "                learn_std=model.spdbnorm.learn_std,\n",
    "                dispersion=model.spdbnorm.dispersion,\n",
    "                mean=model.spdbnorm.mean,\n",
    "                std=model.spdbnorm.std,\n",
    "                eta=model.spdbnorm.eta,\n",
    "                eta_test=model.spdbnorm.eta_test\n",
    "            )\n",
    "        else:\n",
    "            domain_bn = model.spdbnorm.batchnorm[domain_key]\n",
    "\n",
    "        if strategy == 'other':\n",
    "            _sfuda_online_init_other(domain_bn, model, cfg)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        # change the BN mode to perform online adaptation (for each batch)\n",
    "        domain_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.ADAPT)\n",
    "    \n",
    "    for target_domain in target_domains:    \n",
    "        if domain_key not in model.spdbnorm.batchnorm:\n",
    "            model.spdbnorm.batchnorm[domain_key] = domain_bn\n",
    "\n",
    "def sfuda_online_step(inputs : torch.Tensor, domains : torch.LongTensor, model : TSMNet, cfg):\n",
    "\n",
    "    model = model.to(device='cpu')\n",
    "    # the model needs to be in evaluation mode so that the batch norm statistics for testing will be adapted\n",
    "    model.eval()\n",
    "\n",
    "    activations = model(inputs.to(dtype=cfg['dtype']), domains)\n",
    "    # return class probabilities\n",
    "    return activations\n",
    "\n",
    "\n",
    "def sfuda_online_simulate(dataset : DomainDataset, model : TSMNet, cfg, loss_fn):\n",
    "\n",
    "    # do adaptation if the model is configured to do domain adaptation\n",
    "    if not model.domain_adaptation_:\n",
    "        test_loss, score  = None, None\n",
    "    else:\n",
    "        \n",
    "        # extract the target domains from the dataset\n",
    "        target_domains = dataset.domains.unique()\n",
    "\n",
    "        sfuda_online_init(target_domains, model, cfg)\n",
    "        \n",
    "        y_true = []\n",
    "        y_hat = []\n",
    "\n",
    "        test_loss = 0.\n",
    "        # feed each observation through the network\n",
    "        # to adapt and infer the target\n",
    "        for i, (features, y) in enumerate(dataset):\n",
    "            \n",
    "            inputs = features['inputs'][None,...]\n",
    "            domains = features['domains'][None,...]\n",
    "            y = y\n",
    "\n",
    "            pred = sfuda_online_step(inputs, domains, model, cfg)\n",
    "            test_loss += loss_fn(pred, y[None,...]).item()\n",
    "            y_true.append(y[None,...])\n",
    "            y_hat.append(pred.argmax(1)[None,...])\n",
    "            # p_class = torch.nn.functional.softmax(pred, dim=-1)\n",
    "\n",
    "        # compute the overall score \n",
    "        score = balanced_accuracy_score(torch.cat(y_true).detach().cpu().numpy(), torch.cat(y_hat).detach().cpu().numpy())\n",
    "\n",
    "        test_loss /= len(dataset)\n",
    "\n",
    "        # stop adaptation for the target domains\n",
    "        for target_domain in target_domains:\n",
    "            trgt_bn = model.spdbnorm.batchnorm[str(target_domain.item())]\n",
    "            trgt_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.BUFFER)  \n",
    "\n",
    "    return test_loss, score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit and evaluat the model for all domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib Qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing the first None classes from all possible events.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\Documents\\These\\moabb\\moabb\\datasets\\castillos2023.py:131: RuntimeWarning: Data file name in EEG.data (P13_burst100.fdt) is incorrect, the file name must have changed on disk, using the correct file name (P6_burst100.fdt).\n",
      "  raw = mne.io.read_raw_eeglab(file_path_list[0], preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "# subjects = [1,2,3]\n",
    "subjects = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "n_channels = 32\n",
    "on_frame = True\n",
    "if on_frame:\n",
    "    freq = 60\n",
    "else:\n",
    "    freq = 500\n",
    "\n",
    "raw_data,labels,codes,labels_codes = get_BVEP_data(subjects,on_frame)\n",
    "X_parent, Y_parent, domains_parent = prepare_data(subjects, raw_data, labels, on_frame,True,codes)\n",
    "metadata_parent = pd.DataFrame({\"subject\":np.repeat(list(map(str,subjects)),X_parent[0].shape[0]),\"session\":[\"0\"]*len(subjects)*X_parent[0].shape[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original TSMNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\Documents\\These\\moabb\\moabb\\datasets\\castillos2023.py:131: RuntimeWarning: Data file name in EEG.data (P13_burst100.fdt) is incorrect, the file name must have changed on disk, using the correct file name (P6_burst100.fdt).\n",
      "  raw = mne.io.read_raw_eeglab(file_path_list[0], preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "test domain=['1/0']\n",
      "epoch=  0 gd-step=  325 trn_loss= 0.6907 trn_score=0.5251 val_loss= 0.6931 val_score=0.5080 \n",
      "epoch= 10 gd-step= 3577 trn_loss= 0.6595 trn_score=0.6035 val_loss= 0.6970 val_score=0.5479 \n",
      "epoch= 19 gd-step= 6504 trn_loss= 0.6799 trn_score=0.6134 val_loss= 0.7419 val_score=0.5580 \n",
      "ES best epoch=2\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.25\n",
      "Perform UDA offline\n",
      "test domain=['2/0']\n",
      "epoch=  0 gd-step=  327 trn_loss= 0.6905 trn_score=0.5393 val_loss= 0.6925 val_score=0.5216 \n",
      "epoch= 10 gd-step= 3569 trn_loss= 0.6463 trn_score=0.6260 val_loss= 0.6880 val_score=0.5642 \n",
      "epoch= 19 gd-step= 6496 trn_loss= 0.6474 trn_score=0.6249 val_loss= 0.7108 val_score=0.5589 \n",
      "ES best epoch=10\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.25\n",
      "Perform UDA offline\n",
      "test domain=['3/0']\n",
      "epoch=  0 gd-step=  323 trn_loss= 0.6883 trn_score=0.5603 val_loss= 0.6902 val_score=0.5275 \n",
      "epoch= 10 gd-step= 3559 trn_loss= 0.6540 trn_score=0.6103 val_loss= 0.6934 val_score=0.5530 \n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "if 'inter-session' in cfg_org['evaluation']:\n",
    "    subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg_org['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=subjects, return_epochs=False)\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_org['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "\n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg_org['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg_org['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg_org['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg_org['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = TSMNet(**mdl_kwargs).to(device=device, dtype=cfg_org['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_org['epochs']-10,\n",
    "            bs0=cfg_org['batch_size_train'],\n",
    "            bs=cfg_org['batch_size_train']/cfg_org['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=cfg_org['epochs'],\n",
    "            min_epochs=cfg_org['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_org['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred = trainer.pred(net,dataloader=loader_test)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "\n",
    "        # extract model parameters\n",
    "        state_dict = deepcopy(net.state_dict())\n",
    "\n",
    "        # create new model and perform offline SF UDA\n",
    "        print(\"Perform UDA offline\")\n",
    "        sfuda_offline_net = TSMNet(**mdl_kwargs).to(device=device)\n",
    "        sfuda_offline_net.load_state_dict(state_dict)\n",
    "        sfuda_offline(ds_test, sfuda_offline_net, cfg_org)\n",
    "        res = trainer.test(sfuda_offline_net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(SFUDA)', domain=test_domain, **res))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = TSMNet(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online(ds_test, sfuda_online_net, cfg_org, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(onlineSFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = TSMNet(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online_simulate(ds_test, sfuda_online_net, cfg_org, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(online_sim_SFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "resdf = pd.DataFrame(records)    \n",
    "resdf.to_csv(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/TSMNet_code.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified TSMNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test domain=['1/0']\n",
      "epoch=  0 gd-step=  326 trn_loss= 0.6625 trn_score=0.6032 val_loss= 0.6650 val_score=0.5987 \n",
      "epoch= 10 gd-step= 3564 trn_loss= 0.5822 trn_score=0.6939 val_loss= 0.5883 val_score=0.6900 \n",
      "epoch= 19 gd-step= 6485 trn_loss= 0.5801 trn_score=0.6950 val_loss= 0.5842 val_score=0.6934 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.55\n",
      "Perform UDA offline\n",
      "test domain=['10/0']\n",
      "epoch=  0 gd-step=  325 trn_loss= 0.6639 trn_score=0.6122 val_loss= 0.6670 val_score=0.6036 \n",
      "epoch= 10 gd-step= 3559 trn_loss= 0.6235 trn_score=0.6567 val_loss= 0.6295 val_score=0.6468 \n",
      "epoch= 19 gd-step= 6463 trn_loss= 0.6223 trn_score=0.6554 val_loss= 0.6292 val_score=0.6491 \n",
      "ES best epoch=18\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.32\n",
      "Perform UDA offline\n",
      "test domain=['11/0']\n",
      "epoch=  0 gd-step=  323 trn_loss= 0.6764 trn_score=0.5828 val_loss= 0.6770 val_score=0.5777 \n",
      "epoch= 10 gd-step= 3559 trn_loss= 0.6032 trn_score=0.6771 val_loss= 0.6020 val_score=0.6756 \n",
      "epoch= 19 gd-step= 6466 trn_loss= 0.6016 trn_score=0.6773 val_loss= 0.6011 val_score=0.6756 \n",
      "ES best epoch=15\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.37\n",
      "Perform UDA offline\n",
      "test domain=['12/0']\n",
      "epoch=  0 gd-step=  324 trn_loss= 0.6678 trn_score=0.5927 val_loss= 0.6686 val_score=0.5979 \n",
      "epoch= 10 gd-step= 3570 trn_loss= 0.6265 trn_score=0.6455 val_loss= 0.6304 val_score=0.6403 \n",
      "epoch= 19 gd-step= 6480 trn_loss= 0.6255 trn_score=0.6468 val_loss= 0.6298 val_score=0.6453 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.22\n",
      "Perform UDA offline\n",
      "test domain=['2/0']\n",
      "epoch=  0 gd-step=  323 trn_loss= 0.6675 trn_score=0.5947 val_loss= 0.6700 val_score=0.5907 \n",
      "epoch= 10 gd-step= 3563 trn_loss= 0.6440 trn_score=0.6260 val_loss= 0.6513 val_score=0.6180 \n",
      "epoch= 19 gd-step= 6486 trn_loss= 0.6425 trn_score=0.6256 val_loss= 0.6509 val_score=0.6176 \n",
      "ES best epoch=17\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.3\n",
      "Perform UDA offline\n",
      "test domain=['3/0']\n",
      "epoch=  0 gd-step=  319 trn_loss= 0.6874 trn_score=0.5446 val_loss= 0.6866 val_score=0.5466 \n",
      "epoch= 10 gd-step= 3559 trn_loss= 0.6268 trn_score=0.6439 val_loss= 0.6316 val_score=0.6411 \n",
      "epoch= 19 gd-step= 6478 trn_loss= 0.6262 trn_score=0.6403 val_loss= 0.6296 val_score=0.6411 \n",
      "ES best epoch=18\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.22\n",
      "Perform UDA offline\n",
      "test domain=['4/0']\n",
      "epoch=  0 gd-step=  324 trn_loss= 0.6445 trn_score=0.6443 val_loss= 0.6466 val_score=0.6403 \n",
      "epoch= 10 gd-step= 3560 trn_loss= 0.5941 trn_score=0.6811 val_loss= 0.5914 val_score=0.6875 \n",
      "epoch= 19 gd-step= 6474 trn_loss= 0.5931 trn_score=0.6814 val_loss= 0.5903 val_score=0.6892 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.32\n",
      "Perform UDA offline\n",
      "test domain=['5/0']\n",
      "epoch=  0 gd-step=  327 trn_loss= 0.6684 trn_score=0.5968 val_loss= 0.6723 val_score=0.5845 \n",
      "epoch= 10 gd-step= 3558 trn_loss= 0.5925 trn_score=0.6861 val_loss= 0.5910 val_score=0.6886 \n",
      "epoch= 19 gd-step= 6468 trn_loss= 0.5890 trn_score=0.6864 val_loss= 0.5851 val_score=0.6926 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.25\n",
      "Perform UDA offline\n",
      "test domain=['6/0']\n",
      "epoch=  0 gd-step=  320 trn_loss= 0.6920 trn_score=0.5264 val_loss= 0.6906 val_score=0.5312 \n",
      "epoch= 10 gd-step= 3549 trn_loss= 0.6557 trn_score=0.6102 val_loss= 0.6571 val_score=0.6089 \n",
      "epoch= 19 gd-step= 6464 trn_loss= 0.6540 trn_score=0.6117 val_loss= 0.6560 val_score=0.6087 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.33\n",
      "Perform UDA offline\n",
      "test domain=['7/0']\n",
      "epoch=  0 gd-step=  327 trn_loss= 0.6512 trn_score=0.6292 val_loss= 0.6499 val_score=0.6398 \n",
      "epoch= 10 gd-step= 3583 trn_loss= 0.5877 trn_score=0.6886 val_loss= 0.5896 val_score=0.6905 \n",
      "epoch= 19 gd-step= 6495 trn_loss= 0.5871 trn_score=0.6895 val_loss= 0.5885 val_score=0.6900 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.23\n",
      "Perform UDA offline\n",
      "test domain=['8/0']\n",
      "epoch=  0 gd-step=  325 trn_loss= 0.6648 trn_score=0.5995 val_loss= 0.6683 val_score=0.5892 \n",
      "epoch= 10 gd-step= 3564 trn_loss= 0.6186 trn_score=0.6518 val_loss= 0.6302 val_score=0.6339 \n",
      "epoch= 19 gd-step= 6482 trn_loss= 0.6202 trn_score=0.6520 val_loss= 0.6300 val_score=0.6339 \n",
      "ES best epoch=13\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.23\n",
      "Perform UDA offline\n",
      "test domain=['9/0']\n",
      "epoch=  0 gd-step=  326 trn_loss= 0.6475 trn_score=0.6288 val_loss= 0.6440 val_score=0.6326 \n",
      "epoch= 10 gd-step= 3575 trn_loss= 0.6214 trn_score=0.6528 val_loss= 0.6157 val_score=0.6610 \n",
      "epoch= 19 gd-step= 6498 trn_loss= 0.6226 trn_score=0.6549 val_loss= 0.6155 val_score=0.6572 \n",
      "ES best epoch=13\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.23\n",
      "Perform UDA offline\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "if 'inter-session' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    # X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=[1,2,3], return_epochs=False)\n",
    "    # X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=subjects, return_epochs=False)\n",
    "    X = np.concatenate(X_parent)\n",
    "    labels = np.concatenate(Y_parent)\n",
    "    domains = np.concatenate(domains_parent)\n",
    "    metadata = metadata_parent.copy()\n",
    "\n",
    "    X_std = X.std(axis=0)\n",
    "    X /= X_std + 1e-8\n",
    "\n",
    "    # xdawncov = XdawnCovariances(estimator=\"lwf\",xdawn_estimator=\"lwf\",nfilter=8)\n",
    "    # X = xdawncov.fit_transform(X,labels)\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_spd['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "\n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg_spd['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg_spd['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg_spd['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg_spd['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = SPDSMNet(**mdl_kwargs).to(device=device, dtype=cfg_spd['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_spd['epochs']-10,\n",
    "            bs0=cfg_spd['batch_size_train'],\n",
    "            bs=cfg_spd['batch_size_train']/cfg_spd['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=cfg_spd['epochs'],\n",
    "            min_epochs=cfg_spd['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_spd['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred = trainer.pred(net,dataloader=loader_test)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "\n",
    "        # extract model parameters\n",
    "        state_dict = deepcopy(net.state_dict())\n",
    "\n",
    "        # create new model and perform offline SF UDA\n",
    "        print(\"Perform UDA offline\")\n",
    "        sfuda_offline_net = SPDSMNet(**mdl_kwargs).to(device=device)\n",
    "        sfuda_offline_net.load_state_dict(state_dict)\n",
    "        sfuda_offline(ds_test, sfuda_offline_net, cfg_spd)\n",
    "        res = trainer.test(sfuda_offline_net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(SFUDA)', domain=test_domain, **res))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        # sfuda_online_net = SPDSMNet(**mdl_kwargs)\n",
    "        # sfuda_online_net.load_state_dict(state_dict)\n",
    "        # loss, score = sfuda_online(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        # records.append(dict(mode='test(onlineSFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "        # # # create a new model and perform online SF UDA\n",
    "        # sfuda_online_net = SPDSMNet(**mdl_kwargs)\n",
    "        # sfuda_online_net.load_state_dict(state_dict)\n",
    "        # loss, score = sfuda_online_simulate(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        # records.append(dict(mode='test(online_sim_SFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "resdf = pd.DataFrame(records)    \n",
    "resdf.to_csv(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet_test/SPDSMNet_code.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified TSMNET2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test domain=['1/0']\n",
      "epoch=  0 gd-step=  326 trn_loss= 0.6477 trn_score=0.6318 val_loss= 0.6517 val_score=0.6229 \n",
      "epoch= 10 gd-step= 3574 trn_loss= 0.6114 trn_score=0.6700 val_loss= 0.6119 val_score=0.6661 \n",
      "epoch= 19 gd-step= 6487 trn_loss= 0.6068 trn_score=0.6766 val_loss= 0.6075 val_score=0.6737 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.73\n",
      "Perform UDA offline\n",
      "test domain=['10/0']\n",
      "epoch=  0 gd-step=  323 trn_loss= 0.6787 trn_score=0.5710 val_loss= 0.6812 val_score=0.5610 \n",
      "epoch= 10 gd-step= 3564 trn_loss= 0.6455 trn_score=0.6279 val_loss= 0.6393 val_score=0.6265 \n",
      "epoch= 19 gd-step= 6495 trn_loss= 0.6435 trn_score=0.6291 val_loss= 0.6377 val_score=0.6275 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.33\n",
      "Perform UDA offline\n",
      "test domain=['11/0']\n",
      "epoch=  0 gd-step=  326 trn_loss= 0.6604 trn_score=0.6170 val_loss= 0.6618 val_score=0.6091 \n",
      "epoch= 10 gd-step= 3571 trn_loss= 0.5994 trn_score=0.6797 val_loss= 0.6061 val_score=0.6758 \n",
      "epoch= 19 gd-step= 6496 trn_loss= 0.5986 trn_score=0.6829 val_loss= 0.6061 val_score=0.6741 \n",
      "ES best epoch=12\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.33\n",
      "Perform UDA offline\n",
      "test domain=['12/0']\n",
      "epoch=  0 gd-step=  323 trn_loss= 0.6336 trn_score=0.6513 val_loss= 0.6394 val_score=0.6449 \n",
      "epoch= 10 gd-step= 3559 trn_loss= 0.5285 trn_score=0.7414 val_loss= 0.5394 val_score=0.7330 \n",
      "epoch= 19 gd-step= 6466 trn_loss= 0.5248 trn_score=0.7451 val_loss= 0.5359 val_score=0.7360 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.25\n",
      "Perform UDA offline\n",
      "test domain=['2/0']\n",
      "epoch=  0 gd-step=  325 trn_loss= 0.6323 trn_score=0.6631 val_loss= 0.6327 val_score=0.6545 \n",
      "epoch= 10 gd-step= 3573 trn_loss= 0.5318 trn_score=0.7306 val_loss= 0.5372 val_score=0.7252 \n",
      "epoch= 19 gd-step= 6485 trn_loss= 0.5240 trn_score=0.7356 val_loss= 0.5321 val_score=0.7286 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.28\n",
      "Perform UDA offline\n",
      "test domain=['3/0']\n",
      "epoch=  0 gd-step=  327 trn_loss= 0.6899 trn_score=0.5408 val_loss= 0.6913 val_score=0.5362 \n",
      "epoch= 10 gd-step= 3560 trn_loss= 0.6221 trn_score=0.6457 val_loss= 0.6129 val_score=0.6532 \n",
      "epoch= 19 gd-step= 6479 trn_loss= 0.6201 trn_score=0.6505 val_loss= 0.6111 val_score=0.6540 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.23\n",
      "Perform UDA offline\n",
      "test domain=['4/0']\n",
      "epoch=  0 gd-step=  320 trn_loss= 0.6503 trn_score=0.6243 val_loss= 0.6492 val_score=0.6301 \n",
      "epoch= 10 gd-step= 3562 trn_loss= 0.5466 trn_score=0.7261 val_loss= 0.5469 val_score=0.7271 \n",
      "epoch= 19 gd-step= 6474 trn_loss= 0.5384 trn_score=0.7269 val_loss= 0.5386 val_score=0.7295 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.25\n",
      "Perform UDA offline\n",
      "test domain=['5/0']\n",
      "epoch=  0 gd-step=  320 trn_loss= 0.6759 trn_score=0.5708 val_loss= 0.6773 val_score=0.5655 \n",
      "epoch= 10 gd-step= 3577 trn_loss= 0.6540 trn_score=0.6096 val_loss= 0.6574 val_score=0.6070 \n",
      "epoch= 19 gd-step= 6497 trn_loss= 0.6537 trn_score=0.6137 val_loss= 0.6566 val_score=0.6051 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.28\n",
      "Perform UDA offline\n",
      "test domain=['6/0']\n",
      "epoch=  0 gd-step=  325 trn_loss= 0.6214 trn_score=0.6635 val_loss= 0.6188 val_score=0.6557 \n",
      "epoch= 10 gd-step= 3562 trn_loss= 0.5271 trn_score=0.7364 val_loss= 0.5259 val_score=0.7295 \n",
      "epoch= 19 gd-step= 6483 trn_loss= 0.5247 trn_score=0.7375 val_loss= 0.5208 val_score=0.7341 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.27\n",
      "Perform UDA offline\n",
      "test domain=['7/0']\n",
      "epoch=  0 gd-step=  322 trn_loss= 0.6552 trn_score=0.6158 val_loss= 0.6569 val_score=0.6131 \n",
      "epoch= 10 gd-step= 3561 trn_loss= 0.5958 trn_score=0.6755 val_loss= 0.5948 val_score=0.6765 \n",
      "epoch= 19 gd-step= 6484 trn_loss= 0.5923 trn_score=0.6797 val_loss= 0.5922 val_score=0.6816 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.22\n",
      "Perform UDA offline\n",
      "test domain=['8/0']\n",
      "epoch=  0 gd-step=  325 trn_loss= 0.5936 trn_score=0.6975 val_loss= 0.5958 val_score=0.6937 \n",
      "epoch= 10 gd-step= 3571 trn_loss= 0.5447 trn_score=0.7237 val_loss= 0.5450 val_score=0.7250 \n",
      "epoch= 19 gd-step= 6478 trn_loss= 0.5407 trn_score=0.7268 val_loss= 0.5429 val_score=0.7284 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.23\n",
      "Perform UDA offline\n",
      "test domain=['9/0']\n",
      "epoch=  0 gd-step=  326 trn_loss= 0.6372 trn_score=0.6533 val_loss= 0.6442 val_score=0.6458 \n",
      "epoch= 10 gd-step= 3569 trn_loss= 0.6053 trn_score=0.6785 val_loss= 0.6156 val_score=0.6727 \n",
      "epoch= 19 gd-step= 6485 trn_loss= 0.6044 trn_score=0.6801 val_loss= 0.6150 val_score=0.6742 \n",
      "ES best epoch=16\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.23\n",
      "Perform UDA offline\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "if 'inter-session' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    # X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=[1,2,3], return_epochs=False)\n",
    "    # X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=subjects, return_epochs=False)\n",
    "    X = np.concatenate(X_parent)\n",
    "    labels = np.concatenate(Y_parent)\n",
    "    domains = np.concatenate(domains_parent)\n",
    "    metadata = metadata_parent.copy()\n",
    "\n",
    "    X_std = X.std(axis=0)\n",
    "    X /= X_std + 1e-8\n",
    "\n",
    "    # xdawncov = XdawnCovariances(estimator=\"lwf\",xdawn_estimator=\"lwf\",nfilter=8)\n",
    "    # X = xdawncov.fit_transform(X,labels)\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_spd['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "\n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg_spd['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg_spd['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg_spd['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg_spd['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = SPDSMNet2(**mdl_kwargs).to(device=device, dtype=cfg_spd['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_spd['epochs']-10,\n",
    "            bs0=cfg_spd['batch_size_train'],\n",
    "            bs=cfg_spd['batch_size_train']/cfg_spd['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=cfg_spd['epochs'],\n",
    "            min_epochs=cfg_spd['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_spd['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred = trainer.pred(net,dataloader=loader_test)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "\n",
    "        # extract model parameters\n",
    "        state_dict = deepcopy(net.state_dict())\n",
    "\n",
    "        # create new model and perform offline SF UDA\n",
    "        print(\"Perform UDA offline\")\n",
    "        sfuda_offline_net = SPDSMNet2(**mdl_kwargs).to(device=device)\n",
    "        sfuda_offline_net.load_state_dict(state_dict)\n",
    "        sfuda_offline(ds_test, sfuda_offline_net, cfg_spd)\n",
    "        res = trainer.test(sfuda_offline_net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(SFUDA)', domain=test_domain, **res))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        # sfuda_online_net = SPDSMNet2(**mdl_kwargs)\n",
    "        # sfuda_online_net.load_state_dict(state_dict)\n",
    "        # loss, score = sfuda_online(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        # records.append(dict(mode='test(onlineSFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "        # # # create a new model and perform online SF UDA\n",
    "        # sfuda_online_net = SPDSMNet2(**mdl_kwargs)\n",
    "        # sfuda_online_net.load_state_dict(state_dict)\n",
    "        # loss, score = sfuda_online_simulate(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        # records.append(dict(mode='test(online_sim_SFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "resdf = pd.DataFrame(records)    \n",
    "resdf.to_csv(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet_test/SPDSMNet2_code.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSBNSDPBNNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test domain=['1/0']\n",
      "epoch=  0 gd-step=  324 trn_loss= 0.6895 trn_score=0.5462 val_loss= 0.6894 val_score=0.5449 \n",
      "epoch= 10 gd-step= 3578 trn_loss= 0.6559 trn_score=0.6079 val_loss= 0.6557 val_score=0.6076 \n",
      "epoch= 19 gd-step= 6494 trn_loss= 0.6534 trn_score=0.6117 val_loss= 0.6537 val_score=0.6148 \n",
      "ES best epoch=18\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.25\n",
      "test domain=['10/0']\n",
      "epoch=  0 gd-step=  327 trn_loss= 0.6830 trn_score=0.5864 val_loss= 0.6832 val_score=0.5903 \n",
      "epoch= 10 gd-step= 3572 trn_loss= 0.6293 trn_score=0.6390 val_loss= 0.6308 val_score=0.6434 \n",
      "epoch= 19 gd-step= 6486 trn_loss= 0.6338 trn_score=0.6372 val_loss= 0.6336 val_score=0.6371 \n",
      "ES best epoch=9\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.2\n",
      "test domain=['11/0']\n",
      "epoch=  0 gd-step=  324 trn_loss= 0.6846 trn_score=0.5709 val_loss= 0.6842 val_score=0.5617 \n",
      "epoch= 10 gd-step= 3570 trn_loss= 0.6645 trn_score=0.5923 val_loss= 0.6646 val_score=0.5968 \n",
      "epoch= 19 gd-step= 6497 trn_loss= 0.6647 trn_score=0.5935 val_loss= 0.6626 val_score=0.5953 \n",
      "ES best epoch=13\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.28\n",
      "test domain=['12/0']\n",
      "epoch=  0 gd-step=  326 trn_loss= 0.6852 trn_score=0.5797 val_loss= 0.6862 val_score=0.5689 \n",
      "epoch= 10 gd-step= 3559 trn_loss= 0.6450 trn_score=0.6269 val_loss= 0.6481 val_score=0.6152 \n",
      "epoch= 19 gd-step= 6468 trn_loss= 0.6425 trn_score=0.6313 val_loss= 0.6487 val_score=0.6284 \n",
      "ES best epoch=11\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.27\n",
      "test domain=['2/0']\n",
      "epoch=  0 gd-step=  324 trn_loss= 0.6901 trn_score=0.5422 val_loss= 0.6897 val_score=0.5538 \n",
      "epoch= 10 gd-step= 3579 trn_loss= 0.6374 trn_score=0.6346 val_loss= 0.6436 val_score=0.6348 \n",
      "epoch= 19 gd-step= 6505 trn_loss= 0.6347 trn_score=0.6418 val_loss= 0.6410 val_score=0.6394 \n",
      "ES best epoch=13\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.15\n",
      "test domain=['3/0']\n",
      "epoch=  0 gd-step=  321 trn_loss= 0.6753 trn_score=0.6090 val_loss= 0.6756 val_score=0.6055 \n",
      "epoch= 10 gd-step= 3564 trn_loss= 0.6297 trn_score=0.6390 val_loss= 0.6322 val_score=0.6388 \n",
      "epoch= 19 gd-step= 6485 trn_loss= 0.6309 trn_score=0.6406 val_loss= 0.6334 val_score=0.6422 \n",
      "ES best epoch=7\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.28\n",
      "test domain=['4/0']\n",
      "epoch=  0 gd-step=  320 trn_loss= 0.6884 trn_score=0.5522 val_loss= 0.6878 val_score=0.5572 \n",
      "epoch= 10 gd-step= 3570 trn_loss= 0.6452 trn_score=0.6224 val_loss= 0.6418 val_score=0.6305 \n",
      "epoch= 19 gd-step= 6483 trn_loss= 0.6440 trn_score=0.6312 val_loss= 0.6423 val_score=0.6307 \n",
      "ES best epoch=12\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.18\n",
      "test domain=['5/0']\n",
      "epoch=  0 gd-step=  324 trn_loss= 0.6746 trn_score=0.6172 val_loss= 0.6743 val_score=0.6218 \n",
      "epoch= 10 gd-step= 3561 trn_loss= 0.6257 trn_score=0.6507 val_loss= 0.6253 val_score=0.6576 \n",
      "epoch= 19 gd-step= 6470 trn_loss= 0.6256 trn_score=0.6530 val_loss= 0.6255 val_score=0.6598 \n",
      "ES best epoch=14\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.23\n",
      "test domain=['6/0']\n",
      "epoch=  0 gd-step=  322 trn_loss= 0.6762 trn_score=0.6006 val_loss= 0.6747 val_score=0.6076 \n",
      "epoch= 10 gd-step= 3564 trn_loss= 0.6342 trn_score=0.6353 val_loss= 0.6281 val_score=0.6428 \n",
      "epoch= 19 gd-step= 6479 trn_loss= 0.6397 trn_score=0.6322 val_loss= 0.6374 val_score=0.6299 \n",
      "ES best epoch=9\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.22\n",
      "test domain=['7/0']\n",
      "epoch=  0 gd-step=  326 trn_loss= 0.6676 trn_score=0.6237 val_loss= 0.6689 val_score=0.6152 \n",
      "epoch= 10 gd-step= 3560 trn_loss= 0.6229 trn_score=0.6509 val_loss= 0.6262 val_score=0.6477 \n",
      "epoch= 19 gd-step= 6474 trn_loss= 0.6223 trn_score=0.6495 val_loss= 0.6208 val_score=0.6553 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.18\n",
      "test domain=['8/0']\n",
      "epoch=  0 gd-step=  328 trn_loss= 0.6873 trn_score=0.5591 val_loss= 0.6869 val_score=0.5612 \n",
      "epoch= 10 gd-step= 3571 trn_loss= 0.6598 trn_score=0.6049 val_loss= 0.6605 val_score=0.6032 \n",
      "epoch= 19 gd-step= 6486 trn_loss= 0.6575 trn_score=0.6077 val_loss= 0.6609 val_score=0.6036 \n",
      "ES best epoch=9\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.23\n",
      "test domain=['9/0']\n",
      "epoch=  0 gd-step=  326 trn_loss= 0.6765 trn_score=0.6213 val_loss= 0.6761 val_score=0.6244 \n",
      "epoch= 10 gd-step= 3560 trn_loss= 0.5843 trn_score=0.6923 val_loss= 0.5907 val_score=0.6892 \n",
      "epoch= 19 gd-step= 6459 trn_loss= 0.5883 trn_score=0.6879 val_loss= 0.5937 val_score=0.6869 \n",
      "ES best epoch=11\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.23\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "if 'inter-session' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    # X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=subjects, return_epochs=False)\n",
    "    X = np.concatenate(X_parent)\n",
    "    labels = np.concatenate(Y_parent)\n",
    "    domains = np.concatenate(domains_parent)\n",
    "    metadata = metadata_parent.copy()\n",
    "\n",
    "    X_std = X.std(axis=0)\n",
    "    X /= X_std + 1e-8\n",
    "\n",
    "    # xdawncov = XdawnCovariances(estimator=\"lwf\",xdawn_estimator=\"lwf\",nfilter=8)\n",
    "    # X = xdawncov.fit_transform(X,labels)\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_spd['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "\n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg_spd['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg_spd['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg_spd['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg_spd['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = DSBNSPDBNNet(**mdl_kwargs).to(device=device, dtype=cfg_spd['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_spd['epochs']-10,\n",
    "            bs0=cfg_spd['batch_size_train'],\n",
    "            bs=cfg_spd['batch_size_train']/cfg_spd['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=cfg_spd['epochs'],\n",
    "            min_epochs=cfg_spd['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_spd['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred = trainer.pred(net,dataloader=loader_test)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "\n",
    "        # extract model parameters\n",
    "        state_dict = deepcopy(net.state_dict())\n",
    "\n",
    "        # # create new model and perform offline SF UDA\n",
    "        # print(\"Perform UDA offline\")\n",
    "        # sfuda_offline_net = SPDSMNet2(**mdl_kwargs).to(device=device)\n",
    "        # sfuda_offline_net.load_state_dict(state_dict)\n",
    "        # sfuda_offline(ds_test, sfuda_offline_net)\n",
    "        # res = trainer.test(sfuda_offline_net, dataloader=loader_test)\n",
    "        # records.append(dict(mode='test(SFUDA)', domain=test_domain, **res))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        # sfuda_online_net = SPDSMNet2(**mdl_kwargs)\n",
    "        # sfuda_online_net.load_state_dict(state_dict)\n",
    "        # loss, score = sfuda_online(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        # records.append(dict(mode='test(onlineSFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "        # # # create a new model and perform online SF UDA\n",
    "        # sfuda_online_net = SPDSMNet2(**mdl_kwargs)\n",
    "        # sfuda_online_net.load_state_dict(state_dict)\n",
    "        # loss, score = sfuda_online_simulate(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        # records.append(dict(mode='test(online_sim_SFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "resdf = pd.DataFrame(records)    \n",
    "resdf.to_csv(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet_test/DSBMSDPBNNet_code.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21060, 32, 32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified TSMNET_visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "if 'inter-session' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=subjects, return_epochs=False)\n",
    "\n",
    "    X_std = X.std(axis=0)\n",
    "    X /= X_std + 1e-8\n",
    "\n",
    "    xdawncov = XdawnCovariances(estimator=\"lwf\",xdawn_estimator=\"lwf\",nfilter=8)\n",
    "    X = xdawncov.fit_transform(X,labels)\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_spd['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "\n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg_spd['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg_spd['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg_spd['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg_spd['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = SPDSMNet_visu(**mdl_kwargs).to(device=device, dtype=cfg_spd['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_spd['epochs']-10,\n",
    "            bs0=cfg_spd['batch_size_train'],\n",
    "            bs=cfg_spd['batch_size_train']/cfg_spd['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = VisuTrainer(\n",
    "            max_epochs=cfg_spd['epochs'],\n",
    "            min_epochs=cfg_spd['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_spd['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred, inter_layer_out = trainer.pred(net,dataloader=loader_test)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "\n",
    "        # extract model parameters\n",
    "        state_dict = deepcopy(net.state_dict())\n",
    "\n",
    "        # create new model and perform offline SF UDA\n",
    "        print(\"Perform UDA offline\")\n",
    "        sfuda_offline_net = SPDSMNet2(**mdl_kwargs).to(device=device)\n",
    "        sfuda_offline_net.load_state_dict(state_dict)\n",
    "        sfuda_offline(ds_test, sfuda_offline_net)\n",
    "        res = trainer.test(sfuda_offline_net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(SFUDA)', domain=test_domain, **res))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = SPDSMNet2(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(onlineSFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "        # # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = SPDSMNet2(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online_simulate(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(online_sim_SFUDA)', domain=test_domain, loss=loss, score=score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original TSMNET on samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "subjects = [1,2,3]\n",
    "# subjects = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "n_channels = 32\n",
    "on_frame = False\n",
    "if on_frame:\n",
    "    freq = 60\n",
    "else:\n",
    "    freq = 500\n",
    "\n",
    "raw_data,labels,codes,labels_codes = get_BVEP_data(subjects,on_frame)\n",
    "X_parent, labels_parent, domains_parent = prepare_data(subjects,raw_data, labels, on_frame,False,codes)\n",
    "metadata = pd.DataFrame({\"subject\":np.repeat(list(map(str,subjects)),X_parent[0].shape[0]),\"session\":[\"0\"]*len(subjects)*X_parent[0].shape[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test domain=['1/0']\n",
      "epoch=  0 gd-step=  498 trn_loss= 0.6509 trn_score=0.6513 val_loss= 0.6569 val_score=0.6383 \n",
      "epoch= 10 gd-step= 5478 trn_loss= 0.4250 trn_score=0.8138 val_loss= 0.4748 val_score=0.7852 \n",
      "epoch= 19 gd-step= 9960 trn_loss= 0.4401 trn_score=0.7983 val_loss= 0.4919 val_score=0.7658 \n",
      "ES best epoch=18\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.49 GiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.15 GiB is allocated by PyTorch, and 32.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\minimal_tsmnet.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X45sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m res \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mtest(net, dataloader\u001b[39m=\u001b[39mloader_val)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X45sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m records\u001b[39m.\u001b[39mappend(\u001b[39mdict\u001b[39m(mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m, domain\u001b[39m=\u001b[39mtest_domain, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mres))\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X45sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m res \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtest(net, dataloader\u001b[39m=\u001b[39;49mloader_test)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X45sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m records\u001b[39m.\u001b[39mappend(\u001b[39mdict\u001b[39m(mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest(noUDA)\u001b[39m\u001b[39m'\u001b[39m, domain\u001b[39m=\u001b[39mtest_domain, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mres))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X45sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m y_pred \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mpred(net,dataloader\u001b[39m=\u001b[39mloader_test)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\trainer.py:106\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, model, dataloader)\u001b[0m\n\u001b[0;32m    104\u001b[0m features[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype_, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_)\n\u001b[0;32m    105\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_)\n\u001b[1;32m--> 106\u001b[0m pred \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfeatures)\n\u001b[0;32m    107\u001b[0m loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(pred, y)\u001b[39m.\u001b[39mitem()\n\u001b[0;32m    108\u001b[0m y_true\u001b[39m.\u001b[39mappend(y)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\models.py:87\u001b[0m, in \u001b[0;36mTSMNet.forward\u001b[1;34m(self, inputs, domains)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs, domains):\n\u001b[1;32m---> 87\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcnn(inputs[:,\u001b[39mNone\u001b[39;49;00m,\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m])\n\u001b[0;32m     88\u001b[0m     C \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_pooling(h)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspdnet[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mW)\n\u001b[0;32m     89\u001b[0m     l \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspdnet(C)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_conv_forward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, weight: Tensor, bias: Optional[Tensor]):\n\u001b[0;32m    452\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 453\u001b[0m         \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(F\u001b[39m.\u001b[39;49mpad(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_mode),\n\u001b[0;32m    454\u001b[0m                         weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    455\u001b[0m                         _pair(\u001b[39m0\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.49 GiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.15 GiB is allocated by PyTorch, and 32.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "if 'inter-session' in cfg_org['evaluation']:\n",
    "    subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg_org['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    # X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=subjects, return_epochs=False)\n",
    "    X = np.concatenate(X_parent)\n",
    "    labels = np.concatenate(labels_parent)\n",
    "    domains = np.concatenate(domains_parent)\n",
    "\n",
    "    X_std = X.std(axis=0)\n",
    "    X /= X_std + 1e-8\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_org['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "\n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg_org['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg_org['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg_org['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg_org['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = TSMNet(**mdl_kwargs).to(device=device, dtype=cfg_org['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_org['epochs']-10,\n",
    "            bs0=cfg_org['batch_size_train'],\n",
    "            bs=cfg_org['batch_size_train']/cfg_org['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=cfg_org['epochs'],\n",
    "            min_epochs=cfg_org['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_org['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred = trainer.pred(net,dataloader=loader_test)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "\n",
    "        # extract model parameters\n",
    "        state_dict = deepcopy(net.state_dict())\n",
    "\n",
    "        # create new model and perform offline SF UDA\n",
    "        print(\"Perform UDA offline\")\n",
    "        sfuda_offline_net = TSMNet(**mdl_kwargs).to(device=device)\n",
    "        sfuda_offline_net.load_state_dict(state_dict)\n",
    "        sfuda_offline(ds_test, sfuda_offline_net)\n",
    "        res = trainer.test(sfuda_offline_net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(SFUDA)', domain=test_domain, **res))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = TSMNet(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online(ds_test, sfuda_online_net, cfg_org, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(onlineSFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = TSMNet(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online_simulate(ds_test, sfuda_online_net, cfg_org, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(online_sim_SFUDA)', domain=test_domain, loss=loss, score=score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified TSMNET on samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "subjects = [1,2,3]\n",
    "# subjects = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "n_channels = 32\n",
    "on_frame = False\n",
    "if on_frame:\n",
    "    freq = 60\n",
    "else:\n",
    "    freq = 500\n",
    "\n",
    "raw_data,labels,codes,labels_codes = get_BVEP_data(subjects,on_frame)\n",
    "X_parent, labels_parent, domains_parent = prepare_data(subjects,raw_data, labels, on_frame,True,codes)\n",
    "metadata = pd.DataFrame({\"subject\":np.repeat(list(map(str,subjects)),X_parent[0].shape[0]),\"session\":[\"0\"]*len(subjects)*X_parent[0].shape[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175495</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175496</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175497</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175498</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175499</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175500 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject session\n",
       "0            1       0\n",
       "1            1       0\n",
       "2            1       0\n",
       "3            1       0\n",
       "4            1       0\n",
       "...        ...     ...\n",
       "175495       3       0\n",
       "175496       3       0\n",
       "175497       3       0\n",
       "175498       3       0\n",
       "175499       3       0\n",
       "\n",
       "[175500 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metadata = pd.DataFrame({\"subject\":np.repeat(list(map(str,subjects)),X[0].shape[0]),\"session\":[\"0\"]*len(subjects)*X[0].shape[0]})\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test domain=['1/0']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\minimal_tsmnet.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X30sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39m# fit the model\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X30sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest domain=\u001b[39m\u001b[39m{\u001b[39;00mtest_domain\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X30sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(net, train_dataloader\u001b[39m=\u001b[39;49mloader_train, val_dataloader\u001b[39m=\u001b[39;49mloader_val)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X30sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mES best epoch=\u001b[39m\u001b[39m{\u001b[39;00mes\u001b[39m.\u001b[39mbest_epoch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X30sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m fit_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(trainer\u001b[39m.\u001b[39mrecords)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\trainer.py:43\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloader, val_dataloader)\u001b[0m\n\u001b[0;32m     40\u001b[0m [callback\u001b[39m.\u001b[39mon_train_epoch_start(\u001b[39mself\u001b[39m, model) \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks]\n\u001b[0;32m     42\u001b[0m \u001b[39m# print(\"epochs training\")\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch(model, train_dataloader)\n\u001b[0;32m     45\u001b[0m trn_res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest(model, train_dataloader)\n\u001b[0;32m     46\u001b[0m trn_res \u001b[39m=\u001b[39m {\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrn_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m trn_res\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\trainer.py:84\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[1;34m(self, model, train_dataloader)\u001b[0m\n\u001b[0;32m     82\u001b[0m features[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype_, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_)\n\u001b[0;32m     83\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_)\n\u001b[1;32m---> 84\u001b[0m pred \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfeatures)\n\u001b[0;32m     85\u001b[0m \u001b[39m# print(\"Pred is :\", pred[0])\u001b[39;00m\n\u001b[0;32m     86\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(pred, y)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\models.py:206\u001b[0m, in \u001b[0;36mSPDSMNet.forward\u001b[1;34m(self, inputs, domains)\u001b[0m\n\u001b[0;32m    204\u001b[0m l \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbimap_layers[i](l)\n\u001b[0;32m    205\u001b[0m \u001b[39m# print(l.size())\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m l \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspdbnorm_layers[i](l,domains) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdomain_adaptation_ \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspdbnorm(l)\n\u001b[0;32m    207\u001b[0m \u001b[39m# print(l.size())\u001b[39;00m\n\u001b[0;32m    208\u001b[0m l \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mReEig_layers[i](l)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\batchnorm.py:83\u001b[0m, in \u001b[0;36mBaseDomainBatchNorm.forward\u001b[1;34m(self, X, d)\u001b[0m\n\u001b[0;32m     80\u001b[0m du \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39munique()\n\u001b[0;32m     82\u001b[0m X_normalized \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mempty_like(X)\n\u001b[1;32m---> 83\u001b[0m res \u001b[39m=\u001b[39m [(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_domain_(X[d\u001b[39m==\u001b[39;49mdomain], domain),torch\u001b[39m.\u001b[39;49mnonzero(d\u001b[39m==\u001b[39;49mdomain))\n\u001b[0;32m     84\u001b[0m         \u001b[39mfor\u001b[39;49;00m domain \u001b[39min\u001b[39;49;00m du]\n\u001b[0;32m     85\u001b[0m X_out, ixs \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mres)\n\u001b[0;32m     86\u001b[0m X_out, ixs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(X_out), torch\u001b[39m.\u001b[39mcat(ixs)\u001b[39m.\u001b[39mflatten()\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\batchnorm.py:83\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     80\u001b[0m du \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39munique()\n\u001b[0;32m     82\u001b[0m X_normalized \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mempty_like(X)\n\u001b[1;32m---> 83\u001b[0m res \u001b[39m=\u001b[39m [(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_domain_(X[d\u001b[39m==\u001b[39;49mdomain], domain),torch\u001b[39m.\u001b[39mnonzero(d\u001b[39m==\u001b[39mdomain))\n\u001b[0;32m     84\u001b[0m         \u001b[39mfor\u001b[39;00m domain \u001b[39min\u001b[39;00m du]\n\u001b[0;32m     85\u001b[0m X_out, ixs \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mres)\n\u001b[0;32m     86\u001b[0m X_out, ixs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(X_out), torch\u001b[39m.\u001b[39mcat(ixs)\u001b[39m.\u001b[39mflatten()\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\batchnorm.py:76\u001b[0m, in \u001b[0;36mBaseDomainBatchNorm.forward_domain_\u001b[1;34m(self, X, domain)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_domain_\u001b[39m(\u001b[39mself\u001b[39m, X, domain):\n\u001b[1;32m---> 76\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatchnorm[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain_to_key(domain)](X)\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\batchnorm.py:162\u001b[0m, in \u001b[0;36mSPDBatchNormImpl.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    159\u001b[0m         batch_mean \u001b[39m=\u001b[39m bm_sq \u001b[39m@\u001b[39m functionals\u001b[39m.\u001b[39msym_expm\u001b[39m.\u001b[39mapply(GT) \u001b[39m@\u001b[39m bm_sq\n\u001b[0;32m    161\u001b[0m \u001b[39m# update the running mean\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m rm \u001b[39m=\u001b[39m functionals\u001b[39m.\u001b[39;49mspd_2point_interpolation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean, batch_mean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meta)\n\u001b[0;32m    164\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispersion \u001b[39mis\u001b[39;00m BatchNormDispersion\u001b[39m.\u001b[39mSCALAR:\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mshape[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchdim] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\functionals.py:27\u001b[0m, in \u001b[0;36mspd_2point_interpolation\u001b[1;34m(A, B, t)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mspd_2point_interpolation\u001b[39m(A : Tensor, B : Tensor, t : Number) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m     26\u001b[0m     rm_sq, rm_invsq \u001b[39m=\u001b[39m sym_invsqrtm2\u001b[39m.\u001b[39mapply(A)\n\u001b[1;32m---> 27\u001b[0m     \u001b[39mreturn\u001b[39;00m rm_sq \u001b[39m@\u001b[39m sym_powm\u001b[39m.\u001b[39mapply(rm_invsq \u001b[39m@\u001b[39;49m B \u001b[39m@\u001b[39m rm_invsq, torch\u001b[39m.\u001b[39mtensor(t)) \u001b[39m@\u001b[39m rm_sq\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_bmm)"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "if 'inter-session' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    # X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=subjects, return_epochs=False)\n",
    "    X = np.concatenate(X_parent)\n",
    "    labels = np.concatenate(labels_parent)\n",
    "    domains = np.concatenate(domains_parent)\n",
    "\n",
    "    X_std = X.std(axis=0)\n",
    "    X /= X_std + 1e-8\n",
    "\n",
    "    # xdawncov = XdawnCovariances(estimator=\"lwf\",xdawn_estimator=\"lwf\",nfilter=8)\n",
    "    # X = xdawncov.fit_transform(X,labels)\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_spd['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "\n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg_spd['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg_spd['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg_spd['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg_spd['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = SPDSMNet(**mdl_kwargs).to(device=device, dtype=cfg_spd['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_spd['epochs']-10,\n",
    "            bs0=cfg_spd['batch_size_train'],\n",
    "            bs=cfg_spd['batch_size_train']/cfg_spd['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=cfg_spd['epochs'],\n",
    "            min_epochs=cfg_spd['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_spd['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred = trainer.pred(net,dataloader=loader_test)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "\n",
    "        # extract model parameters\n",
    "        state_dict = deepcopy(net.state_dict())\n",
    "\n",
    "        # create new model and perform offline SF UDA\n",
    "        print(\"Perform UDA offline\")\n",
    "        sfuda_offline_net = SPDSMNet(**mdl_kwargs).to(device=device)\n",
    "        sfuda_offline_net.load_state_dict(state_dict)\n",
    "        sfuda_offline(ds_test, sfuda_offline_net)\n",
    "        res = trainer.test(sfuda_offline_net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(SFUDA)', domain=test_domain, **res))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        # sfuda_online_net = SPDSMNet(**mdl_kwargs)\n",
    "        # sfuda_online_net.load_state_dict(state_dict)\n",
    "        # loss, score = sfuda_online(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        # records.append(dict(mode='test(onlineSFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "        # # create a new model and perform online SF UDA\n",
    "        # sfuda_online_net = SPDSMNet(**mdl_kwargs)\n",
    "        # sfuda_online_net.load_state_dict(state_dict)\n",
    "        # loss, score = sfuda_online_simulate(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        # records.append(dict(mode='test(online_sim_SFUDA)', domain=test_domain, loss=loss, score=score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original TSMNET train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing the first None classes from all possible events.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\Documents\\These\\moabb\\moabb\\datasets\\castillos2023.py:131: RuntimeWarning: Data file name in EEG.data (P13_burst100.fdt) is incorrect, the file name must have changed on disk, using the correct file name (P6_burst100.fdt).\n",
      "  raw = mne.io.read_raw_eeglab(file_path_list[0], preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "# subjects = [1,2,3]\n",
    "subjects = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "n_channels = 32\n",
    "on_frame = True\n",
    "if on_frame:\n",
    "    freq = 60\n",
    "else:\n",
    "    freq = 500\n",
    "\n",
    "raw_data,labels,codes,labels_codes = get_BVEP_data(subjects,on_frame)\n",
    "X_parent, labels_parent, domains_parent = prepare_data(subjects,raw_data, labels, on_frame,False,codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test domain=['1/0']\n",
      "epoch=  0 gd-step=   17 trn_loss= 0.6866 trn_score=0.5558 val_loss= 0.6869 val_score=0.5527 \n",
      "epoch= 10 gd-step=  187 trn_loss= 0.5855 trn_score=0.8646 val_loss= 0.5852 val_score=0.8661 \n",
      "epoch= 19 gd-step=  340 trn_loss= 0.4530 trn_score=0.9173 val_loss= 0.4537 val_score=0.9170 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.22\n",
      "test domain=['2/0']\n",
      "epoch=  0 gd-step=   17 trn_loss= 0.6868 trn_score=0.6206 val_loss= 0.6868 val_score=0.6241 \n",
      "epoch= 10 gd-step=  187 trn_loss= 0.6020 trn_score=0.8336 val_loss= 0.6028 val_score=0.8313 \n",
      "epoch= 19 gd-step=  340 trn_loss= 0.4903 trn_score=0.8796 val_loss= 0.4903 val_score=0.8795 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.28\n",
      "test domain=['3/0']\n",
      "epoch=  0 gd-step=   17 trn_loss= 0.6862 trn_score=0.6344 val_loss= 0.6863 val_score=0.6321 \n",
      "epoch= 10 gd-step=  187 trn_loss= 0.5969 trn_score=0.8125 val_loss= 0.5967 val_score=0.8152 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\minimal_tsmnet copy.ipynb Cell 36\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet%20copy.ipynb#X63sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39m# fit the model\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet%20copy.ipynb#X63sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest domain=\u001b[39m\u001b[39m{\u001b[39;00mtest_domain\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet%20copy.ipynb#X63sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(net, train_dataloader\u001b[39m=\u001b[39;49mloader_train, val_dataloader\u001b[39m=\u001b[39;49mloader_val)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet%20copy.ipynb#X63sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mES best epoch=\u001b[39m\u001b[39m{\u001b[39;00mes\u001b[39m.\u001b[39mbest_epoch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet%20copy.ipynb#X63sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m fit_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(trainer\u001b[39m.\u001b[39mrecords)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\trainer.py:48\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloader, val_dataloader)\u001b[0m\n\u001b[0;32m     45\u001b[0m trn_res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest(model, train_dataloader)\n\u001b[0;32m     46\u001b[0m trn_res \u001b[39m=\u001b[39m {\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrn_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m trn_res\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m---> 48\u001b[0m val_res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest(model, val_dataloader)\n\u001b[0;32m     49\u001b[0m val_res \u001b[39m=\u001b[39m {\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m val_res\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m     51\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_dict(trn_res)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\trainer.py:106\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, model, dataloader)\u001b[0m\n\u001b[0;32m    104\u001b[0m features[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype_, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_)\n\u001b[0;32m    105\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_)\n\u001b[1;32m--> 106\u001b[0m pred \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfeatures)\n\u001b[0;32m    107\u001b[0m loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(pred, y)\u001b[39m.\u001b[39mitem()\n\u001b[0;32m    108\u001b[0m y_true\u001b[39m.\u001b[39mappend(y)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\models.py:94\u001b[0m, in \u001b[0;36mTSMNet.forward\u001b[1;34m(self, inputs, domains)\u001b[0m\n\u001b[0;32m     92\u001b[0m C \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_pooling(h)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspdnet[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mW)\n\u001b[0;32m     93\u001b[0m l \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspdnet(C)\n\u001b[1;32m---> 94\u001b[0m l \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspdbnorm(l,domains) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdomain_adaptation_ \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspdbnorm(l)\n\u001b[0;32m     95\u001b[0m l \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogeig(l)\n\u001b[0;32m     96\u001b[0m l \u001b[39m=\u001b[39m l\u001b[39m.\u001b[39mto(inputs)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\batchnorm.py:83\u001b[0m, in \u001b[0;36mBaseDomainBatchNorm.forward\u001b[1;34m(self, X, d)\u001b[0m\n\u001b[0;32m     80\u001b[0m du \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39munique()\n\u001b[0;32m     82\u001b[0m X_normalized \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mempty_like(X)\n\u001b[1;32m---> 83\u001b[0m res \u001b[39m=\u001b[39m [(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_domain_(X[d\u001b[39m==\u001b[39;49mdomain], domain),torch\u001b[39m.\u001b[39;49mnonzero(d\u001b[39m==\u001b[39;49mdomain))\n\u001b[0;32m     84\u001b[0m         \u001b[39mfor\u001b[39;49;00m domain \u001b[39min\u001b[39;49;00m du]\n\u001b[0;32m     85\u001b[0m X_out, ixs \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mres)\n\u001b[0;32m     86\u001b[0m X_out, ixs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(X_out), torch\u001b[39m.\u001b[39mcat(ixs)\u001b[39m.\u001b[39mflatten()\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\batchnorm.py:83\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     80\u001b[0m du \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39munique()\n\u001b[0;32m     82\u001b[0m X_normalized \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mempty_like(X)\n\u001b[1;32m---> 83\u001b[0m res \u001b[39m=\u001b[39m [(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_domain_(X[d\u001b[39m==\u001b[39;49mdomain], domain),torch\u001b[39m.\u001b[39mnonzero(d\u001b[39m==\u001b[39mdomain))\n\u001b[0;32m     84\u001b[0m         \u001b[39mfor\u001b[39;00m domain \u001b[39min\u001b[39;00m du]\n\u001b[0;32m     85\u001b[0m X_out, ixs \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mres)\n\u001b[0;32m     86\u001b[0m X_out, ixs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(X_out), torch\u001b[39m.\u001b[39mcat(ixs)\u001b[39m.\u001b[39mflatten()\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\batchnorm.py:76\u001b[0m, in \u001b[0;36mBaseDomainBatchNorm.forward_domain_\u001b[1;34m(self, X, domain)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_domain_\u001b[39m(\u001b[39mself\u001b[39m, X, domain):\n\u001b[1;32m---> 76\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatchnorm[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain_to_key(domain)](X)\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\batchnorm.py:208\u001b[0m, in \u001b[0;36mSPDBatchNormImpl.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39m# rescale to desired dispersion\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispersion \u001b[39mis\u001b[39;00m BatchNormDispersion\u001b[39m.\u001b[39mSCALAR:\n\u001b[1;32m--> 208\u001b[0m     Xn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmanifold\u001b[39m.\u001b[39;49mtransp_identity_rescale_transp(X, \n\u001b[0;32m    209\u001b[0m         rm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd\u001b[39m/\u001b[39;49m(rv \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\u001b[39m.\u001b[39;49msqrt(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean)\n\u001b[0;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     Xn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanifold\u001b[39m.\u001b[39mtransp_via_identity(X, rm, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\manifolds.py:140\u001b[0m, in \u001b[0;36mSymmetricPositiveDefinite.transp_identity_rescale_transp\u001b[1;34m(self, X, A, s, B)\u001b[0m\n\u001b[0;32m    138\u001b[0m Ainvsq \u001b[39m=\u001b[39m functionals\u001b[39m.\u001b[39msym_invsqrtm\u001b[39m.\u001b[39mapply(A)\n\u001b[0;32m    139\u001b[0m Bsq \u001b[39m=\u001b[39m functionals\u001b[39m.\u001b[39msym_sqrtm\u001b[39m.\u001b[39mapply(B)\n\u001b[1;32m--> 140\u001b[0m \u001b[39mreturn\u001b[39;00m Bsq \u001b[39m@\u001b[39m functionals\u001b[39m.\u001b[39;49msym_powm\u001b[39m.\u001b[39;49mapply(Ainvsq \u001b[39m@\u001b[39;49m X \u001b[39m@\u001b[39;49m Ainvsq, s) \u001b[39m@\u001b[39m Bsq\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    537\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    538\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[0;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    547\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\functionals.py:264\u001b[0m, in \u001b[0;36msym_powm.forward\u001b[1;34m(ctx, M, exponent, ensure_symmetric)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx: Any, M: Tensor, exponent : Tensor, ensure_symmetric : \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 264\u001b[0m     X, s, smod, U \u001b[39m=\u001b[39m sym_modeig\u001b[39m.\u001b[39;49mforward(M, sym_powm\u001b[39m.\u001b[39;49mvalue, exponent, ensure_symmetric\u001b[39m=\u001b[39;49mensure_symmetric)\n\u001b[0;32m    265\u001b[0m     ctx\u001b[39m.\u001b[39msave_for_backward(s, smod, U, exponent)\n\u001b[0;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m X\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\functionals.py:58\u001b[0m, in \u001b[0;36msym_modeig.forward\u001b[1;34m(M, fun, fun_param, ensure_symmetric, ensure_psd)\u001b[0m\n\u001b[0;32m     55\u001b[0m     M \u001b[39m=\u001b[39m ensure_sym(M)\n\u001b[0;32m     57\u001b[0m \u001b[39m# compute the eigenvalues and vectors\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m s, U \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49meigh(M)\n\u001b[0;32m     59\u001b[0m \u001b[39mif\u001b[39;00m ensure_psd:\n\u001b[0;32m     60\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39mEPS[s\u001b[39m.\u001b[39mdtype])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "records = []\n",
    "n_cal = 7\n",
    "nb_frame = int(4*n_cal*(2.2-0.25)*60)\n",
    "nb_frame_train = int(4*(n_cal-1)*(2.2-0.25)*60)\n",
    "nb_frame_val = int(4*1*(2.2-0.25)*60)\n",
    "\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_sub, sub in enumerate(subjects):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    # X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=subjects, return_epochs=False)\n",
    "    X = X_parent[ix_sub]\n",
    "    labels = labels_parent[ix_sub]\n",
    "    domains = domains_parent[ix_sub]\n",
    "    metadata = pd.DataFrame({\"subject\":np.repeat(list(map(str,[sub])),X.shape[0]),\"session\":[\"0\"]*X.shape[0]})\n",
    "\n",
    "    train = np.arange(0,nb_frame_train,1)\n",
    "    val = np.arange(nb_frame_train,nb_frame,1)\n",
    "    test = np.arange(nb_frame, len(X),1)\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(domains)\n",
    "    \n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # Balancing the data\n",
    "    rus = RandomUnderSampler()\n",
    "    counter=np.array(range(0,len(y[train]))).reshape(-1,1)\n",
    "    index,_ = rus.fit_resample(counter,y[train][:])\n",
    "    index = np.sort(index,axis=0)\n",
    "    # X_train = np.squeeze(X[train][index,:,:], axis=1)\n",
    "    # y_train = np.squeeze(y[train][index])\n",
    "    # domain_train = np.squeeze(domain[train][index])\n",
    "    # metadata_train = metadata.loc[train].iloc[np.concatenate(index)]\n",
    "    train = np.squeeze(train[index], axis=1)\n",
    "\n",
    "    X_std = X[train].std(axis=0)\n",
    "    X[train] /= X_std + 1e-8\n",
    "    X_std = X[test].std(axis=0)\n",
    "    X[test] /= X_std + 1e-8\n",
    "\n",
    "    # adjust number of \n",
    "    du = np.unique(domains[train])\n",
    "    if cfg_org['domains_per_batch'] > len(du):\n",
    "        domains_per_batch = len(du)\n",
    "    else:\n",
    "        domains_per_batch = cfg_org['domains_per_batch']\n",
    "\n",
    "    \n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_org['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "\n",
    "    # split entire dataset into train/validation/test\n",
    "    ds_train = DomainDataset(X[train], y[train], domain[train], metadata.iloc[train])\n",
    "    ds_val = DomainDataset(X[val], y[val], domain[val], metadata.iloc[val])\n",
    "    ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "    # create dataloaders\n",
    "    # for training use specific loader/sampler so taht \n",
    "    # batches contain a specific number of domains with equal observations per domain\n",
    "    # and stratified labels\n",
    "    loader_train = StratifiedDomainDataLoader(ds_train, cfg_org['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "    loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "    loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "    # extract domains in the test dataset\n",
    "    test_domain = np.unique(domains[test])\n",
    "\n",
    "    # create the model\n",
    "    net = TSMNet(**mdl_kwargs).to(device=device, dtype=cfg_org['dtype'])\n",
    "\n",
    "    # create the momentum scheduler    \n",
    "    bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_org['epochs']-10,\n",
    "            bs0=cfg_org['batch_size_train'],\n",
    "            bs=cfg_org['batch_size_train']/cfg_org['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "    # create the trainer\n",
    "    trainer = Trainer(\n",
    "            max_epochs=cfg_org['epochs'],\n",
    "            min_epochs=cfg_org['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_org['dtype']\n",
    "    )\n",
    "\n",
    "    # fit the model\n",
    "    print(f\"test domain={test_domain}\")\n",
    "    trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "    print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "    fit_df = pd.DataFrame(trainer.records)\n",
    "    fit_df['fold'] = 1\n",
    "    fit_df['subset'] = 1\n",
    "    fit_records.append(fit_df)\n",
    "\n",
    "    # evaluation\n",
    "    print(\"evaluate the estimator\")\n",
    "    res = trainer.test(net, dataloader=loader_train)\n",
    "    records.append(dict(mode='train', domain=test_domain, **res))\n",
    "    res = trainer.test(net, dataloader=loader_test)\n",
    "    records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "    y_pred = trainer.pred(net,dataloader=loader_test)\n",
    "    labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "        y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "    )\n",
    "    accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_sub][4*n_cal:][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "    print(\" accuracy score of the participant\",accuracy_code)\n",
    "    records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "resdf = pd.DataFrame(records)\n",
    "resdf.to_csv(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet_test/SPDSMNet_traintest_code.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "        1, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[train][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3744,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0].cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\minimal_tsmnet.ipynb Cell 34\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# report the results\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m resdf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(records)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m resdf\u001b[39m.\u001b[39;49mgroupby([\u001b[39m'\u001b[39;49m\u001b[39mmode\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39magg([\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstd\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mround(\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:8402\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   8399\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to supply one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mby\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   8400\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8402\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   8403\u001b[0m     obj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   8404\u001b[0m     keys\u001b[39m=\u001b[39;49mby,\n\u001b[0;32m   8405\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   8406\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   8407\u001b[0m     as_index\u001b[39m=\u001b[39;49mas_index,\n\u001b[0;32m   8408\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m   8409\u001b[0m     group_keys\u001b[39m=\u001b[39;49mgroup_keys,\n\u001b[0;32m   8410\u001b[0m     squeeze\u001b[39m=\u001b[39;49msqueeze,\n\u001b[0;32m   8411\u001b[0m     observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[0;32m   8412\u001b[0m     dropna\u001b[39m=\u001b[39;49mdropna,\n\u001b[0;32m   8413\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:965\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[39mif\u001b[39;00m grouper \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgroupby\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgrouper\u001b[39;00m \u001b[39mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 965\u001b[0m     grouper, exclusions, obj \u001b[39m=\u001b[39m get_grouper(\n\u001b[0;32m    966\u001b[0m         obj,\n\u001b[0;32m    967\u001b[0m         keys,\n\u001b[0;32m    968\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    969\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m    970\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    971\u001b[0m         observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[0;32m    972\u001b[0m         mutated\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmutated,\n\u001b[0;32m    973\u001b[0m         dropna\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropna,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    976\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m obj\n\u001b[0;32m    977\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:888\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    886\u001b[0m         in_axis, level, gpr \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, gpr, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    889\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(gpr, Grouper) \u001b[39mand\u001b[39;00m gpr\u001b[39m.\u001b[39mkey \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[39m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     exclusions\u001b[39m.\u001b[39madd(gpr\u001b[39m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'mode'"
     ]
    }
   ],
   "source": [
    "# report the results\n",
    "resdf = pd.DataFrame(records)\n",
    "\n",
    "resdf.groupby(['mode']).agg(['mean', 'std']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf.to_csv(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/SPDSMNet_code.csv\")\n",
    "# resdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/SPDSMNet_code.csv\")\n",
    "print()\n",
    "df_score_code = np.array(df[\"score\"])[df[\"mode\"]==\"test(noUDA)_code\"]\n",
    "df_score = np.array(df[\"score\"])[df[\"mode\"]==\"test(noUDA)\"]\n",
    "\n",
    "np.save(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/SPDSMNet_score_code\",df_score_code)\n",
    "np.save(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/SPDSMNet_score\",df_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x23432e9ee10>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_df = pd.concat(fit_records)\n",
    "fit_df = fit_df.groupby(['subset', 'fold', 'epoch']).mean()\n",
    "fit_df.columns = fit_df.columns.str.split('_', expand=True)\n",
    "fit_df.columns.names = ['set', 'metric']\n",
    "fit_df = fit_df.stack('set').reset_index()\n",
    "\n",
    "\n",
    "sns.relplot(data=fit_df, x='epoch', y='loss', hue='set', col='subset', col_wrap=5, kind='line', n_boot=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
