{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal TSMNet demo notebook for inter-session/-subject source-free (SF) offline and online unsupervised domain adaptation (UDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moabb\\pipelines\\__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "from moabb.datasets.bnci import BNCI2015001, BNCI2014001\n",
    "from moabb.paradigms import MotorImagery, CVEP\n",
    "\n",
    "# from library.utils.torch import StratifiedDomainDataLoader\n",
    "from spdnets.utils.data import StratifiedDomainDataLoader, DomainDataset\n",
    "from spdnets.models import TSMNet\n",
    "import spdnets.batchnorm as bn\n",
    "import spdnets.functionals as fn\n",
    "\n",
    "from spdnets.trainer import Trainer\n",
    "from spdnets.callbacks import MomentumBatchNormScheduler, EarlyStopping\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0,\"C:\\\\Users\\\\s.velut\\\\Documents\\\\These\\\\Protheus_PHD\\\\Scripts\")\n",
    "sys.path.insert(0,\"C:\\\\Users\\\\s.velut\\\\Documents\\\\These\\\\Protheus_PHD\")\n",
    "from utils import prepare_data,get_BVEP_data,balance,get_y_pred\n",
    "from _utils import make_preds_accumul_aggresive, make_preds_pvalue\n",
    "sys.path.insert(0,\"C:\\\\Users\\\\s.velut\\\\Documents\\\\These\\\\moabb\\\\moabb\\\\datasets\")\n",
    "from castillos2023 import CasitllosCVEP100,CasitllosCVEP40,CasitllosBurstVEP100,CasitllosBurstVEP40\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network and training configuration\n",
    "cfg = dict(\n",
    "    epochs = 20,\n",
    "    batch_size_train = 64,\n",
    "    domains_per_batch = 4,\n",
    "    validation_size = 0.2,\n",
    "    evaluation = 'inter-subject', # or 'inter-subject'\n",
    "    dtype = torch.float32,\n",
    "    # parameters for the TSMNet model\n",
    "    mdl_kwargs = dict(\n",
    "        temporal_filters=2,\n",
    "        spatial_filters=30,\n",
    "        subspacedims=20, \n",
    "        bnorm_dispersion=bn.BatchNormDispersion.SCALAR,\n",
    "        spd_device='cpu',\n",
    "        spd_dtype=torch.double,\n",
    "        domain_adaptation=True\n",
    "    )\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load a MOABB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing the first None classes from all possible events.\n"
     ]
    }
   ],
   "source": [
    "# moabb_ds = BNCI2015001()\n",
    "# n_classes = 2\n",
    "# moabb_paradigm = MotorImagery(n_classes=n_classes, events=['right_hand', 'feet'], fmin=4, fmax=36, tmin=0.0, tmax=4.0, resample=256)\n",
    "\n",
    "moabb_ds = CasitllosBurstVEP100()\n",
    "n_classes = 2\n",
    "moabb_paradigm = CVEP()\n",
    "\n",
    "# moabb_ds = BNCI2014001()\n",
    "# n_classes = 4\n",
    "# moabb_paradigm = MotorImagery(n_classes=n_classes, events=[\"left_hand\", \"right_hand\", \"feet\", \"tongue\"], fmin=4, fmax=36, tmin=0.5, tmax=3.496, resample=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SF offline UDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfuda_offline(dataset : DomainDataset, model : TSMNet):\n",
    "    model.eval()\n",
    "    model.domainadapt_finetune(dataset.features.to(dtype=cfg['dtype'], device=device), dataset.labels.to(device=device), dataset.domains, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SF online UDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sfuda_online_init_other(domain_bn : torch.nn.Module, model : TSMNet, cfg):\n",
    "\n",
    "    # initialize new mean with the grand average of the other domains\n",
    "    M = []\n",
    "    s = []\n",
    "    for _, val in model.spdbnorm.batchnorm.items():\n",
    "        M += [val.running_mean_test]\n",
    "        s += [val.running_var_test]\n",
    "    M = torch.cat(M, dim=0)\n",
    "    s = torch.cat(s, dim=0)\n",
    "    prev_mean = fn.spd_mean_kracher_flow(M, dim=0, return_dist=False)\n",
    "    prev_var = fn.spd_mean_kracher_flow(s[...,None], dim=0, return_dist=False).squeeze(-1)\n",
    "\n",
    "    # because we use precomputed means advance the observation number\n",
    "    # so that we start with slower adaptation\n",
    "    domain_bn.adapt_observation = 5 if cfg['evaluation'] == 'inter-subject' else 15\n",
    "    domain_bn.running_mean_test.data = prev_mean.data.clone()\n",
    "    domain_bn.running_var_test = prev_var.clone()\n",
    "\n",
    "\n",
    "def sfuda_online_init(target_domains : torch.LongTensor, model : TSMNet, cfg, strategy : str = 'other'):\n",
    "\n",
    "    assert target_domains.ndim == 1\n",
    "    assert isinstance(model.spdbnorm, bn.BaseDomainBatchNorm)\n",
    "    for target_domain in target_domains:\n",
    "        domain_key = model.spdbnorm.domain_to_key(target_domain)\n",
    "        # add domain if not yet in the model\n",
    "        if domain_key not in model.spdbnorm.batchnorm:\n",
    "            bncls = model.spdbnorm.domain_bn_cls\n",
    "            domain_bn = bncls(\n",
    "                shape=model.spdbnorm.mean.shape,\n",
    "                batchdim=model.spdbnorm.batchdim, \n",
    "                learn_mean=model.spdbnorm.learn_mean,\n",
    "                learn_std=model.spdbnorm.learn_std,\n",
    "                dispersion=model.spdbnorm.dispersion,\n",
    "                mean=model.spdbnorm.mean,\n",
    "                std=model.spdbnorm.std,\n",
    "                eta=model.spdbnorm.eta,\n",
    "                eta_test=model.spdbnorm.eta_test\n",
    "            )\n",
    "        else:\n",
    "            domain_bn = model.spdbnorm.batchnorm[domain_key]\n",
    "\n",
    "        if strategy == 'other':\n",
    "            _sfuda_online_init_other(domain_bn, model, cfg)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        # change the BN mode to perform online adaptation (for each batch)\n",
    "        domain_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.ADAPT)\n",
    "    \n",
    "    for target_domain in target_domains:    \n",
    "        if domain_key not in model.spdbnorm.batchnorm:\n",
    "            model.spdbnorm.batchnorm[domain_key] = domain_bn\n",
    "\n",
    "def sfuda_online_step(inputs : torch.Tensor, domains : torch.LongTensor, model : TSMNet, cfg):\n",
    "\n",
    "    model = model.to(device='cpu')\n",
    "    # the model needs to be in evaluation mode so that the batch norm statistics for testing will be adapted\n",
    "    model.eval()\n",
    "\n",
    "    activations = model(inputs.to(dtype=cfg['dtype']), domains)\n",
    "    # return class probabilities\n",
    "    return activations\n",
    "\n",
    "\n",
    "def sfuda_online_simulate(dataset : DomainDataset, model : TSMNet, cfg, loss_fn):\n",
    "\n",
    "    # do adaptation if the model is configured to do domain adaptation\n",
    "    if not model.domain_adaptation_:\n",
    "        test_loss, score  = None, None\n",
    "    else:\n",
    "        \n",
    "        # extract the target domains from the dataset\n",
    "        target_domains = dataset.domains.unique()\n",
    "\n",
    "        sfuda_online_init(target_domains, model, cfg)\n",
    "        \n",
    "        y_true = []\n",
    "        y_hat = []\n",
    "\n",
    "        test_loss = 0.\n",
    "        # feed each observation through the network\n",
    "        # to adapt and infer the target\n",
    "        for i, (features, y) in enumerate(dataset):\n",
    "            \n",
    "            inputs = features['inputs'][None,...]\n",
    "            domains = features['domains'][None,...]\n",
    "            y = y\n",
    "\n",
    "            pred = sfuda_online_step(inputs, domains, model, cfg)\n",
    "            test_loss += loss_fn(pred, y[None,...]).item()\n",
    "            y_true.append(y[None,...])\n",
    "            y_hat.append(pred.argmax(1)[None,...])\n",
    "            # p_class = torch.nn.functional.softmax(pred, dim=-1)\n",
    "\n",
    "        # compute the overall score \n",
    "        score = balanced_accuracy_score(torch.cat(y_true).detach().cpu().numpy(), torch.cat(y_hat).detach().cpu().numpy())\n",
    "\n",
    "        test_loss /= len(dataset)\n",
    "\n",
    "        # stop adaptation for the target domains\n",
    "        for target_domain in target_domains:\n",
    "            trgt_bn = model.spdbnorm.batchnorm[str(target_domain.item())]\n",
    "            trgt_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.BUFFER)  \n",
    "\n",
    "    return test_loss, score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfuda_online(dataset : DomainDataset, model : TSMNet, cfg, loss_fn):\n",
    "\n",
    "    model = model.to(device='cpu')\n",
    "    # the model needs to be in evaluation mode so that the batch norm statistics for testing will be adapted\n",
    "    model.eval()\n",
    "\n",
    "    # do adaptation if the model is configured to do domain adaptation\n",
    "    if not model.domain_adaptation_:\n",
    "        test_loss, score  = None, None\n",
    "    else:\n",
    "        \n",
    "        # extract the target domains from the dataset\n",
    "        target_domains = dataset.domains.unique()\n",
    "        # initialize new mean with the grand average of the other domains\n",
    "        M = []\n",
    "        s = []\n",
    "        for key, val in model.spdbnorm.batchnorm.items():\n",
    "            if key in [f'dom {t}' for t in target_domains]:\n",
    "                continue\n",
    "            M += [val.running_mean_test]\n",
    "            s += [val.running_var_test]\n",
    "        M = torch.cat(M, dim=0)\n",
    "        s = torch.cat(s, dim=0)\n",
    "        prev_mean = fn.spd_mean_kracher_flow(M, dim=0, return_dist=False)\n",
    "        prev_var = fn.spd_mean_kracher_flow(s[...,None], dim=0, return_dist=False).squeeze(-1)\n",
    "        \n",
    "        # assign the grand average mean and var to the target domains\n",
    "        for target_domain in target_domains:\n",
    "\n",
    "            trgt_bn = model.spdbnorm.batchnorm[str(target_domain.item())]\n",
    "            # because we use precomputed means advance the observation number\n",
    "            # so that we start with slower adaptation\n",
    "            trgt_bn.adapt_observation = 5 if cfg['evaluation'] == 'inter-subject' else 15\n",
    "            trgt_bn.running_mean_test.data = prev_mean.data.clone()\n",
    "            trgt_bn.running_var_test = prev_var.clone()\n",
    "\n",
    "            # change the BN mode to perform online adaptation (for each batch)\n",
    "            trgt_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.ADAPT)\n",
    "        \n",
    "        y_true = []\n",
    "        y_hat = []\n",
    "\n",
    "        test_loss = 0.\n",
    "        # feed each observation through the network\n",
    "        # to adapt and infer the target\n",
    "        for i, (features, y) in enumerate(dataset):\n",
    "            \n",
    "            inputs = features['inputs'][None,...].to(dtype=cfg['dtype'])\n",
    "            domains = features['domains'][None,...]\n",
    "            y = y\n",
    "            pred = model.forward(inputs=inputs, domains=domains)\n",
    "            test_loss += loss_fn(pred, y[None,...]).item()\n",
    "            y_true.append(y[None,...])\n",
    "            y_hat.append(pred.argmax(1)[None,...])\n",
    "\n",
    "        # compute the overall score \n",
    "        score = balanced_accuracy_score(torch.cat(y_true).detach().cpu().numpy(), torch.cat(y_hat).detach().cpu().numpy())\n",
    "\n",
    "        test_loss /= len(dataset)\n",
    "\n",
    "        # stop adaptation for the target domains\n",
    "        for target_domain in target_domains:\n",
    "            trgt_bn = model.spdbnorm.batchnorm[str(target_domain.item())]\n",
    "            trgt_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.BUFFER)  \n",
    "\n",
    "    return test_loss, score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit and evaluat the model for all domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\minimal_tsmnet.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X, labels, metadata \u001b[39m=\u001b[39m moabb_paradigm\u001b[39m.\u001b[39;49mget_data(moabb_ds, subjects\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m], return_epochs\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moabb\\paradigms\\base.py:278\u001b[0m, in \u001b[0;36mBaseProcessing.get_data\u001b[1;34m(self, dataset, subjects, return_epochs, return_raws, cache_config, postprocess_pipeline)\u001b[0m\n\u001b[0;32m    273\u001b[0m process_pipelines \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_process_pipelines(\n\u001b[0;32m    274\u001b[0m     dataset, return_epochs, return_raws, postprocess_pipeline\n\u001b[0;32m    275\u001b[0m )\n\u001b[0;32m    276\u001b[0m labels_pipeline \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_labels_pipeline(dataset, return_epochs, return_raws)\n\u001b[1;32m--> 278\u001b[0m data \u001b[39m=\u001b[39m [\n\u001b[0;32m    279\u001b[0m     dataset\u001b[39m.\u001b[39;49mget_data(\n\u001b[0;32m    280\u001b[0m         subjects\u001b[39m=\u001b[39;49msubjects,\n\u001b[0;32m    281\u001b[0m         cache_config\u001b[39m=\u001b[39;49mcache_config,\n\u001b[0;32m    282\u001b[0m         process_pipeline\u001b[39m=\u001b[39;49mprocess_pipeline,\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    284\u001b[0m     \u001b[39mfor\u001b[39;49;00m process_pipeline \u001b[39min\u001b[39;49;00m process_pipelines\n\u001b[0;32m    285\u001b[0m ]\n\u001b[0;32m    287\u001b[0m X \u001b[39m=\u001b[39m []\n\u001b[0;32m    288\u001b[0m labels \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moabb\\paradigms\\base.py:279\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    273\u001b[0m process_pipelines \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_process_pipelines(\n\u001b[0;32m    274\u001b[0m     dataset, return_epochs, return_raws, postprocess_pipeline\n\u001b[0;32m    275\u001b[0m )\n\u001b[0;32m    276\u001b[0m labels_pipeline \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_labels_pipeline(dataset, return_epochs, return_raws)\n\u001b[0;32m    278\u001b[0m data \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 279\u001b[0m     dataset\u001b[39m.\u001b[39;49mget_data(\n\u001b[0;32m    280\u001b[0m         subjects\u001b[39m=\u001b[39;49msubjects,\n\u001b[0;32m    281\u001b[0m         cache_config\u001b[39m=\u001b[39;49mcache_config,\n\u001b[0;32m    282\u001b[0m         process_pipeline\u001b[39m=\u001b[39;49mprocess_pipeline,\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    284\u001b[0m     \u001b[39mfor\u001b[39;00m process_pipeline \u001b[39min\u001b[39;00m process_pipelines\n\u001b[0;32m    285\u001b[0m ]\n\u001b[0;32m    287\u001b[0m X \u001b[39m=\u001b[39m []\n\u001b[0;32m    288\u001b[0m labels \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moabb\\datasets\\base.py:342\u001b[0m, in \u001b[0;36mBaseDataset.get_data\u001b[1;34m(self, subjects, cache_config, process_pipeline)\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[39mif\u001b[39;00m subject \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubject_list:\n\u001b[0;32m    341\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid subject \u001b[39m\u001b[39m{:d}\u001b[39;00m\u001b[39m given\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(subject))\n\u001b[1;32m--> 342\u001b[0m     data[subject] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_single_subject_data_using_cache(\n\u001b[0;32m    343\u001b[0m         subject,\n\u001b[0;32m    344\u001b[0m         cache_config,\n\u001b[0;32m    345\u001b[0m         process_pipeline,\n\u001b[0;32m    346\u001b[0m     )\n\u001b[0;32m    347\u001b[0m check_subject_names(data)\n\u001b[0;32m    348\u001b[0m check_session_names(data)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moabb\\datasets\\base.py:436\u001b[0m, in \u001b[0;36mBaseDataset._get_single_subject_data_using_cache\u001b[1;34m(self, subject, cache_config, process_pipeline)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[39m# Load and eventually overwrite:\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(cached_steps) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# last option: we don't use cache\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m     sessions_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_single_subject_data(subject)\n\u001b[0;32m    437\u001b[0m     \u001b[39massert\u001b[39;00m sessions_data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# should not happen\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Documents\\These\\moabb\\moabb\\datasets\\castillos2023.py:130\u001b[0m, in \u001b[0;36mBaseCastillos2023._get_single_subject_data\u001b[1;34m(self, subject)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_single_subject_data\u001b[39m(\u001b[39mself\u001b[39m, subject):\n\u001b[0;32m    129\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the data of a single subject.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     file_path_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_path(subject, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparadigm_type)\n\u001b[0;32m    131\u001b[0m     raw \u001b[39m=\u001b[39m mne\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mread_raw_eeglab(file_path_list[\u001b[39m0\u001b[39m], preload\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    133\u001b[0m     \u001b[39m# Strip the annotations that were script to make them easier to process\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\These\\moabb\\moabb\\datasets\\castillos2023.py:238\u001b[0m, in \u001b[0;36mBaseCastillos2023.data_path\u001b[1;34m(self, subject, paradigm_type, path, force_update, update_path, verbose)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (osp\u001b[39m.\u001b[39misdir(path_folder \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m4Class-VEP\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m    237\u001b[0m     zip_ref \u001b[39m=\u001b[39m z\u001b[39m.\u001b[39mZipFile(path_zip, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 238\u001b[0m     zip_ref\u001b[39m.\u001b[39;49mextractall(path_folder)\n\u001b[0;32m    240\u001b[0m subject_paths\u001b[39m.\u001b[39mappend(\n\u001b[0;32m    241\u001b[0m     path_folder\n\u001b[0;32m    242\u001b[0m     \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m4Class-CVEP/P\u001b[39m\u001b[39m{:d}\u001b[39;00m\u001b[39m/P\u001b[39m\u001b[39m{:d}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{:s}\u001b[39;00m\u001b[39m.set\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(subject, subject, paradigm_type)\n\u001b[0;32m    243\u001b[0m )\n\u001b[0;32m    245\u001b[0m \u001b[39mreturn\u001b[39;00m subject_paths\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1681\u001b[0m, in \u001b[0;36mZipFile.extractall\u001b[1;34m(self, path, members, pwd)\u001b[0m\n\u001b[0;32m   1678\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(path)\n\u001b[0;32m   1680\u001b[0m \u001b[39mfor\u001b[39;00m zipinfo \u001b[39min\u001b[39;00m members:\n\u001b[1;32m-> 1681\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_member(zipinfo, path, pwd)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1736\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[1;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[0;32m   1732\u001b[0m     \u001b[39mreturn\u001b[39;00m targetpath\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen(member, pwd\u001b[39m=\u001b[39mpwd) \u001b[39mas\u001b[39;00m source, \\\n\u001b[0;32m   1735\u001b[0m      \u001b[39mopen\u001b[39m(targetpath, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m target:\n\u001b[1;32m-> 1736\u001b[0m     shutil\u001b[39m.\u001b[39;49mcopyfileobj(source, target)\n\u001b[0;32m   1738\u001b[0m \u001b[39mreturn\u001b[39;00m targetpath\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:197\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[1;34m(fsrc, fdst, length)\u001b[0m\n\u001b[0;32m    195\u001b[0m fdst_write \u001b[39m=\u001b[39m fdst\u001b[39m.\u001b[39mwrite\n\u001b[0;32m    196\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m     buf \u001b[39m=\u001b[39m fsrc_read(length)\n\u001b[0;32m    198\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m buf:\n\u001b[0;32m    199\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:955\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    954\u001b[0m \u001b[39mwhile\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n\u001b[1;32m--> 955\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read1(n)\n\u001b[0;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[0;32m    957\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readbuffer \u001b[39m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1031\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_type \u001b[39m==\u001b[39m ZIP_DEFLATED:\n\u001b[0;32m   1030\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(n, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMIN_READ_SIZE)\n\u001b[1;32m-> 1031\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decompressor\u001b[39m.\u001b[39;49mdecompress(data, n)\n\u001b[0;32m   1032\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39meof \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m                  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_left \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m                  \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail)\n\u001b[0;32m   1035\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=[1,2], return_epochs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib Qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing the first None classes from all possible events.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\Documents\\These\\moabb\\moabb\\datasets\\castillos2023.py:131: RuntimeWarning: Data file name in EEG.data (P13_burst100.fdt) is incorrect, the file name must have changed on disk, using the correct file name (P6_burst100.fdt).\n",
      "  raw = mne.io.read_raw_eeglab(file_path_list[0], preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "subjects = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "n_channels = 32\n",
    "on_frame = True\n",
    "if on_frame:\n",
    "    freq = 60\n",
    "else:\n",
    "    freq = 500\n",
    "\n",
    "raw_data,labels,codes,labels_codes = get_BVEP_data(subjects,on_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\Documents\\These\\moabb\\moabb\\datasets\\castillos2023.py:131: RuntimeWarning: Data file name in EEG.data (P13_burst100.fdt) is incorrect, the file name must have changed on disk, using the correct file name (P6_burst100.fdt).\n",
      "  raw = mne.io.read_raw_eeglab(file_path_list[0], preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "test domain=['1/0']\n",
      "epoch=  0 gd-step=  323 trn_loss= 0.6902 trn_score=0.5351 val_loss= 0.6918 val_score=0.5205 \n",
      "epoch= 10 gd-step= 3565 trn_loss= 0.6575 trn_score=0.6005 val_loss= 0.6918 val_score=0.5564 \n",
      "epoch= 19 gd-step= 6471 trn_loss= 0.6362 trn_score=0.6387 val_loss= 0.6866 val_score=0.5763 \n",
      "ES best epoch=12\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.43\n",
      "Perform UDA offline\n",
      "test domain=['2/0']\n",
      "epoch=  0 gd-step=  327 trn_loss= 0.6896 trn_score=0.5410 val_loss= 0.6920 val_score=0.5152 \n",
      "epoch= 10 gd-step= 3569 trn_loss= 0.6490 trn_score=0.6235 val_loss= 0.6836 val_score=0.5884 \n",
      "epoch= 19 gd-step= 6489 trn_loss= 0.6420 trn_score=0.6244 val_loss= 0.6957 val_score=0.5661 \n",
      "ES best epoch=5\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.28\n",
      "Perform UDA offline\n",
      "test domain=['3/0']\n",
      "epoch=  0 gd-step=  327 trn_loss= 0.6861 trn_score=0.5608 val_loss= 0.6889 val_score=0.5439 \n",
      "epoch= 10 gd-step= 3578 trn_loss= 0.6388 trn_score=0.6326 val_loss= 0.6712 val_score=0.5873 \n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "if 'inter-session' in cfg['evaluation']:\n",
    "    subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=subjects, return_epochs=False)\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "\n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = TSMNet(**mdl_kwargs).to(device=device, dtype=cfg['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg['epochs']-10,\n",
    "            bs0=cfg['batch_size_train'],\n",
    "            bs=cfg['batch_size_train']/cfg['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=cfg['epochs'],\n",
    "            min_epochs=cfg['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred = trainer.pred(net,dataloader=loader_test)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "\n",
    "        # extract model parameters\n",
    "        state_dict = deepcopy(net.state_dict())\n",
    "\n",
    "        # create new model and perform offline SF UDA\n",
    "        print(\"Perform UDA offline\")\n",
    "        sfuda_offline_net = TSMNet(**mdl_kwargs).to(device=device)\n",
    "        sfuda_offline_net.load_state_dict(state_dict)\n",
    "        sfuda_offline(ds_test, sfuda_offline_net)\n",
    "        res = trainer.test(sfuda_offline_net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(SFUDA)', domain=test_domain, **res))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = TSMNet(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online(ds_test, sfuda_online_net, cfg, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(onlineSFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = TSMNet(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online_simulate(ds_test, sfuda_online_net, cfg, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(online_sim_SFUDA)', domain=test_domain, loss=loss, score=score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\AppData\\Local\\Temp\\ipykernel_16796\\1954672811.py:4: FutureWarning: ['domain'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n",
      "  resdf.groupby(['mode']).agg(['mean', 'std']).round(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">loss</th>\n",
       "      <th colspan=\"2\" halign=\"left\">score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test(SFUDA)</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test(noUDA)</th>\n",
       "      <td>5.59</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test(noUDA)_code</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test(onlineSFUDA)</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test(online_sim_SFUDA)</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        loss       score      \n",
       "                        mean   std  mean   std\n",
       "mode                                          \n",
       "test(SFUDA)             0.71  0.02  0.54  0.03\n",
       "test(noUDA)             5.59  3.64  0.51  0.01\n",
       "test(noUDA)_code         NaN   NaN  0.29  0.07\n",
       "test(onlineSFUDA)       0.70  0.03  0.54  0.03\n",
       "test(online_sim_SFUDA)  0.70  0.03  0.54  0.03\n",
       "train                   0.65  0.03  0.62  0.03\n",
       "validation              0.68  0.00  0.57  0.02"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# report the results\n",
    "resdf = pd.DataFrame(records)\n",
    "\n",
    "resdf.groupby(['mode']).agg(['mean', 'std']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf.to_csv(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/TSMNet_code.csv\")\n",
    "# resdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/TSMNet_code.csv\")\n",
    "print()\n",
    "df_score_code = np.array(df[\"score\"])[df[\"mode\"]==\"test(noUDA)_code\"]\n",
    "df_score = np.array(df[\"score\"])[df[\"mode\"]==\"test(noUDA)\"]\n",
    "\n",
    "np.save(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/TSMNet_score_code\",df_score_code)\n",
    "np.save(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/TSMNet_score\",df_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x16dbc746e90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_df = pd.concat(fit_records)\n",
    "fit_df = fit_df.groupby(['subset', 'fold', 'epoch']).mean()\n",
    "fit_df.columns = fit_df.columns.str.split('_', expand=True)\n",
    "fit_df.columns.names = ['set', 'metric']\n",
    "fit_df = fit_df.stack('set').reset_index()\n",
    "\n",
    "\n",
    "sns.relplot(data=fit_df, x='epoch', y='loss', hue='set', col='subset', col_wrap=5, kind='line', n_boot=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
