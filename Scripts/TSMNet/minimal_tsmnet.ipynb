{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal TSMNet demo notebook for inter-session/-subject source-free (SF) offline and online unsupervised domain adaptation (UDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moabb\\pipelines\\__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from moabb.datasets.bnci import BNCI2015001, BNCI2014001\n",
    "from moabb.paradigms import MotorImagery, CVEP\n",
    "\n",
    "# from library.utils.torch import StratifiedDomainDataLoader\n",
    "from spdnets.utils.data import StratifiedDomainDataLoader, DomainDataset\n",
    "from spdnets.models import TSMNet, SPDSMNet, SPDSMNet2, DSBNSPDBNNet, SPDSMNet_visu, DSBNSPDBNNet_Visu\n",
    "import spdnets.batchnorm as bn\n",
    "import spdnets.functionals as fn\n",
    "\n",
    "from spdnets.trainer import Trainer, VisuTrainer\n",
    "from spdnets.callbacks import MomentumBatchNormScheduler, EarlyStopping\n",
    "\n",
    "import sys\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.estimation import XdawnCovariances\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0,\"C:\\\\Users\\\\s.velut\\\\Documents\\\\These\\\\Protheus_PHD\\\\Scripts\")\n",
    "sys.path.insert(0,\"C:\\\\Users\\\\s.velut\\\\Documents\\\\These\\\\Protheus_PHD\")\n",
    "from utils import prepare_data,get_BVEP_data,balance,get_y_pred, get_TSNE_visu\n",
    "from _utils import make_preds_accumul_aggresive, make_preds_pvalue\n",
    "sys.path.insert(0,\"C:\\\\Users\\\\s.velut\\\\Documents\\\\These\\\\moabb\\\\moabb\\\\datasets\")\n",
    "from castillos2023 import CasitllosCVEP100,CasitllosCVEP40,CasitllosBurstVEP100,CasitllosBurstVEP40\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network and training configuration\n",
    "cfg_org = dict(\n",
    "    epochs = 30,\n",
    "    batch_size_train = 64,\n",
    "    domains_per_batch = 4,\n",
    "    validation_size = 0.2,\n",
    "    evaluation = 'inter-subject', # or 'inter-subject'\n",
    "    dtype = torch.float32,\n",
    "    # parameters for the TSMNet model\n",
    "    mdl_kwargs = dict(\n",
    "        temporal_filters=4,\n",
    "        nclasses = 2,\n",
    "        spatial_filters=24,\n",
    "        subspacedims=20, \n",
    "        bnorm_dispersion=bn.BatchNormDispersion.SCALAR,\n",
    "        spd_device='cpu',\n",
    "        spd_dtype=torch.double,\n",
    "        domain_adaptation=True\n",
    "    )\n",
    ")\n",
    "\n",
    "cfg_spd = dict(\n",
    "    epochs = 20,\n",
    "    batch_size_train = 64,\n",
    "    domains_per_batch = 4,\n",
    "    validation_size = 0.2,\n",
    "    evaluation = 'inter-subject', # or 'inter-subject'\n",
    "    dtype = torch.double,\n",
    "    spd_device='cuda',\n",
    "    # parameters for the TSMNet model\n",
    "    mdl_kwargs = dict(\n",
    "        temporal_filters=2,\n",
    "        nclasses = 2,\n",
    "        spatial_filters=32,\n",
    "        bimap_dims = [32,28,14,7],\n",
    "        subspacedims=32, \n",
    "        bnorm_dispersion=bn.BatchNormDispersion.SCALAR,\n",
    "        spd_device='cuda',\n",
    "        spd_dtype=torch.double,\n",
    "        domain_adaptation=True\n",
    "    )\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load a MOABB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing the first None classes from all possible events.\n"
     ]
    }
   ],
   "source": [
    "# moabb_ds = BNCI2015001()\n",
    "# n_classes = 2\n",
    "# moabb_paradigm = MotorImagery(n_classes=n_classes, events=['right_hand', 'feet'], fmin=4, fmax=36, tmin=0.0, tmax=4.0, resample=256)\n",
    "\n",
    "moabb_ds = CasitllosBurstVEP100()\n",
    "n_classes = 2\n",
    "moabb_paradigm = CVEP()\n",
    "\n",
    "# moabb_ds = BNCI2014001()\n",
    "# n_classes = 4\n",
    "# moabb_paradigm = MotorImagery(n_classes=n_classes, events=[\"left_hand\", \"right_hand\", \"feet\", \"tongue\"], fmin=4, fmax=36, tmin=0.5, tmax=3.496, resample=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SF offline UDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfuda_offline(dataset : DomainDataset, model : TSMNet, cfg):\n",
    "    model.eval()\n",
    "    model.domainadapt_finetune(dataset.features.to(dtype=cfg['dtype'], device=device), dataset.labels.to(device=device), dataset.domains, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SF online UDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sfuda_online_init_other_spd(domain_bn : torch.nn.Module, model : TSMNet, cfg):\n",
    "\n",
    "    # initialize new mean with the grand average of the other domains\n",
    "    M = []\n",
    "    s = []\n",
    "    for _, val in model.spdbnorm_layers[0].batchnorm.items():\n",
    "        M += [val.running_mean_test]\n",
    "        s += [val.running_var_test]\n",
    "    M = torch.cat(M, dim=0)\n",
    "    s = torch.cat(s, dim=0)\n",
    "    prev_mean = fn.spd_mean_kracher_flow(M, dim=0, return_dist=False)\n",
    "    prev_var = fn.spd_mean_kracher_flow(s[...,None], dim=0, return_dist=False).squeeze(-1)\n",
    "\n",
    "    # because we use precomputed means advance the observation number\n",
    "    # so that we start with slower adaptation\n",
    "    domain_bn.adapt_observation = 5 if cfg['evaluation'] == 'inter-subject' else 15\n",
    "    domain_bn.running_mean_test.data = prev_mean.data.clone()\n",
    "    domain_bn.running_var_test = prev_var.clone()\n",
    "\n",
    "\n",
    "def sfuda_online_init_spd(target_domains : torch.LongTensor, model : TSMNet, cfg, strategy : str = 'other'):\n",
    "\n",
    "    assert target_domains.ndim == 1\n",
    "    # assert isinstance(model.spdbnorm, bn.BaseDomainBatchNorm)\n",
    "    for target_domain in target_domains:\n",
    "        for i in range(len(model.bimap_dims[1:])):\n",
    "            domain_key = model.spdbnorm_layers[i].domain_to_key(target_domain)\n",
    "            # add domain if not yet in the model\n",
    "            if domain_key not in model.spdbnorm_layers[i].batchnorm:\n",
    "                bncls = model.spdbnorm_layers[i].domain_bn_cls\n",
    "                domain_bn = bncls(\n",
    "                    shape=model.spdbnorm_layers[i].mean.shape,\n",
    "                    batchdim=model.spdbnorm_layers[i].batchdim, \n",
    "                    learn_mean=model.spdbnorm_layers[i].learn_mean,\n",
    "                    learn_std=model.spdbnorm_layers[i].learn_std,\n",
    "                    dispersion=model.spdbnorm_layers[i].dispersion,\n",
    "                    mean=model.spdbnorm_layers[i].mean,\n",
    "                    std=model.spdbnorm_layers[i].std,\n",
    "                    eta=model.spdbnorm_layers[i].eta,\n",
    "                    eta_test=model.spdbnorm_layers[i].eta_test\n",
    "                )\n",
    "            else:\n",
    "                domain_bn = model.spdbnorm_layers[i].batchnorm[domain_key]\n",
    "\n",
    "        if strategy == 'other':\n",
    "            _sfuda_online_init_other_spd(domain_bn, model, cfg)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        # change the BN mode to perform online adaptation (for each batch)\n",
    "        domain_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.ADAPT)\n",
    "    \n",
    "    for target_domain in target_domains:    \n",
    "        for i in range(len(model.bimap_dims[1:])):\n",
    "            if domain_key not in model.spdbnorm_layers[i].batchnorm:\n",
    "                model.spdbnorm_layers[i].batchnorm[domain_key] = domain_bn\n",
    "\n",
    "def sfuda_online_step(inputs : torch.Tensor, domains : torch.LongTensor, model : TSMNet, cfg):\n",
    "\n",
    "    model = model.to(device='cpu')\n",
    "    # the model needs to be in evaluation mode so that the batch norm statistics for testing will be adapted\n",
    "    model.eval()\n",
    "\n",
    "    activations = model(inputs.to(dtype=cfg['dtype'],device=cfg['spd_device']), domains)\n",
    "    # return class probabilities\n",
    "    return activations\n",
    "\n",
    "\n",
    "def sfuda_online_simulate_spd(dataset : DomainDataset, model : TSMNet, cfg, loss_fn):\n",
    "\n",
    "    # do adaptation if the model is configured to do domain adaptation\n",
    "    if not model.domain_adaptation_:\n",
    "        test_loss, score  = None, None\n",
    "    else:\n",
    "        \n",
    "        # extract the target domains from the dataset\n",
    "        target_domains = dataset.domains.unique()\n",
    "\n",
    "        sfuda_online_init_spd(target_domains, model, cfg)\n",
    "        \n",
    "        y_true = []\n",
    "        y_hat = []\n",
    "\n",
    "        test_loss = 0.\n",
    "        # feed each observation through the network\n",
    "        # to adapt and infer the target\n",
    "        for i, (features, y) in enumerate(dataset):\n",
    "            \n",
    "            inputs = features['inputs'][None,...]\n",
    "            domains = features['domains'][None,...]\n",
    "            y = y\n",
    "\n",
    "            pred = sfuda_online_step(inputs, domains, model, cfg)\n",
    "            test_loss += loss_fn(pred, y[None,...]).item()\n",
    "            y_true.append(y[None,...])\n",
    "            y_hat.append(pred.argmax(1)[None,...])\n",
    "            # p_class = torch.nn.functional.softmax(pred, dim=-1)\n",
    "\n",
    "        # compute the overall score \n",
    "        score = balanced_accuracy_score(torch.cat(y_true).detach().cpu().numpy(), torch.cat(y_hat).detach().cpu().numpy())\n",
    "\n",
    "        test_loss /= len(dataset)\n",
    "\n",
    "        # stop adaptation for the target domains\n",
    "        for target_domain in target_domains:\n",
    "            for i in range(len(model.bimap_dims[1:])):\n",
    "                trgt_bn = model.spdbnorm_layers[i].batchnorm[str(target_domain.item())]\n",
    "                trgt_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.BUFFER)  \n",
    "\n",
    "    return test_loss, score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfuda_online(dataset : DomainDataset, model : TSMNet, cfg, loss_fn):\n",
    "\n",
    "    model = model.to(device='cpu')\n",
    "    # the model needs to be in evaluation mode so that the batch norm statistics for testing will be adapted\n",
    "    model.eval()\n",
    "\n",
    "    # do adaptation if the model is configured to do domain adaptation\n",
    "    if not model.domain_adaptation_:\n",
    "        test_loss, score  = None, None\n",
    "    else:\n",
    "        \n",
    "        # extract the target domains from the dataset\n",
    "        target_domains = dataset.domains.unique()\n",
    "        # initialize new mean with the grand average of the other domains\n",
    "        M = []\n",
    "        s = []\n",
    "        for key, val in model.spdbnorm.batchnorm.items():\n",
    "            if key in [f'dom {t}' for t in target_domains]:\n",
    "                continue\n",
    "            M += [val.running_mean_test]\n",
    "            s += [val.running_var_test]\n",
    "        M = torch.cat(M, dim=0)\n",
    "        s = torch.cat(s, dim=0)\n",
    "        prev_mean = fn.spd_mean_kracher_flow(M, dim=0, return_dist=False)\n",
    "        prev_var = fn.spd_mean_kracher_flow(s[...,None], dim=0, return_dist=False).squeeze(-1)\n",
    "        \n",
    "        # assign the grand average mean and var to the target domains\n",
    "        for target_domain in target_domains:\n",
    "\n",
    "            trgt_bn = model.spdbnorm.batchnorm[str(target_domain.item())]\n",
    "            # because we use precomputed means advance the observation number\n",
    "            # so that we start with slower adaptation\n",
    "            trgt_bn.adapt_observation = 5 if cfg['evaluation'] == 'inter-subject' else 15\n",
    "            trgt_bn.running_mean_test.data = prev_mean.data.clone()\n",
    "            trgt_bn.running_var_test = prev_var.clone()\n",
    "\n",
    "            # change the BN mode to perform online adaptation (for each batch)\n",
    "            trgt_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.ADAPT)\n",
    "        \n",
    "        y_true = []\n",
    "        y_hat = []\n",
    "\n",
    "        test_loss = 0.\n",
    "        # feed each observation through the network\n",
    "        # to adapt and infer the target\n",
    "        for i, (features, y) in enumerate(dataset):\n",
    "            \n",
    "            inputs = features['inputs'][None,...].to(dtype=cfg['dtype'])\n",
    "            domains = features['domains'][None,...]\n",
    "            y = y\n",
    "            pred = model.forward(inputs=inputs, domains=domains)\n",
    "            test_loss += loss_fn(pred, y[None,...]).item()\n",
    "            y_true.append(y[None,...])\n",
    "            y_hat.append(pred.argmax(1)[None,...])\n",
    "\n",
    "        # compute the overall score \n",
    "        score = balanced_accuracy_score(torch.cat(y_true).detach().cpu().numpy(), torch.cat(y_hat).detach().cpu().numpy())\n",
    "\n",
    "        test_loss /= len(dataset)\n",
    "\n",
    "        # stop adaptation for the target domains\n",
    "        for target_domain in target_domains:\n",
    "            trgt_bn = model.spdbnorm.batchnorm[str(target_domain.item())]\n",
    "            trgt_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.BUFFER)  \n",
    "\n",
    "    return test_loss, score \n",
    "\n",
    "def sfuda_online_spd(dataset : DomainDataset, model : TSMNet, cfg, loss_fn):\n",
    "\n",
    "    model = model.to(device='cpu')\n",
    "    # the model needs to be in evaluation mode so that the batch norm statistics for testing will be adapted\n",
    "    model.eval()\n",
    "\n",
    "    # do adaptation if the model is configured to do domain adaptation\n",
    "    if not model.domain_adaptation_:\n",
    "        test_loss, score  = None, None\n",
    "    else:\n",
    "        \n",
    "        # extract the target domains from the dataset\n",
    "        target_domains = dataset.domains.unique()\n",
    "        # initialize new mean with the grand average of the other domains\n",
    "        M = []\n",
    "        s = []\n",
    "        for key, val in model.spdbnorm_layers[0].batchnorm.items():\n",
    "            if key in [f'dom {t}' for t in target_domains]:\n",
    "                continue\n",
    "            M += [val.running_mean_test]\n",
    "            s += [val.running_var_test]\n",
    "        M = torch.cat(M, dim=0)\n",
    "        s = torch.cat(s, dim=0)\n",
    "        prev_mean = fn.spd_mean_kracher_flow(M, dim=0, return_dist=False)\n",
    "        prev_var = fn.spd_mean_kracher_flow(s[...,None], dim=0, return_dist=False).squeeze(-1)\n",
    "        \n",
    "        # assign the grand average mean and var to the target domains\n",
    "        for target_domain in target_domains:\n",
    "            trgt_bn = model.spdbnorm_layers[0].batchnorm[str(target_domain.item())]\n",
    "            # because we use precomputed means advance the observation number\n",
    "            # so that we start with slower adaptation\n",
    "            trgt_bn.adapt_observation = 5 if cfg['evaluation'] == 'inter-subject' else 15\n",
    "            trgt_bn.running_mean_test.data = prev_mean.data.clone()\n",
    "            trgt_bn.running_var_test = prev_var.clone()\n",
    "\n",
    "            # change the BN mode to perform online adaptation (for each batch)\n",
    "            trgt_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.ADAPT)\n",
    "        \n",
    "        y_true = []\n",
    "        y_hat = []\n",
    "\n",
    "        test_loss = 0.\n",
    "        # feed each observation through the network\n",
    "        # to adapt and infer the target\n",
    "        for i, (features, y) in enumerate(dataset):\n",
    "            \n",
    "            inputs = features['inputs'][None,...].to(dtype=cfg['dtype'],device=cfg['spd_device'])\n",
    "            domains = features['domains'][None,...]\n",
    "            y = y\n",
    "            pred = model.forward(inputs=inputs, domains=domains)\n",
    "            test_loss += loss_fn(pred, y[None,...]).item()\n",
    "            y_true.append(y[None,...])\n",
    "            y_hat.append(pred.argmax(1)[None,...])\n",
    "\n",
    "        # compute the overall score \n",
    "        score = balanced_accuracy_score(torch.cat(y_true).detach().cpu().numpy(), torch.cat(y_hat).detach().cpu().numpy())\n",
    "\n",
    "        test_loss /= len(dataset)\n",
    "\n",
    "        # stop adaptation for the target domains\n",
    "        for target_domain in target_domains:\n",
    "            trgt_bn = model.spdbnorm_layers[0].batchnorm[str(target_domain.item())]\n",
    "            trgt_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.BUFFER)  \n",
    "\n",
    "    return test_loss, score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sfuda_online_init_other(domain_bn : torch.nn.Module, model : TSMNet, cfg):\n",
    "\n",
    "    # initialize new mean with the grand average of the other domains\n",
    "    M = []\n",
    "    s = []\n",
    "    for _, val in model.spdbnorm.batchnorm.items():\n",
    "        M += [val.running_mean_test]\n",
    "        s += [val.running_var_test]\n",
    "    M = torch.cat(M, dim=0)\n",
    "    s = torch.cat(s, dim=0)\n",
    "    prev_mean = fn.spd_mean_kracher_flow(M, dim=0, return_dist=False)\n",
    "    prev_var = fn.spd_mean_kracher_flow(s[...,None], dim=0, return_dist=False).squeeze(-1)\n",
    "\n",
    "    # because we use precomputed means advance the observation number\n",
    "    # so that we start with slower adaptation\n",
    "    domain_bn.adapt_observation = 5 if cfg['evaluation'] == 'inter-subject' else 15\n",
    "    domain_bn.running_mean_test.data = prev_mean.data.clone()\n",
    "    domain_bn.running_var_test = prev_var.clone()\n",
    "\n",
    "\n",
    "def sfuda_online_init(target_domains : torch.LongTensor, model : TSMNet, cfg, strategy : str = 'other'):\n",
    "\n",
    "    assert target_domains.ndim == 1\n",
    "    assert isinstance(model.spdbnorm, bn.BaseDomainBatchNorm)\n",
    "    for target_domain in target_domains:\n",
    "        domain_key = model.spdbnorm.domain_to_key(target_domain)\n",
    "        # add domain if not yet in the model\n",
    "        if domain_key not in model.spdbnorm.batchnorm:\n",
    "            bncls = model.spdbnorm.domain_bn_cls\n",
    "            domain_bn = bncls(\n",
    "                shape=model.spdbnorm.mean.shape,\n",
    "                batchdim=model.spdbnorm.batchdim, \n",
    "                learn_mean=model.spdbnorm.learn_mean,\n",
    "                learn_std=model.spdbnorm.learn_std,\n",
    "                dispersion=model.spdbnorm.dispersion,\n",
    "                mean=model.spdbnorm.mean,\n",
    "                std=model.spdbnorm.std,\n",
    "                eta=model.spdbnorm.eta,\n",
    "                eta_test=model.spdbnorm.eta_test\n",
    "            )\n",
    "        else:\n",
    "            domain_bn = model.spdbnorm.batchnorm[domain_key]\n",
    "\n",
    "        if strategy == 'other':\n",
    "            _sfuda_online_init_other(domain_bn, model, cfg)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        # change the BN mode to perform online adaptation (for each batch)\n",
    "        domain_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.ADAPT)\n",
    "    \n",
    "    for target_domain in target_domains:    \n",
    "        if domain_key not in model.spdbnorm.batchnorm:\n",
    "            model.spdbnorm.batchnorm[domain_key] = domain_bn\n",
    "\n",
    "def sfuda_online_step(inputs : torch.Tensor, domains : torch.LongTensor, model : TSMNet, cfg):\n",
    "\n",
    "    model = model.to(device='cpu')\n",
    "    # the model needs to be in evaluation mode so that the batch norm statistics for testing will be adapted\n",
    "    model.eval()\n",
    "\n",
    "    activations = model(inputs.to(dtype=cfg['dtype']), domains)\n",
    "    # return class probabilities\n",
    "    return activations\n",
    "\n",
    "\n",
    "def sfuda_online_simulate(dataset : DomainDataset, model : TSMNet, cfg, loss_fn):\n",
    "\n",
    "    # do adaptation if the model is configured to do domain adaptation\n",
    "    if not model.domain_adaptation_:\n",
    "        test_loss, score  = None, None\n",
    "    else:\n",
    "        \n",
    "        # extract the target domains from the dataset\n",
    "        target_domains = dataset.domains.unique()\n",
    "\n",
    "        sfuda_online_init(target_domains, model, cfg)\n",
    "        \n",
    "        y_true = []\n",
    "        y_hat = []\n",
    "\n",
    "        test_loss = 0.\n",
    "        # feed each observation through the network\n",
    "        # to adapt and infer the target\n",
    "        for i, (features, y) in enumerate(dataset):\n",
    "            \n",
    "            inputs = features['inputs'][None,...]\n",
    "            domains = features['domains'][None,...]\n",
    "            y = y\n",
    "\n",
    "            pred = sfuda_online_step(inputs, domains, model, cfg)\n",
    "            test_loss += loss_fn(pred, y[None,...]).item()\n",
    "            y_true.append(y[None,...])\n",
    "            y_hat.append(pred.argmax(1)[None,...])\n",
    "            # p_class = torch.nn.functional.softmax(pred, dim=-1)\n",
    "\n",
    "        # compute the overall score \n",
    "        score = balanced_accuracy_score(torch.cat(y_true).detach().cpu().numpy(), torch.cat(y_hat).detach().cpu().numpy())\n",
    "\n",
    "        test_loss /= len(dataset)\n",
    "\n",
    "        # stop adaptation for the target domains\n",
    "        for target_domain in target_domains:\n",
    "            trgt_bn = model.spdbnorm.batchnorm[str(target_domain.item())]\n",
    "            trgt_bn.set_test_stats_mode(bn.BatchNormTestStatsMode.BUFFER)  \n",
    "\n",
    "    return test_loss, score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit and evaluat the model for all domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib Qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing the first None classes from all possible events.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\Documents\\These\\moabb\\moabb\\datasets\\castillos2023.py:131: RuntimeWarning: Data file name in EEG.data (P13_burst100.fdt) is incorrect, the file name must have changed on disk, using the correct file name (P6_burst100.fdt).\n",
      "  raw = mne.io.read_raw_eeglab(file_path_list[0], preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "# subjects = [1,2,3]\n",
    "subjects = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "n_channels = 32\n",
    "on_frame = True\n",
    "if on_frame:\n",
    "    freq = 60\n",
    "else:\n",
    "    freq = 500\n",
    "\n",
    "raw_data,labels,codes,labels_codes = get_BVEP_data(subjects,on_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original TSMNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\Documents\\These\\moabb\\moabb\\datasets\\castillos2023.py:131: RuntimeWarning: Data file name in EEG.data (P13_burst100.fdt) is incorrect, the file name must have changed on disk, using the correct file name (P6_burst100.fdt).\n",
      "  raw = mne.io.read_raw_eeglab(file_path_list[0], preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "test domain=['1/0']\n",
      "epoch=  0 gd-step=  325 trn_loss= 0.6873 trn_score=0.5506 val_loss= 0.6899 val_score=0.5347 \n",
      "epoch= 10 gd-step= 3574 trn_loss= 0.6118 trn_score=0.6662 val_loss= 0.6877 val_score=0.5805 \n",
      "epoch= 20 gd-step= 6813 trn_loss= 0.5953 trn_score=0.6749 val_loss= 0.7130 val_score=0.5799 \n",
      "epoch= 29 gd-step= 9734 trn_loss= 0.5847 trn_score=0.6913 val_loss= 0.7156 val_score=0.5938 \n",
      "ES best epoch=4\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.33\n",
      "Perform UDA offline\n",
      "test domain=['2/0']\n",
      "epoch=  0 gd-step=  323 trn_loss= 0.6854 trn_score=0.5622 val_loss= 0.6885 val_score=0.5383 \n",
      "epoch= 10 gd-step= 3565 trn_loss= 0.6096 trn_score=0.6624 val_loss= 0.6847 val_score=0.5824 \n",
      "epoch= 20 gd-step= 6794 trn_loss= 0.5827 trn_score=0.6920 val_loss= 0.6955 val_score=0.5941 \n",
      "epoch= 29 gd-step= 9709 trn_loss= 0.5753 trn_score=0.6991 val_loss= 0.7049 val_score=0.5989 \n",
      "ES best epoch=6\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.28\n",
      "Perform UDA offline\n",
      "test domain=['3/0']\n",
      "epoch=  0 gd-step=  326 trn_loss= 0.6877 trn_score=0.5515 val_loss= 0.6905 val_score=0.5322 \n",
      "epoch= 10 gd-step= 3570 trn_loss= 0.6239 trn_score=0.6456 val_loss= 0.6939 val_score=0.5659 \n",
      "epoch= 20 gd-step= 6800 trn_loss= 0.5898 trn_score=0.6845 val_loss= 0.7055 val_score=0.5900 \n",
      "epoch= 29 gd-step= 9715 trn_loss= 0.5783 trn_score=0.6936 val_loss= 0.7161 val_score=0.5907 \n",
      "ES best epoch=4\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.32\n",
      "Perform UDA offline\n",
      "test domain=['4/0']\n",
      "epoch=  0 gd-step=  327 trn_loss= 0.6839 trn_score=0.5736 val_loss= 0.6878 val_score=0.5509 \n",
      "epoch= 10 gd-step= 3570 trn_loss= 0.6151 trn_score=0.6584 val_loss= 0.6939 val_score=0.5644 \n",
      "epoch= 20 gd-step= 6803 trn_loss= 0.5927 trn_score=0.6794 val_loss= 0.7104 val_score=0.5769 \n",
      "epoch= 29 gd-step= 9712 trn_loss= 0.5780 trn_score=0.6904 val_loss= 0.7184 val_score=0.5712 \n",
      "ES best epoch=4\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.33\n",
      "Perform UDA offline\n",
      "test domain=['5/0']\n",
      "epoch=  0 gd-step=  325 trn_loss= 0.6878 trn_score=0.5477 val_loss= 0.6922 val_score=0.5167 \n",
      "epoch= 10 gd-step= 3560 trn_loss= 0.6257 trn_score=0.6542 val_loss= 0.6966 val_score=0.5621 \n",
      "epoch= 20 gd-step= 6801 trn_loss= 0.5970 trn_score=0.6787 val_loss= 0.7057 val_score=0.5742 \n",
      "epoch= 29 gd-step= 9731 trn_loss= 0.5836 trn_score=0.6891 val_loss= 0.7038 val_score=0.5773 \n",
      "ES best epoch=2\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.35\n",
      "Perform UDA offline\n",
      "test domain=['6/0']\n",
      "epoch=  0 gd-step=  323 trn_loss= 0.6872 trn_score=0.5529 val_loss= 0.6912 val_score=0.5222 \n",
      "epoch= 10 gd-step= 3562 trn_loss= 0.6234 trn_score=0.6485 val_loss= 0.6905 val_score=0.5756 \n",
      "epoch= 20 gd-step= 6801 trn_loss= 0.5936 trn_score=0.6833 val_loss= 0.7067 val_score=0.5824 \n",
      "epoch= 29 gd-step= 9712 trn_loss= 0.5824 trn_score=0.6890 val_loss= 0.7210 val_score=0.5831 \n",
      "ES best epoch=6\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.18\n",
      "Perform UDA offline\n",
      "test domain=['7/0']\n",
      "epoch=  0 gd-step=  327 trn_loss= 0.6915 trn_score=0.5311 val_loss= 0.6946 val_score=0.5174 \n",
      "epoch= 10 gd-step= 3562 trn_loss= 0.6246 trn_score=0.6521 val_loss= 0.6833 val_score=0.5811 \n",
      "epoch= 20 gd-step= 6800 trn_loss= 0.5924 trn_score=0.6843 val_loss= 0.6980 val_score=0.5926 \n",
      "epoch= 29 gd-step= 9717 trn_loss= 0.5818 trn_score=0.6890 val_loss= 0.7065 val_score=0.5867 \n",
      "ES best epoch=8\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.25\n",
      "Perform UDA offline\n",
      "test domain=['8/0']\n",
      "epoch=  0 gd-step=  323 trn_loss= 0.6879 trn_score=0.5501 val_loss= 0.6898 val_score=0.5362 \n",
      "epoch= 10 gd-step= 3553 trn_loss= 0.6262 trn_score=0.6471 val_loss= 0.6863 val_score=0.5725 \n",
      "epoch= 20 gd-step= 6790 trn_loss= 0.6017 trn_score=0.6684 val_loss= 0.7035 val_score=0.5835 \n",
      "epoch= 29 gd-step= 9711 trn_loss= 0.5892 trn_score=0.6788 val_loss= 0.7104 val_score=0.5799 \n",
      "ES best epoch=8\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.25\n",
      "Perform UDA offline\n",
      "test domain=['9/0']\n",
      "epoch=  0 gd-step=  324 trn_loss= 0.6866 trn_score=0.5577 val_loss= 0.6921 val_score=0.5284 \n",
      "epoch= 10 gd-step= 3561 trn_loss= 0.6149 trn_score=0.6609 val_loss= 0.7027 val_score=0.5670 \n",
      "epoch= 20 gd-step= 6783 trn_loss= 0.5962 trn_score=0.6786 val_loss= 0.7268 val_score=0.5670 \n",
      "epoch= 29 gd-step= 9704 trn_loss= 0.5882 trn_score=0.6896 val_loss= 0.7394 val_score=0.5693 \n",
      "ES best epoch=1\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.25\n",
      "Perform UDA offline\n",
      "test domain=['10/0']\n",
      "epoch=  0 gd-step=  324 trn_loss= 0.6886 trn_score=0.5571 val_loss= 0.6916 val_score=0.5301 \n",
      "epoch= 10 gd-step= 3547 trn_loss= 0.6216 trn_score=0.6521 val_loss= 0.6860 val_score=0.5811 \n",
      "epoch= 20 gd-step= 6790 trn_loss= 0.6010 trn_score=0.6754 val_loss= 0.7055 val_score=0.5877 \n",
      "epoch= 29 gd-step= 9723 trn_loss= 0.5853 trn_score=0.6900 val_loss= 0.7157 val_score=0.5801 \n",
      "ES best epoch=7\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.5\n",
      "Perform UDA offline\n",
      "test domain=['11/0']\n",
      "epoch=  0 gd-step=  320 trn_loss= 0.6887 trn_score=0.5373 val_loss= 0.6927 val_score=0.5170 \n",
      "epoch= 10 gd-step= 3554 trn_loss= 0.6355 trn_score=0.6349 val_loss= 0.7099 val_score=0.5583 \n",
      "epoch= 20 gd-step= 6781 trn_loss= 0.5948 trn_score=0.6805 val_loss= 0.7220 val_score=0.5786 \n",
      "epoch= 29 gd-step= 9699 trn_loss= 0.5914 trn_score=0.6843 val_loss= 0.7381 val_score=0.5693 \n",
      "ES best epoch=2\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.23\n",
      "Perform UDA offline\n",
      "test domain=['12/0']\n",
      "epoch=  0 gd-step=  325 trn_loss= 0.6847 trn_score=0.5557 val_loss= 0.6892 val_score=0.5339 \n",
      "epoch= 10 gd-step= 3549 trn_loss= 0.6166 trn_score=0.6602 val_loss= 0.7021 val_score=0.5758 \n",
      "epoch= 20 gd-step= 6783 trn_loss= 0.5892 trn_score=0.6825 val_loss= 0.7130 val_score=0.5705 \n",
      "epoch= 29 gd-step= 9704 trn_loss= 0.5868 trn_score=0.6895 val_loss= 0.7358 val_score=0.5661 \n",
      "ES best epoch=2\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.23\n",
      "Perform UDA offline\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "if 'inter-session' in cfg_org['evaluation']:\n",
    "    subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg_org['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=subjects, return_epochs=False)\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_org['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "\n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        X_std = X_fit.std(axis=0)\n",
    "        X_fit /= X_std + 1e-8\n",
    "        X_std = X[test].std(axis=0)\n",
    "        X[test] /= X_std + 1e-8\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg_org['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg_org['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg_org['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg_org['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = TSMNet(**mdl_kwargs).to(device=device, dtype=cfg_org['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_org['epochs']-10,\n",
    "            bs0=cfg_org['batch_size_train'],\n",
    "            bs=cfg_org['batch_size_train']/cfg_org['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=cfg_org['epochs'],\n",
    "            min_epochs=cfg_org['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_org['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred = trainer.pred(net,dataloader=loader_test)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "\n",
    "        # extract model parameters\n",
    "        state_dict = deepcopy(net.state_dict())\n",
    "\n",
    "        # create new model and perform offline SF UDA\n",
    "        print(\"Perform UDA offline\")\n",
    "        sfuda_offline_net = TSMNet(**mdl_kwargs).to(device=device)\n",
    "        sfuda_offline_net.load_state_dict(state_dict)\n",
    "        sfuda_offline(ds_test, sfuda_offline_net, cfg_org)\n",
    "        res = trainer.test(sfuda_offline_net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(SFUDA)', domain=test_domain, **res))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = TSMNet(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online(ds_test, sfuda_online_net, cfg_org, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(onlineSFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = TSMNet(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online_simulate(ds_test, sfuda_online_net, cfg_org, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(online_sim_SFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "resdf = pd.DataFrame(records)    \n",
    "resdf.to_csv(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/TSMNet_code.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified TSMNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\Documents\\These\\moabb\\moabb\\datasets\\castillos2023.py:131: RuntimeWarning: Data file name in EEG.data (P13_burst100.fdt) is incorrect, the file name must have changed on disk, using the correct file name (P6_burst100.fdt).\n",
      "  raw = mne.io.read_raw_eeglab(file_path_list[0], preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "test domain=['1/0']\n",
      "epoch=  0 gd-step=  327 trn_loss= 0.6754 trn_score=0.5836 val_loss= 0.6758 val_score=0.5830 \n",
      "epoch= 10 gd-step= 3571 trn_loss= 0.5908 trn_score=0.6896 val_loss= 0.5892 val_score=0.6881 \n",
      "epoch= 19 gd-step= 6482 trn_loss= 0.5874 trn_score=0.6948 val_loss= 0.5869 val_score=0.6850 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.7\n",
      "Perform UDA offline\n",
      "test domain=['2/0']\n",
      "epoch=  0 gd-step=  322 trn_loss= 0.6793 trn_score=0.5699 val_loss= 0.6803 val_score=0.5680 \n",
      "epoch= 10 gd-step= 3570 trn_loss= 0.6295 trn_score=0.6501 val_loss= 0.6278 val_score=0.6479 \n",
      "epoch= 19 gd-step= 6481 trn_loss= 0.6247 trn_score=0.6537 val_loss= 0.6244 val_score=0.6508 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.52\n",
      "Perform UDA offline\n",
      "test domain=['3/0']\n",
      "epoch=  0 gd-step=  323 trn_loss= 0.6586 trn_score=0.6297 val_loss= 0.6580 val_score=0.6265 \n",
      "epoch= 10 gd-step= 3572 trn_loss= 0.6070 trn_score=0.6738 val_loss= 0.6029 val_score=0.6780 \n",
      "epoch= 19 gd-step= 6485 trn_loss= 0.6002 trn_score=0.6819 val_loss= 0.5962 val_score=0.6830 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.68\n",
      "Perform UDA offline\n",
      "test domain=['4/0']\n",
      "epoch=  0 gd-step=  325 trn_loss= 0.6561 trn_score=0.6352 val_loss= 0.6548 val_score=0.6445 \n",
      "epoch= 10 gd-step= 3569 trn_loss= 0.5634 trn_score=0.7125 val_loss= 0.5683 val_score=0.7144 \n",
      "epoch= 19 gd-step= 6490 trn_loss= 0.5589 trn_score=0.7131 val_loss= 0.5630 val_score=0.7136 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.75\n",
      "Perform UDA offline\n",
      "test domain=['5/0']\n",
      "epoch=  0 gd-step=  328 trn_loss= 0.6594 trn_score=0.6080 val_loss= 0.6592 val_score=0.6089 \n",
      "epoch= 10 gd-step= 3569 trn_loss= 0.6103 trn_score=0.6680 val_loss= 0.6076 val_score=0.6653 \n",
      "epoch= 19 gd-step= 6485 trn_loss= 0.6075 trn_score=0.6699 val_loss= 0.6044 val_score=0.6676 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.65\n",
      "Perform UDA offline\n",
      "test domain=['6/0']\n",
      "epoch=  0 gd-step=  327 trn_loss= 0.6772 trn_score=0.5707 val_loss= 0.6810 val_score=0.5612 \n",
      "epoch= 10 gd-step= 3574 trn_loss= 0.6362 trn_score=0.6429 val_loss= 0.6411 val_score=0.6326 \n",
      "epoch= 19 gd-step= 6496 trn_loss= 0.6335 trn_score=0.6444 val_loss= 0.6393 val_score=0.6371 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.78\n",
      "Perform UDA offline\n",
      "test domain=['7/0']\n",
      "epoch=  0 gd-step=  323 trn_loss= 0.6603 trn_score=0.6271 val_loss= 0.6619 val_score=0.6271 \n",
      "epoch= 10 gd-step= 3567 trn_loss= 0.5719 trn_score=0.7112 val_loss= 0.5717 val_score=0.7110 \n",
      "epoch= 19 gd-step= 6476 trn_loss= 0.5572 trn_score=0.7224 val_loss= 0.5588 val_score=0.7208 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.52\n",
      "Perform UDA offline\n",
      "test domain=['8/0']\n",
      "epoch=  0 gd-step=  324 trn_loss= 0.6634 trn_score=0.6101 val_loss= 0.6641 val_score=0.6070 \n",
      "epoch= 10 gd-step= 3556 trn_loss= 0.6172 trn_score=0.6595 val_loss= 0.6216 val_score=0.6553 \n",
      "epoch= 19 gd-step= 6476 trn_loss= 0.6104 trn_score=0.6694 val_loss= 0.6129 val_score=0.6659 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.9\n",
      "Perform UDA offline\n",
      "test domain=['9/0']\n",
      "epoch=  0 gd-step=  326 trn_loss= 0.6593 trn_score=0.6294 val_loss= 0.6586 val_score=0.6330 \n",
      "epoch= 10 gd-step= 3563 trn_loss= 0.6022 trn_score=0.6825 val_loss= 0.6032 val_score=0.6758 \n",
      "epoch= 19 gd-step= 6483 trn_loss= 0.5926 trn_score=0.6891 val_loss= 0.5931 val_score=0.6803 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.63\n",
      "Perform UDA offline\n",
      "test domain=['10/0']\n",
      "epoch=  0 gd-step=  326 trn_loss= 0.6738 trn_score=0.5852 val_loss= 0.6760 val_score=0.5795 \n",
      "epoch= 10 gd-step= 3569 trn_loss= 0.6314 trn_score=0.6492 val_loss= 0.6377 val_score=0.6472 \n",
      "epoch= 19 gd-step= 6492 trn_loss= 0.6276 trn_score=0.6509 val_loss= 0.6354 val_score=0.6464 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.83\n",
      "Perform UDA offline\n",
      "test domain=['11/0']\n",
      "epoch=  0 gd-step=  322 trn_loss= 0.6593 trn_score=0.6129 val_loss= 0.6609 val_score=0.6074 \n",
      "epoch= 10 gd-step= 3561 trn_loss= 0.5942 trn_score=0.6832 val_loss= 0.6035 val_score=0.6718 \n",
      "epoch= 19 gd-step= 6465 trn_loss= 0.5906 trn_score=0.6882 val_loss= 0.6013 val_score=0.6727 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.7\n",
      "Perform UDA offline\n",
      "test domain=['12/0']\n",
      "epoch=  0 gd-step=  324 trn_loss= 0.6697 trn_score=0.6019 val_loss= 0.6736 val_score=0.5947 \n",
      "epoch= 10 gd-step= 3564 trn_loss= 0.6437 trn_score=0.6329 val_loss= 0.6536 val_score=0.6229 \n",
      "epoch= 19 gd-step= 6484 trn_loss= 0.6416 trn_score=0.6366 val_loss= 0.6520 val_score=0.6271 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.6\n",
      "Perform UDA offline\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "if 'inter-session' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=subjects, return_epochs=False)\n",
    "\n",
    "    X_std = X.std(axis=0)\n",
    "    X /= X_std + 1e-8\n",
    "\n",
    "    xdawncov = XdawnCovariances(estimator=\"lwf\",xdawn_estimator=\"lwf\",nfilter=8)\n",
    "    X = xdawncov.fit_transform(X,labels)\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_spd['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "\n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg_spd['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg_spd['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg_spd['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg_spd['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = SPDSMNet(**mdl_kwargs).to(device=device, dtype=cfg_spd['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_spd['epochs']-10,\n",
    "            bs0=cfg_spd['batch_size_train'],\n",
    "            bs=cfg_spd['batch_size_train']/cfg_spd['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=cfg_spd['epochs'],\n",
    "            min_epochs=cfg_spd['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_spd['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred = trainer.pred(net,dataloader=loader_test)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "\n",
    "        # extract model parameters\n",
    "        state_dict = deepcopy(net.state_dict())\n",
    "\n",
    "        # create new model and perform offline SF UDA\n",
    "        print(\"Perform UDA offline\")\n",
    "        sfuda_offline_net = SPDSMNet(**mdl_kwargs).to(device=device)\n",
    "        sfuda_offline_net.load_state_dict(state_dict)\n",
    "        sfuda_offline(ds_test, sfuda_offline_net, cfg_spd)\n",
    "        res = trainer.test(sfuda_offline_net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(SFUDA)', domain=test_domain, **res))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = SPDSMNet(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(onlineSFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "        # # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = SPDSMNet(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online_simulate(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(online_sim_SFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "resdf = pd.DataFrame(records)    \n",
    "resdf.to_csv(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/SDPSMNet_code.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified TSMNET2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\minimal_tsmnet.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X52sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# iterate over subject groups\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X52sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m ix_subset, subjects \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(subset_iter):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X52sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X52sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# get the data from the MOABB paradigm/dataset\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X52sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     X, labels, metadata \u001b[39m=\u001b[39m moabb_paradigm\u001b[39m.\u001b[39;49mget_data(moabb_ds, subjects\u001b[39m=\u001b[39;49msubjects, return_epochs\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X52sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     X_std \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mstd(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X52sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     X \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m X_std \u001b[39m+\u001b[39m \u001b[39m1e-8\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moabb\\paradigms\\base.py:278\u001b[0m, in \u001b[0;36mBaseProcessing.get_data\u001b[1;34m(self, dataset, subjects, return_epochs, return_raws, cache_config, postprocess_pipeline)\u001b[0m\n\u001b[0;32m    273\u001b[0m process_pipelines \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_process_pipelines(\n\u001b[0;32m    274\u001b[0m     dataset, return_epochs, return_raws, postprocess_pipeline\n\u001b[0;32m    275\u001b[0m )\n\u001b[0;32m    276\u001b[0m labels_pipeline \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_labels_pipeline(dataset, return_epochs, return_raws)\n\u001b[1;32m--> 278\u001b[0m data \u001b[39m=\u001b[39m [\n\u001b[0;32m    279\u001b[0m     dataset\u001b[39m.\u001b[39;49mget_data(\n\u001b[0;32m    280\u001b[0m         subjects\u001b[39m=\u001b[39;49msubjects,\n\u001b[0;32m    281\u001b[0m         cache_config\u001b[39m=\u001b[39;49mcache_config,\n\u001b[0;32m    282\u001b[0m         process_pipeline\u001b[39m=\u001b[39;49mprocess_pipeline,\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    284\u001b[0m     \u001b[39mfor\u001b[39;49;00m process_pipeline \u001b[39min\u001b[39;49;00m process_pipelines\n\u001b[0;32m    285\u001b[0m ]\n\u001b[0;32m    287\u001b[0m X \u001b[39m=\u001b[39m []\n\u001b[0;32m    288\u001b[0m labels \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moabb\\paradigms\\base.py:279\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    273\u001b[0m process_pipelines \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_process_pipelines(\n\u001b[0;32m    274\u001b[0m     dataset, return_epochs, return_raws, postprocess_pipeline\n\u001b[0;32m    275\u001b[0m )\n\u001b[0;32m    276\u001b[0m labels_pipeline \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_labels_pipeline(dataset, return_epochs, return_raws)\n\u001b[0;32m    278\u001b[0m data \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 279\u001b[0m     dataset\u001b[39m.\u001b[39;49mget_data(\n\u001b[0;32m    280\u001b[0m         subjects\u001b[39m=\u001b[39;49msubjects,\n\u001b[0;32m    281\u001b[0m         cache_config\u001b[39m=\u001b[39;49mcache_config,\n\u001b[0;32m    282\u001b[0m         process_pipeline\u001b[39m=\u001b[39;49mprocess_pipeline,\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    284\u001b[0m     \u001b[39mfor\u001b[39;00m process_pipeline \u001b[39min\u001b[39;00m process_pipelines\n\u001b[0;32m    285\u001b[0m ]\n\u001b[0;32m    287\u001b[0m X \u001b[39m=\u001b[39m []\n\u001b[0;32m    288\u001b[0m labels \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moabb\\datasets\\base.py:342\u001b[0m, in \u001b[0;36mBaseDataset.get_data\u001b[1;34m(self, subjects, cache_config, process_pipeline)\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[39mif\u001b[39;00m subject \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubject_list:\n\u001b[0;32m    341\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid subject \u001b[39m\u001b[39m{:d}\u001b[39;00m\u001b[39m given\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(subject))\n\u001b[1;32m--> 342\u001b[0m     data[subject] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_single_subject_data_using_cache(\n\u001b[0;32m    343\u001b[0m         subject,\n\u001b[0;32m    344\u001b[0m         cache_config,\n\u001b[0;32m    345\u001b[0m         process_pipeline,\n\u001b[0;32m    346\u001b[0m     )\n\u001b[0;32m    347\u001b[0m check_subject_names(data)\n\u001b[0;32m    348\u001b[0m check_session_names(data)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moabb\\datasets\\base.py:436\u001b[0m, in \u001b[0;36mBaseDataset._get_single_subject_data_using_cache\u001b[1;34m(self, subject, cache_config, process_pipeline)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[39m# Load and eventually overwrite:\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(cached_steps) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# last option: we don't use cache\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m     sessions_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_single_subject_data(subject)\n\u001b[0;32m    437\u001b[0m     \u001b[39massert\u001b[39;00m sessions_data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# should not happen\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Documents\\These\\moabb\\moabb\\datasets\\castillos2023.py:130\u001b[0m, in \u001b[0;36mBaseCastillos2023._get_single_subject_data\u001b[1;34m(self, subject)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_single_subject_data\u001b[39m(\u001b[39mself\u001b[39m, subject):\n\u001b[0;32m    129\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the data of a single subject.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     file_path_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_path(subject, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparadigm_type)\n\u001b[0;32m    131\u001b[0m     raw \u001b[39m=\u001b[39m mne\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mread_raw_eeglab(file_path_list[\u001b[39m0\u001b[39m], preload\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    133\u001b[0m     \u001b[39m# Strip the annotations that were script to make them easier to process\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\These\\moabb\\moabb\\datasets\\castillos2023.py:238\u001b[0m, in \u001b[0;36mBaseCastillos2023.data_path\u001b[1;34m(self, subject, paradigm_type, path, force_update, update_path, verbose)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (osp\u001b[39m.\u001b[39misdir(path_folder \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m4Class-VEP\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m    237\u001b[0m     zip_ref \u001b[39m=\u001b[39m z\u001b[39m.\u001b[39mZipFile(path_zip, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 238\u001b[0m     zip_ref\u001b[39m.\u001b[39;49mextractall(path_folder)\n\u001b[0;32m    240\u001b[0m subject_paths\u001b[39m.\u001b[39mappend(\n\u001b[0;32m    241\u001b[0m     path_folder\n\u001b[0;32m    242\u001b[0m     \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m4Class-CVEP/P\u001b[39m\u001b[39m{:d}\u001b[39;00m\u001b[39m/P\u001b[39m\u001b[39m{:d}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{:s}\u001b[39;00m\u001b[39m.set\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(subject, subject, paradigm_type)\n\u001b[0;32m    243\u001b[0m )\n\u001b[0;32m    245\u001b[0m \u001b[39mreturn\u001b[39;00m subject_paths\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1681\u001b[0m, in \u001b[0;36mZipFile.extractall\u001b[1;34m(self, path, members, pwd)\u001b[0m\n\u001b[0;32m   1678\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(path)\n\u001b[0;32m   1680\u001b[0m \u001b[39mfor\u001b[39;00m zipinfo \u001b[39min\u001b[39;00m members:\n\u001b[1;32m-> 1681\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_member(zipinfo, path, pwd)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1736\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[1;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[0;32m   1732\u001b[0m     \u001b[39mreturn\u001b[39;00m targetpath\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen(member, pwd\u001b[39m=\u001b[39mpwd) \u001b[39mas\u001b[39;00m source, \\\n\u001b[0;32m   1735\u001b[0m      \u001b[39mopen\u001b[39m(targetpath, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m target:\n\u001b[1;32m-> 1736\u001b[0m     shutil\u001b[39m.\u001b[39;49mcopyfileobj(source, target)\n\u001b[0;32m   1738\u001b[0m \u001b[39mreturn\u001b[39;00m targetpath\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:197\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[1;34m(fsrc, fdst, length)\u001b[0m\n\u001b[0;32m    195\u001b[0m fdst_write \u001b[39m=\u001b[39m fdst\u001b[39m.\u001b[39mwrite\n\u001b[0;32m    196\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m     buf \u001b[39m=\u001b[39m fsrc_read(length)\n\u001b[0;32m    198\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m buf:\n\u001b[0;32m    199\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:955\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    954\u001b[0m \u001b[39mwhile\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n\u001b[1;32m--> 955\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read1(n)\n\u001b[0;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[0;32m    957\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readbuffer \u001b[39m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1031\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_type \u001b[39m==\u001b[39m ZIP_DEFLATED:\n\u001b[0;32m   1030\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(n, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMIN_READ_SIZE)\n\u001b[1;32m-> 1031\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decompressor\u001b[39m.\u001b[39;49mdecompress(data, n)\n\u001b[0;32m   1032\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39meof \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m                  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_left \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m                  \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail)\n\u001b[0;32m   1035\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "if 'inter-session' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=subjects, return_epochs=False)\n",
    "\n",
    "    X_std = X.std(axis=0)\n",
    "    X /= X_std + 1e-8\n",
    "\n",
    "    xdawncov = XdawnCovariances(estimator=\"lwf\",xdawn_estimator=\"lwf\",nfilter=8)\n",
    "    X = xdawncov.fit_transform(X,labels)\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_spd['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "\n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg_spd['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg_spd['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg_spd['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg_spd['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = SPDSMNet2(**mdl_kwargs).to(device=device, dtype=cfg_spd['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_spd['epochs']-10,\n",
    "            bs0=cfg_spd['batch_size_train'],\n",
    "            bs=cfg_spd['batch_size_train']/cfg_spd['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=cfg_spd['epochs'],\n",
    "            min_epochs=cfg_spd['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_spd['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred = trainer.pred(net,dataloader=loader_test)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "\n",
    "        # extract model parameters\n",
    "        state_dict = deepcopy(net.state_dict())\n",
    "\n",
    "        # create new model and perform offline SF UDA\n",
    "        print(\"Perform UDA offline\")\n",
    "        sfuda_offline_net = SPDSMNet2(**mdl_kwargs).to(device=device)\n",
    "        sfuda_offline_net.load_state_dict(state_dict)\n",
    "        sfuda_offline(ds_test, sfuda_offline_ne, cfg_spdt)\n",
    "        res = trainer.test(sfuda_offline_net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(SFUDA)', domain=test_domain, **res))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = SPDSMNet2(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(onlineSFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "        # # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = SPDSMNet2(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online_simulate(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(online_sim_SFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "resdf = pd.DataFrame(records)    \n",
    "resdf.to_csv(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/SDPSMNet2_code.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSBNSDPBNNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "if 'inter-session' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=subjects, return_epochs=False)\n",
    "\n",
    "    X_std = X.std(axis=0)\n",
    "    X /= X_std + 1e-8\n",
    "\n",
    "    xdawncov = XdawnCovariances(estimator=\"lwf\",xdawn_estimator=\"lwf\",nfilter=8)\n",
    "    X = xdawncov.fit_transform(X,labels)\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_spd['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "\n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg_spd['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg_spd['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg_spd['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg_spd['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = DSBNSPDBNNet(**mdl_kwargs).to(device=device, dtype=cfg_spd['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_spd['epochs']-10,\n",
    "            bs0=cfg_spd['batch_size_train'],\n",
    "            bs=cfg_spd['batch_size_train']/cfg_spd['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=cfg_spd['epochs'],\n",
    "            min_epochs=cfg_spd['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_spd['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred = trainer.pred(net,dataloader=loader_test)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "\n",
    "        # extract model parameters\n",
    "        state_dict = deepcopy(net.state_dict())\n",
    "\n",
    "        # create new model and perform offline SF UDA\n",
    "        print(\"Perform UDA offline\")\n",
    "        sfuda_offline_net = SPDSMNet2(**mdl_kwargs).to(device=device)\n",
    "        sfuda_offline_net.load_state_dict(state_dict)\n",
    "        sfuda_offline(ds_test, sfuda_offline_net, cfg_spd)\n",
    "        res = trainer.test(sfuda_offline_net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(SFUDA)', domain=test_domain, **res))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = SPDSMNet2(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(onlineSFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "        # # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = SPDSMNet2(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online_simulate(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(online_sim_SFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "resdf = pd.DataFrame(records)    \n",
    "resdf.to_csv(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/DSBMSDPBNNet_code.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified TSMNET_visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\minimal_tsmnet.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# iterate over subject groups\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X33sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m ix_subset, subjects \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(subset_iter):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X33sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X33sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# get the data from the MOABB paradigm/dataset\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X33sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     X, labels, metadata \u001b[39m=\u001b[39m moabb_paradigm\u001b[39m.\u001b[39;49mget_data(moabb_ds, subjects\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m], return_epochs\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X33sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     X_std \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mstd(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X33sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     X \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m X_std \u001b[39m+\u001b[39m \u001b[39m1e-8\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moabb\\paradigms\\base.py:278\u001b[0m, in \u001b[0;36mBaseProcessing.get_data\u001b[1;34m(self, dataset, subjects, return_epochs, return_raws, cache_config, postprocess_pipeline)\u001b[0m\n\u001b[0;32m    273\u001b[0m process_pipelines \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_process_pipelines(\n\u001b[0;32m    274\u001b[0m     dataset, return_epochs, return_raws, postprocess_pipeline\n\u001b[0;32m    275\u001b[0m )\n\u001b[0;32m    276\u001b[0m labels_pipeline \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_labels_pipeline(dataset, return_epochs, return_raws)\n\u001b[1;32m--> 278\u001b[0m data \u001b[39m=\u001b[39m [\n\u001b[0;32m    279\u001b[0m     dataset\u001b[39m.\u001b[39;49mget_data(\n\u001b[0;32m    280\u001b[0m         subjects\u001b[39m=\u001b[39;49msubjects,\n\u001b[0;32m    281\u001b[0m         cache_config\u001b[39m=\u001b[39;49mcache_config,\n\u001b[0;32m    282\u001b[0m         process_pipeline\u001b[39m=\u001b[39;49mprocess_pipeline,\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    284\u001b[0m     \u001b[39mfor\u001b[39;49;00m process_pipeline \u001b[39min\u001b[39;49;00m process_pipelines\n\u001b[0;32m    285\u001b[0m ]\n\u001b[0;32m    287\u001b[0m X \u001b[39m=\u001b[39m []\n\u001b[0;32m    288\u001b[0m labels \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moabb\\paradigms\\base.py:279\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    273\u001b[0m process_pipelines \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_process_pipelines(\n\u001b[0;32m    274\u001b[0m     dataset, return_epochs, return_raws, postprocess_pipeline\n\u001b[0;32m    275\u001b[0m )\n\u001b[0;32m    276\u001b[0m labels_pipeline \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_labels_pipeline(dataset, return_epochs, return_raws)\n\u001b[0;32m    278\u001b[0m data \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 279\u001b[0m     dataset\u001b[39m.\u001b[39;49mget_data(\n\u001b[0;32m    280\u001b[0m         subjects\u001b[39m=\u001b[39;49msubjects,\n\u001b[0;32m    281\u001b[0m         cache_config\u001b[39m=\u001b[39;49mcache_config,\n\u001b[0;32m    282\u001b[0m         process_pipeline\u001b[39m=\u001b[39;49mprocess_pipeline,\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    284\u001b[0m     \u001b[39mfor\u001b[39;00m process_pipeline \u001b[39min\u001b[39;00m process_pipelines\n\u001b[0;32m    285\u001b[0m ]\n\u001b[0;32m    287\u001b[0m X \u001b[39m=\u001b[39m []\n\u001b[0;32m    288\u001b[0m labels \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moabb\\datasets\\base.py:342\u001b[0m, in \u001b[0;36mBaseDataset.get_data\u001b[1;34m(self, subjects, cache_config, process_pipeline)\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[39mif\u001b[39;00m subject \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubject_list:\n\u001b[0;32m    341\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid subject \u001b[39m\u001b[39m{:d}\u001b[39;00m\u001b[39m given\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(subject))\n\u001b[1;32m--> 342\u001b[0m     data[subject] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_single_subject_data_using_cache(\n\u001b[0;32m    343\u001b[0m         subject,\n\u001b[0;32m    344\u001b[0m         cache_config,\n\u001b[0;32m    345\u001b[0m         process_pipeline,\n\u001b[0;32m    346\u001b[0m     )\n\u001b[0;32m    347\u001b[0m check_subject_names(data)\n\u001b[0;32m    348\u001b[0m check_session_names(data)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moabb\\datasets\\base.py:436\u001b[0m, in \u001b[0;36mBaseDataset._get_single_subject_data_using_cache\u001b[1;34m(self, subject, cache_config, process_pipeline)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[39m# Load and eventually overwrite:\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(cached_steps) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# last option: we don't use cache\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m     sessions_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_single_subject_data(subject)\n\u001b[0;32m    437\u001b[0m     \u001b[39massert\u001b[39;00m sessions_data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# should not happen\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Documents\\These\\moabb\\moabb\\datasets\\castillos2023.py:130\u001b[0m, in \u001b[0;36mBaseCastillos2023._get_single_subject_data\u001b[1;34m(self, subject)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_single_subject_data\u001b[39m(\u001b[39mself\u001b[39m, subject):\n\u001b[0;32m    129\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the data of a single subject.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     file_path_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_path(subject, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparadigm_type)\n\u001b[0;32m    131\u001b[0m     raw \u001b[39m=\u001b[39m mne\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mread_raw_eeglab(file_path_list[\u001b[39m0\u001b[39m], preload\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    133\u001b[0m     \u001b[39m# Strip the annotations that were script to make them easier to process\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\These\\moabb\\moabb\\datasets\\castillos2023.py:238\u001b[0m, in \u001b[0;36mBaseCastillos2023.data_path\u001b[1;34m(self, subject, paradigm_type, path, force_update, update_path, verbose)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (osp\u001b[39m.\u001b[39misdir(path_folder \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m4Class-VEP\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m    237\u001b[0m     zip_ref \u001b[39m=\u001b[39m z\u001b[39m.\u001b[39mZipFile(path_zip, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 238\u001b[0m     zip_ref\u001b[39m.\u001b[39;49mextractall(path_folder)\n\u001b[0;32m    240\u001b[0m subject_paths\u001b[39m.\u001b[39mappend(\n\u001b[0;32m    241\u001b[0m     path_folder\n\u001b[0;32m    242\u001b[0m     \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m4Class-CVEP/P\u001b[39m\u001b[39m{:d}\u001b[39;00m\u001b[39m/P\u001b[39m\u001b[39m{:d}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{:s}\u001b[39;00m\u001b[39m.set\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(subject, subject, paradigm_type)\n\u001b[0;32m    243\u001b[0m )\n\u001b[0;32m    245\u001b[0m \u001b[39mreturn\u001b[39;00m subject_paths\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1681\u001b[0m, in \u001b[0;36mZipFile.extractall\u001b[1;34m(self, path, members, pwd)\u001b[0m\n\u001b[0;32m   1678\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(path)\n\u001b[0;32m   1680\u001b[0m \u001b[39mfor\u001b[39;00m zipinfo \u001b[39min\u001b[39;00m members:\n\u001b[1;32m-> 1681\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_member(zipinfo, path, pwd)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1736\u001b[0m, in \u001b[0;36mZipFile._extract_member\u001b[1;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[0;32m   1732\u001b[0m     \u001b[39mreturn\u001b[39;00m targetpath\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen(member, pwd\u001b[39m=\u001b[39mpwd) \u001b[39mas\u001b[39;00m source, \\\n\u001b[0;32m   1735\u001b[0m      \u001b[39mopen\u001b[39m(targetpath, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m target:\n\u001b[1;32m-> 1736\u001b[0m     shutil\u001b[39m.\u001b[39;49mcopyfileobj(source, target)\n\u001b[0;32m   1738\u001b[0m \u001b[39mreturn\u001b[39;00m targetpath\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:197\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[1;34m(fsrc, fdst, length)\u001b[0m\n\u001b[0;32m    195\u001b[0m fdst_write \u001b[39m=\u001b[39m fdst\u001b[39m.\u001b[39mwrite\n\u001b[0;32m    196\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m     buf \u001b[39m=\u001b[39m fsrc_read(length)\n\u001b[0;32m    198\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m buf:\n\u001b[0;32m    199\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:955\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    954\u001b[0m \u001b[39mwhile\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n\u001b[1;32m--> 955\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read1(n)\n\u001b[0;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[0;32m    957\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readbuffer \u001b[39m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1023\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail\n\u001b[0;32m   1022\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[1;32m-> 1023\u001b[0m         data \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read2(n \u001b[39m-\u001b[39;49m \u001b[39mlen\u001b[39;49m(data))\n\u001b[0;32m   1024\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1025\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read2(n)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1055\u001b[0m, in \u001b[0;36mZipExtFile._read2\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1052\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(n, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMIN_READ_SIZE)\n\u001b[0;32m   1053\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_left)\n\u001b[1;32m-> 1055\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fileobj\u001b[39m.\u001b[39;49mread(n)\n\u001b[0;32m   1056\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compress_left \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data)\n\u001b[0;32m   1057\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:776\u001b[0m, in \u001b[0;36m_SharedFile.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file\u001b[39m.\u001b[39mseek(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos)\n\u001b[0;32m    775\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file\u001b[39m.\u001b[39mread(n)\n\u001b[1;32m--> 776\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file\u001b[39m.\u001b[39mtell()\n\u001b[0;32m    777\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "if 'inter-session' in cfg_spd['evaluation']:\n",
    "    # subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    subset_iter = iter([[s] for s in [1,2,3]])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=[1,2,3], return_epochs=False)\n",
    "\n",
    "    X_std = X.std(axis=0)\n",
    "    X /= X_std + 1e-8\n",
    "\n",
    "    xdawncov = XdawnCovariances(estimator=\"lwf\",xdawn_estimator=\"lwf\",nfilter=8)\n",
    "    X = xdawncov.fit_transform(X,labels)\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_spd['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "    \n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "        visualisation_layers = []\n",
    "        visualisation_layers_val = []\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg_spd['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg_spd['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg_spd['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg_spd['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = SPDSMNet_visu(**mdl_kwargs).to(device=device, dtype=cfg_spd['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_spd['epochs']-10,\n",
    "            bs0=cfg_spd['batch_size_train'],\n",
    "            bs=cfg_spd['batch_size_train']/cfg_spd['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = VisuTrainer(\n",
    "            max_epochs=cfg_spd['epochs'],\n",
    "            min_epochs=cfg_spd['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_spd['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred, inter_layer_out = trainer.pred(net,dataloader=loader_test, return_interbn=True)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "        # Get visual of the data between the layers\n",
    "        visu_layer = []\n",
    "        ind_1 = np.random.choice(np.where(y[test].numpy()==1)[0],size=400,replace=False)\n",
    "        ind_0 = np.random.choice(np.where(y[test].numpy()==0)[0],size=400,replace=False)\n",
    "        for l in inter_layer_out[0]:\n",
    "            cov1 = np.array([l[i].cpu().numpy() for i in ind_1])\n",
    "            cov0 = np.array([l[i].cpu().numpy() for i in ind_0])\n",
    "            visu_layer.append(get_TSNE_visu(cov0,400))\n",
    "            visu_layer.append(get_TSNE_visu(cov1,400))\n",
    "\n",
    "        visualisation_layers.append(visu_layer)\n",
    "        # extract model parameters\n",
    "        val_pred, val_inter_layer_out = trainer.pred(net,dataloader=loader_val, return_interbn=True)\n",
    "        # Get visual of the data between the layers\n",
    "        visu_layer_val = []\n",
    "        ind_1 = np.where(y_fit[val].numpy()==1)[0]\n",
    "        ind_0 = np.where(y_fit[val].numpy()==0)[0]\n",
    "        for l in val_inter_layer_out[0]:\n",
    "            for d in np.unique(domain_fit[train].numpy()):\n",
    "                ind_dom1 = np.random.choice(np.where(domain_fit[val][ind_1].numpy()==d)[0],size=200,replace=False)\n",
    "                ind_dom0 = np.random.choice(np.where(domain_fit[val][ind_0].numpy()==d)[0],size=200,replace=False)\n",
    "                cov1 = np.array([l[i].cpu().numpy() for i in ind_dom1])\n",
    "                cov0 = np.array([l[i].cpu().numpy() for i in ind_dom0])\n",
    "                visu_layer_val.append(get_TSNE_visu(cov0,200))\n",
    "                visu_layer_val.append(get_TSNE_visu(cov1,200))\n",
    "\n",
    "        visualisation_layers_val.append(visu_layer_val)\n",
    "        \n",
    "\n",
    "        # # create new model and perform offline SF UDA\n",
    "        # print(\"Perform UDA offline\")\n",
    "        # sfuda_offline_net = SPDSMNet2(**mdl_kwargs).to(device=device)\n",
    "        # sfuda_offline_net.load_state_dict(state_dict)\n",
    "        # sfuda_offline(ds_test, sfuda_offline_net, cfg_spd)\n",
    "        # res = trainer.test(sfuda_offline_net, dataloader=loader_test)\n",
    "        # records.append(dict(mode='test(SFUDA)', domain=test_domain, **res))\n",
    "\n",
    "        # # create a new model and perform online SF UDA\n",
    "        # sfuda_online_net = SPDSMNet2(**mdl_kwargs)\n",
    "        # sfuda_online_net.load_state_dict(state_dict)\n",
    "        # loss, score = sfuda_online(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        # records.append(dict(mode='test(onlineSFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "        # # # create a new model and perform online SF UDA\n",
    "        # sfuda_online_net = SPDSMNet2(**mdl_kwargs)\n",
    "        # sfuda_online_net.load_state_dict(state_dict)\n",
    "        # loss, score = sfuda_online_simulate(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        # records.append(dict(mode='test(online_sim_SFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "        np.save(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet_test/SDPSMNet_visu_{}.npy\".format(ix_fold), np.array(visualisation_layers))\n",
    "        np.save(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet_test/SDPSMNet_val_visu_{}.npy\".format(ix_fold), np.array(visualisation_layers_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified DSBNSPDBNNet_visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\Documents\\These\\moabb\\moabb\\datasets\\castillos2023.py:131: RuntimeWarning: Data file name in EEG.data (P13_burst100.fdt) is incorrect, the file name must have changed on disk, using the correct file name (P6_burst100.fdt).\n",
      "  raw = mne.io.read_raw_eeglab(file_path_list[0], preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.velut\\Documents\\These\\moabb\\moabb\\datasets\\castillos2023.py:131: RuntimeWarning: Data file name in EEG.data (P13_burst100.fdt) is incorrect, the file name must have changed on disk, using the correct file name (P6_burst100.fdt).\n",
      "  raw = mne.io.read_raw_eeglab(file_path_list[0], preload=True, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test domain=['4/0']\n",
      "epoch=  0 gd-step=   59 trn_loss= 0.6849 trn_score=0.5871 val_loss= 0.6865 val_score=0.5771 \n",
      "epoch= 10 gd-step=  649 trn_loss= 0.6194 trn_score=0.6925 val_loss= 0.6254 val_score=0.6937 \n",
      "epoch= 19 gd-step= 1180 trn_loss= 0.5842 trn_score=0.6992 val_loss= 0.5924 val_score=0.6958 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.25\n",
      "test domain=['5/0']\n",
      "epoch=  0 gd-step=   59 trn_loss= 0.6912 trn_score=0.5265 val_loss= 0.6926 val_score=0.5083 \n",
      "epoch= 10 gd-step=  649 trn_loss= 0.6660 trn_score=0.6176 val_loss= 0.6714 val_score=0.5865 \n",
      "epoch= 19 gd-step= 1180 trn_loss= 0.6472 trn_score=0.6297 val_loss= 0.6546 val_score=0.6094 \n",
      "ES best epoch=17\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.3\n",
      "test domain=['6/0']\n",
      "epoch=  0 gd-step=   59 trn_loss= 0.6900 trn_score=0.5339 val_loss= 0.6910 val_score=0.5198 \n",
      "epoch= 10 gd-step=  649 trn_loss= 0.6707 trn_score=0.6017 val_loss= 0.6759 val_score=0.5833 \n",
      "epoch= 19 gd-step= 1180 trn_loss= 0.6554 trn_score=0.6205 val_loss= 0.6668 val_score=0.5875 \n",
      "ES best epoch=17\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.25\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "if 'inter-session' in cfg_spd['evaluation']:\n",
    "    # subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    subset_iter = iter([[s] for s in [[4,5,61,2,3]]])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    _,_, metadata = moabb_paradigm.get_data(moabb_ds, subjects=[1,2,3], return_epochs=False)\n",
    "    X = []\n",
    "    labels = []\n",
    "    for s in [1,2,3]:\n",
    "        X_temp, labels_temp, _ = moabb_paradigm.get_data(moabb_ds, subjects=[s], return_epochs=False)\n",
    "\n",
    "        X_std = X_temp.std(axis=0)\n",
    "        X_temp /= X_std + 1e-8\n",
    "\n",
    "        xdawncov = XdawnCovariances(estimator=\"lwf\",xdawn_estimator=\"lwf\",nfilter=8)\n",
    "        X_temp = xdawncov.fit_transform(X_temp,labels_temp)\n",
    "        X.append(X_temp)\n",
    "        labels.append(labels_temp)\n",
    "\n",
    "    X = np.concatenate(np.array(X))\n",
    "    labels = np.concatenate(np.array(labels))\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_spd['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "    \n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "        visualisation_layers = []\n",
    "        visualisation_layers_val = []\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg_spd['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg_spd['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg_spd['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg_spd['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = DSBNSPDBNNet_Visu(**mdl_kwargs).to(device=device, dtype=cfg_spd['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_spd['epochs']-10,\n",
    "            bs0=cfg_spd['batch_size_train'],\n",
    "            bs=cfg_spd['batch_size_train']/cfg_spd['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = VisuTrainer(\n",
    "            max_epochs=cfg_spd['epochs'],\n",
    "            min_epochs=cfg_spd['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_spd['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred, inter_layer_out = trainer.pred(net,dataloader=loader_test, return_interbn=True)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "        # Get visual of the data between the layers\n",
    "        val_pred, val_inter_layer_out = trainer.pred(net,dataloader=loader_val, return_interbn=True)\n",
    "        train_pred, train_inter_layer_out = trainer.pred(net,dataloader=torch.utils.data.DataLoader(ds_train, batch_size=len(ds_train),shuffle=False),\n",
    "                                                          return_interbn=True)\n",
    "        visu_layer = []\n",
    "        ind_1 = np.random.choice(np.where(y[test].numpy()==1)[0],size=150,replace=False)\n",
    "        ind_0 = np.random.choice(np.where(y[test].numpy()==0)[0],size=150,replace=False)\n",
    "        ind_1_val = np.where(y_fit[val].numpy()==1)[0]\n",
    "        ind_0_val = np.where(y_fit[val].numpy()==0)[0]\n",
    "        ind_1_train = np.where(y_fit[train].numpy()==1)[0]\n",
    "        ind_0_train = np.where(y_fit[train].numpy()==0)[0]\n",
    "        for l, l_val,l_train in zip(inter_layer_out[0],val_inter_layer_out[0],train_inter_layer_out[0]):\n",
    "            visu_layer = []\n",
    "            for d in np.unique(domain_fit[val].numpy()):\n",
    "                ind_dom1 = np.random.choice(np.where(domain_fit[val][ind_1_val].numpy()==d)[0],size=150,replace=False)\n",
    "                ind_dom0 = np.random.choice(np.where(domain_fit[val][ind_0_val].numpy()==d)[0],size=150,replace=False)\n",
    "                cov1_val = np.array([l_val[ind_1_val][i].cpu().numpy() for i in ind_dom1])\n",
    "                cov0_val = np.array([l_val[ind_0_val][i].cpu().numpy() for i in ind_dom0])\n",
    "                visu_layer.append(cov1_val)\n",
    "                visu_layer.append(cov0_val)\n",
    "            for d in np.unique(domain_fit[train].numpy()):\n",
    "                ind_dom1 = np.random.choice(np.where(domain_fit[train][ind_1_train].numpy()==d)[0],size=150,replace=False)\n",
    "                ind_dom0 = np.random.choice(np.where(domain_fit[train][ind_0_train].numpy()==d)[0],size=150,replace=False)\n",
    "                cov1_train = np.array([l_train[ind_1_train][i].cpu().numpy() for i in ind_dom1])\n",
    "                cov0_train = np.array([l_train[ind_0_train][i].cpu().numpy() for i in ind_dom0])\n",
    "                visu_layer.append(cov1_train)\n",
    "                visu_layer.append(cov0_train)\n",
    "            cov1 = np.array([l[i].cpu().numpy() for i in ind_1])\n",
    "            cov0 = np.array([l[i].cpu().numpy() for i in ind_0])\n",
    "            visu_layer.append(cov0)\n",
    "            visu_layer.append(cov1)\n",
    "\n",
    "            visu_layer = np.concatenate(visu_layer)\n",
    "            to_visualize = TangentSpace().fit_transform(visu_layer)\n",
    "            X_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=300).fit_transform(to_visualize)\n",
    "            visualisation_layers.append(X_embedded)\n",
    "        # visualisation_layers.append(get_TSNE_visu(visu_layer,visu_layer.shape[0]))\n",
    "        \n",
    "\n",
    "        np.save(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet_test/DSBNSPDBNNet_visu_{}.npy\".format(ix_fold), np.array(visualisation_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 14037, 14038, 14039])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified DSBNSPDBNNet_visu LOOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Choosing the first None classes from all possible events.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "C:\\Users\\s.velut\\mne_data\\MNE-4class-vep-data\\records\\8255618\\files\\\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "[1, 2, 3]\n",
      "[1, 2, 3]\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 25 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 25.00 Hz\n",
      "- Upper transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 28.12 Hz)\n",
      "- Filter length: 1651 samples (3.302 s)\n",
      "\n",
      "7020 events found\n",
      "Event IDs: [100 101]\n",
      "Not setting metadata\n",
      "7020 matching events found\n",
      "Setting baseline interval to [-0.01, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 7020 events and 131 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "subjects = [1,2,3]\n",
    "# subjects = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "n_channels = 32\n",
    "on_frame = True\n",
    "if on_frame:\n",
    "    freq = 60\n",
    "else:\n",
    "    freq = 500\n",
    "\n",
    "raw_data,labels,codes,labels_codes = get_BVEP_data(subjects,on_frame)\n",
    "X_parent, labels_parent, domains_parent = prepare_data(subjects,raw_data, labels, on_frame,True,False,codes)\n",
    "metadata = pd.DataFrame({\"subject\":np.repeat(list(map(str,subjects)),X_parent[0].shape[0]),\"session\":[\"0\"]*len(subjects)*X_parent[0].shape[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test domain=['1/0']\n",
      "epoch=  0 gd-step=   30 trn_loss= 0.6901 trn_score=0.5439 val_loss= 0.6901 val_score=0.5509 \n",
      "epoch= 10 gd-step=  330 trn_loss= 0.6803 trn_score=0.5931 val_loss= 0.6835 val_score=0.5670 \n",
      "epoch= 19 gd-step=  600 trn_loss= 0.6712 trn_score=0.6153 val_loss= 0.6799 val_score=0.5527 \n",
      "ES best epoch=16\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.4\n",
      "test domain=['2/0']\n",
      "epoch=  0 gd-step=   29 trn_loss= 0.6932 trn_score=0.5041 val_loss= 0.6945 val_score=0.4821 \n",
      "epoch= 10 gd-step=  319 trn_loss= 0.6732 trn_score=0.6196 val_loss= 0.6751 val_score=0.6161 \n",
      "epoch= 19 gd-step=  580 trn_loss= 0.6570 trn_score=0.6244 val_loss= 0.6549 val_score=0.6366 \n",
      "ES best epoch=19\n",
      "evaluate the estimator\n",
      " accuracy score of the participant 0.6\n",
      "test domain=['3/0']\n",
      "epoch=  0 gd-step=   30 trn_loss=    nan trn_score=0.5000 val_loss=    nan val_score=0.5000 \n",
      "epoch= 10 gd-step=  330 trn_loss=    nan trn_score=0.5000 val_loss=    nan val_score=0.5000 \n",
      "epoch= 19 gd-step=  600 trn_loss=    nan trn_score=0.5000 val_loss=    nan val_score=0.5000 \n",
      "ES best epoch=-1\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score of the participant 0.25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Matrices must be positive definite. Add regularization to avoid this error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\minimal_tsmnet.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X66sZmlsZQ%3D%3D?line=180'>181</a>\u001b[0m visu_layer\u001b[39m.\u001b[39mappend(cov1)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X66sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m visu_layer \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(visu_layer)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X66sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m to_visualize \u001b[39m=\u001b[39m TangentSpace()\u001b[39m.\u001b[39;49mfit_transform(visu_layer)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X66sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m X_embedded \u001b[39m=\u001b[39m TSNE(n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m,init\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrandom\u001b[39m\u001b[39m'\u001b[39m, perplexity\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\u001b[39m.\u001b[39mfit_transform(to_visualize)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X66sZmlsZQ%3D%3D?line=185'>186</a>\u001b[0m visualisation_layers\u001b[39m.\u001b[39mappend(X_embedded)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyriemann\\tangentspace.py:200\u001b[0m, in \u001b[0;36mTangentSpace.fit_transform\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit and transform in a single function.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39m    Tangent space projections of SPD matrices.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric_mean, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_metric(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric)\n\u001b[1;32m--> 200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreference_ \u001b[39m=\u001b[39m mean_covariance(\n\u001b[0;32m    201\u001b[0m     X,\n\u001b[0;32m    202\u001b[0m     metric\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetric_mean,\n\u001b[0;32m    203\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[0;32m    204\u001b[0m )\n\u001b[0;32m    205\u001b[0m \u001b[39mreturn\u001b[39;00m tangent_space(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreference_, metric\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric_map)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyriemann\\utils\\mean.py:677\u001b[0m, in \u001b[0;36mmean_covariance\u001b[1;34m(X, metric, sample_weight, covmats, **kwargs)\u001b[0m\n\u001b[0;32m    675\u001b[0m X \u001b[39m=\u001b[39m _deprecate_covmats(covmats, X)\n\u001b[0;32m    676\u001b[0m mean_function \u001b[39m=\u001b[39m _check_mean_function(metric)\n\u001b[1;32m--> 677\u001b[0m M \u001b[39m=\u001b[39m mean_function(\n\u001b[0;32m    678\u001b[0m     X,\n\u001b[0;32m    679\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    680\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    681\u001b[0m )\n\u001b[0;32m    682\u001b[0m \u001b[39mreturn\u001b[39;00m M\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyriemann\\utils\\mean.py:526\u001b[0m, in \u001b[0;36mmean_riemann\u001b[1;34m(X, tol, maxiter, init, sample_weight, covmats)\u001b[0m\n\u001b[0;32m    524\u001b[0m crit \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfinfo(np\u001b[39m.\u001b[39mfloat64)\u001b[39m.\u001b[39mmax\n\u001b[0;32m    525\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(maxiter):\n\u001b[1;32m--> 526\u001b[0m     M12, Mm12 \u001b[39m=\u001b[39m sqrtm(M), invsqrtm(M)\n\u001b[0;32m    527\u001b[0m     J \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39ma,abc->bc\u001b[39m\u001b[39m'\u001b[39m, sample_weight, logm(Mm12 \u001b[39m@\u001b[39m X \u001b[39m@\u001b[39m Mm12))\n\u001b[0;32m    528\u001b[0m     M \u001b[39m=\u001b[39m M12 \u001b[39m@\u001b[39m expm(nu \u001b[39m*\u001b[39m J) \u001b[39m@\u001b[39m M12\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyriemann\\utils\\base.py:154\u001b[0m, in \u001b[0;36msqrtm\u001b[1;34m(C)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msqrtm\u001b[39m(C):\n\u001b[0;32m    132\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Square root of SPD/HPD matrices.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \n\u001b[0;32m    134\u001b[0m \u001b[39m    The symmetric matrix square root of a SPD/HPD matrix\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39m        Matrix square root of C.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m     \u001b[39mreturn\u001b[39;00m _matrix_operator(C, np\u001b[39m.\u001b[39;49msqrt)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyriemann\\utils\\base.py:14\u001b[0m, in \u001b[0;36m_matrix_operator\u001b[1;34m(C, operator)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInput must be at least a 2D ndarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[39mif\u001b[39;00m C\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mchar \u001b[39min\u001b[39;00m typecodes[\u001b[39m'\u001b[39m\u001b[39mAllFloat\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mand\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m         np\u001b[39m.\u001b[39misinf(C)\u001b[39m.\u001b[39many() \u001b[39mor\u001b[39;00m np\u001b[39m.\u001b[39misnan(C)\u001b[39m.\u001b[39many()):\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     15\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMatrices must be positive definite. Add \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mregularization to avoid this error.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m eigvals, eigvecs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39meigh(C)\n\u001b[0;32m     18\u001b[0m eigvals \u001b[39m=\u001b[39m operator(eigvals)\n",
      "\u001b[1;31mValueError\u001b[0m: Matrices must be positive definite. Add regularization to avoid this error."
     ]
    }
   ],
   "source": [
    "records = []\n",
    "n_cal = 5\n",
    "\n",
    "if 'inter-session' in cfg_spd['evaluation']:\n",
    "    # subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    subset_iter = iter([[s] for s in [4,5,6]])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    X = np.concatenate(X_parent)\n",
    "    labels = np.concatenate(labels_parent)\n",
    "    domains = np.concatenate(domains_parent)\n",
    "\n",
    "    X_std = X.std(axis=0)\n",
    "    X /= X_std + 1e-8\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_spd['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "\n",
    "\n",
    "    \n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "        fit = np.concatenate([fit,test[:int(1.95*n_cal*4*60)]])\n",
    "        test = test[int(1.95*n_cal*4*60):]\n",
    "        visualisation_layers = []\n",
    "        visualisation_layers_val = []\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg_spd['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg_spd['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg_spd['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg_spd['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = DSBNSPDBNNet_Visu(**mdl_kwargs).to(device=device, dtype=cfg_spd['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_spd['epochs']-10,\n",
    "            bs0=cfg_spd['batch_size_train'],\n",
    "            bs=cfg_spd['batch_size_train']/cfg_spd['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = VisuTrainer(\n",
    "            max_epochs=cfg_spd['epochs'],\n",
    "            min_epochs=cfg_spd['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_spd['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred, inter_layer_out = trainer.pred(net,dataloader=loader_test, return_interbn=True)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][n_cal*4:][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "        # Get visual of the data between the layers\n",
    "        val_pred, val_inter_layer_out = trainer.pred(net,dataloader=loader_val, return_interbn=True)\n",
    "        train_pred, train_inter_layer_out = trainer.pred(net,dataloader=torch.utils.data.DataLoader(ds_train, batch_size=len(ds_train),shuffle=False),\n",
    "                                                          return_interbn=True)\n",
    "        visu_layer = []\n",
    "        ind_1 = np.random.choice(np.where(y[test].numpy()==1)[0],size=50,replace=False)\n",
    "        ind_0 = np.random.choice(np.where(y[test].numpy()==0)[0],size=50,replace=False)\n",
    "        ind_1_val = np.where(y_fit[val].numpy()==1)[0]\n",
    "        ind_0_val = np.where(y_fit[val].numpy()==0)[0]\n",
    "        ind_1_train = np.where(y_fit[train].numpy()==1)[0]\n",
    "        ind_0_train = np.where(y_fit[train].numpy()==0)[0]\n",
    "        for l, l_val,l_train in zip(inter_layer_out[0],val_inter_layer_out[0],train_inter_layer_out[0]):\n",
    "            visu_layer = []\n",
    "            for d in np.unique(domain_fit[val].numpy()):\n",
    "                ind_dom1 = np.random.choice(np.where(domain_fit[val][ind_1_val].numpy()==d)[0],size=50,replace=False)\n",
    "                ind_dom0 = np.random.choice(np.where(domain_fit[val][ind_0_val].numpy()==d)[0],size=50,replace=False)\n",
    "                cov1_val = np.array([l_val[ind_1_val][i].cpu().numpy() for i in ind_dom1])\n",
    "                cov0_val = np.array([l_val[ind_0_val][i].cpu().numpy() for i in ind_dom0])\n",
    "                visu_layer.append(cov1_val)\n",
    "                visu_layer.append(cov0_val)\n",
    "            for d in np.unique(domain_fit[train].numpy()):\n",
    "                ind_dom1 = np.random.choice(np.where(domain_fit[train][ind_1_train].numpy()==d)[0],size=50,replace=False)\n",
    "                ind_dom0 = np.random.choice(np.where(domain_fit[train][ind_0_train].numpy()==d)[0],size=50,replace=False)\n",
    "                cov1_train = np.array([l_train[ind_1_train][i].cpu().numpy() for i in ind_dom1])\n",
    "                cov0_train = np.array([l_train[ind_0_train][i].cpu().numpy() for i in ind_dom0])\n",
    "                visu_layer.append(cov1_train)\n",
    "                visu_layer.append(cov0_train)\n",
    "            cov1 = np.array([l[i].cpu().numpy() for i in ind_1])\n",
    "            cov0 = np.array([l[i].cpu().numpy() for i in ind_0])\n",
    "            visu_layer.append(cov0)\n",
    "            visu_layer.append(cov1)\n",
    "\n",
    "            visu_layer = np.concatenate(visu_layer)\n",
    "            to_visualize = TangentSpace().fit_transform(visu_layer)\n",
    "            X_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=100).fit_transform(to_visualize)\n",
    "            visualisation_layers.append(X_embedded)\n",
    "        # visualisation_layers.append(get_TSNE_visu(visu_layer,visu_layer.shape[0]))\n",
    "        \n",
    "\n",
    "        np.save(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet_test/DSBNSPDBNNet_visu_{}.npy\".format(ix_fold), np.array(visualisation_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(domain_fit[val].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original TSMNET on samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "subjects = [1,2,3]\n",
    "# subjects = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "n_channels = 32\n",
    "on_frame = False\n",
    "if on_frame:\n",
    "    freq = 60\n",
    "else:\n",
    "    freq = 500\n",
    "\n",
    "raw_data,labels,codes,labels_codes = get_BVEP_data(subjects,on_frame)\n",
    "X_parent, labels_parent, domains_parent = prepare_data(subjects,raw_data, labels, on_frame,False,codes)\n",
    "metadata = pd.DataFrame({\"subject\":np.repeat(list(map(str,subjects)),X_parent[0].shape[0]),\"session\":[\"0\"]*len(subjects)*X_parent[0].shape[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test domain=['1/0']\n",
      "epoch=  0 gd-step=  498 trn_loss= 0.6509 trn_score=0.6513 val_loss= 0.6569 val_score=0.6383 \n",
      "epoch= 10 gd-step= 5478 trn_loss= 0.4250 trn_score=0.8138 val_loss= 0.4748 val_score=0.7852 \n",
      "epoch= 19 gd-step= 9960 trn_loss= 0.4401 trn_score=0.7983 val_loss= 0.4919 val_score=0.7658 \n",
      "ES best epoch=18\n",
      "evaluate the estimator\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.49 GiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.15 GiB is allocated by PyTorch, and 32.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\minimal_tsmnet.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X45sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m res \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mtest(net, dataloader\u001b[39m=\u001b[39mloader_val)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X45sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m records\u001b[39m.\u001b[39mappend(\u001b[39mdict\u001b[39m(mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m, domain\u001b[39m=\u001b[39mtest_domain, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mres))\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X45sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m res \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtest(net, dataloader\u001b[39m=\u001b[39;49mloader_test)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X45sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m records\u001b[39m.\u001b[39mappend(\u001b[39mdict\u001b[39m(mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest(noUDA)\u001b[39m\u001b[39m'\u001b[39m, domain\u001b[39m=\u001b[39mtest_domain, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mres))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X45sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m y_pred \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mpred(net,dataloader\u001b[39m=\u001b[39mloader_test)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\trainer.py:106\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, model, dataloader)\u001b[0m\n\u001b[0;32m    104\u001b[0m features[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype_, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_)\n\u001b[0;32m    105\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_)\n\u001b[1;32m--> 106\u001b[0m pred \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfeatures)\n\u001b[0;32m    107\u001b[0m loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(pred, y)\u001b[39m.\u001b[39mitem()\n\u001b[0;32m    108\u001b[0m y_true\u001b[39m.\u001b[39mappend(y)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\models.py:87\u001b[0m, in \u001b[0;36mTSMNet.forward\u001b[1;34m(self, inputs, domains)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs, domains):\n\u001b[1;32m---> 87\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcnn(inputs[:,\u001b[39mNone\u001b[39;49;00m,\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m])\n\u001b[0;32m     88\u001b[0m     C \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_pooling(h)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspdnet[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mW)\n\u001b[0;32m     89\u001b[0m     l \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspdnet(C)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_conv_forward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, weight: Tensor, bias: Optional[Tensor]):\n\u001b[0;32m    452\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 453\u001b[0m         \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(F\u001b[39m.\u001b[39;49mpad(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_mode),\n\u001b[0;32m    454\u001b[0m                         weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    455\u001b[0m                         _pair(\u001b[39m0\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.49 GiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.15 GiB is allocated by PyTorch, and 32.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "if 'inter-session' in cfg_org['evaluation']:\n",
    "    subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg_org['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    # X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=subjects, return_epochs=False)\n",
    "    X = np.concatenate(X_parent)\n",
    "    labels = np.concatenate(labels_parent)\n",
    "    domains = np.concatenate(domains_parent)\n",
    "\n",
    "    X_std = X.std(axis=0)\n",
    "    X /= X_std + 1e-8\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_org['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "\n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg_org['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg_org['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg_org['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg_org['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = TSMNet(**mdl_kwargs).to(device=device, dtype=cfg_org['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_org['epochs']-10,\n",
    "            bs0=cfg_org['batch_size_train'],\n",
    "            bs=cfg_org['batch_size_train']/cfg_org['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=cfg_org['epochs'],\n",
    "            min_epochs=cfg_org['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_org['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred = trainer.pred(net,dataloader=loader_test)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "\n",
    "        # extract model parameters\n",
    "        state_dict = deepcopy(net.state_dict())\n",
    "\n",
    "        # create new model and perform offline SF UDA\n",
    "        print(\"Perform UDA offline\")\n",
    "        sfuda_offline_net = TSMNet(**mdl_kwargs).to(device=device)\n",
    "        sfuda_offline_net.load_state_dict(state_dict)\n",
    "        sfuda_offline(ds_test, sfuda_offline_net)\n",
    "        res = trainer.test(sfuda_offline_net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(SFUDA)', domain=test_domain, **res))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = TSMNet(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online(ds_test, sfuda_online_net, cfg_org, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(onlineSFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        sfuda_online_net = TSMNet(**mdl_kwargs)\n",
    "        sfuda_online_net.load_state_dict(state_dict)\n",
    "        loss, score = sfuda_online_simulate(ds_test, sfuda_online_net, cfg_org, trainer.loss_fn)\n",
    "        records.append(dict(mode='test(online_sim_SFUDA)', domain=test_domain, loss=loss, score=score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified TSMNET on samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "subjects = [1,2,3]\n",
    "# subjects = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "n_channels = 32\n",
    "on_frame = False\n",
    "if on_frame:\n",
    "    freq = 60\n",
    "else:\n",
    "    freq = 500\n",
    "\n",
    "raw_data,labels,codes,labels_codes = get_BVEP_data(subjects,on_frame)\n",
    "X_parent, labels_parent, domains_parent = prepare_data(subjects,raw_data, labels, on_frame,True,codes)\n",
    "metadata = pd.DataFrame({\"subject\":np.repeat(list(map(str,subjects)),X_parent[0].shape[0]),\"session\":[\"0\"]*len(subjects)*X_parent[0].shape[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175495</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175496</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175497</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175498</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175499</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175500 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject session\n",
       "0            1       0\n",
       "1            1       0\n",
       "2            1       0\n",
       "3            1       0\n",
       "4            1       0\n",
       "...        ...     ...\n",
       "175495       3       0\n",
       "175496       3       0\n",
       "175497       3       0\n",
       "175498       3       0\n",
       "175499       3       0\n",
       "\n",
       "[175500 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metadata = pd.DataFrame({\"subject\":np.repeat(list(map(str,subjects)),X[0].shape[0]),\"session\":[\"0\"]*len(subjects)*X[0].shape[0]})\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test domain=['1/0']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\minimal_tsmnet.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X30sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39m# fit the model\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X30sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest domain=\u001b[39m\u001b[39m{\u001b[39;00mtest_domain\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X30sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(net, train_dataloader\u001b[39m=\u001b[39;49mloader_train, val_dataloader\u001b[39m=\u001b[39;49mloader_val)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X30sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mES best epoch=\u001b[39m\u001b[39m{\u001b[39;00mes\u001b[39m.\u001b[39mbest_epoch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X30sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m fit_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(trainer\u001b[39m.\u001b[39mrecords)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\trainer.py:43\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloader, val_dataloader)\u001b[0m\n\u001b[0;32m     40\u001b[0m [callback\u001b[39m.\u001b[39mon_train_epoch_start(\u001b[39mself\u001b[39m, model) \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks]\n\u001b[0;32m     42\u001b[0m \u001b[39m# print(\"epochs training\")\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch(model, train_dataloader)\n\u001b[0;32m     45\u001b[0m trn_res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest(model, train_dataloader)\n\u001b[0;32m     46\u001b[0m trn_res \u001b[39m=\u001b[39m {\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrn_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m trn_res\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\trainer.py:84\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[1;34m(self, model, train_dataloader)\u001b[0m\n\u001b[0;32m     82\u001b[0m features[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype_, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_)\n\u001b[0;32m     83\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_)\n\u001b[1;32m---> 84\u001b[0m pred \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfeatures)\n\u001b[0;32m     85\u001b[0m \u001b[39m# print(\"Pred is :\", pred[0])\u001b[39;00m\n\u001b[0;32m     86\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(pred, y)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\models.py:206\u001b[0m, in \u001b[0;36mSPDSMNet.forward\u001b[1;34m(self, inputs, domains)\u001b[0m\n\u001b[0;32m    204\u001b[0m l \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbimap_layers[i](l)\n\u001b[0;32m    205\u001b[0m \u001b[39m# print(l.size())\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m l \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspdbnorm_layers[i](l,domains) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdomain_adaptation_ \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspdbnorm(l)\n\u001b[0;32m    207\u001b[0m \u001b[39m# print(l.size())\u001b[39;00m\n\u001b[0;32m    208\u001b[0m l \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mReEig_layers[i](l)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\batchnorm.py:83\u001b[0m, in \u001b[0;36mBaseDomainBatchNorm.forward\u001b[1;34m(self, X, d)\u001b[0m\n\u001b[0;32m     80\u001b[0m du \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39munique()\n\u001b[0;32m     82\u001b[0m X_normalized \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mempty_like(X)\n\u001b[1;32m---> 83\u001b[0m res \u001b[39m=\u001b[39m [(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_domain_(X[d\u001b[39m==\u001b[39;49mdomain], domain),torch\u001b[39m.\u001b[39;49mnonzero(d\u001b[39m==\u001b[39;49mdomain))\n\u001b[0;32m     84\u001b[0m         \u001b[39mfor\u001b[39;49;00m domain \u001b[39min\u001b[39;49;00m du]\n\u001b[0;32m     85\u001b[0m X_out, ixs \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mres)\n\u001b[0;32m     86\u001b[0m X_out, ixs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(X_out), torch\u001b[39m.\u001b[39mcat(ixs)\u001b[39m.\u001b[39mflatten()\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\batchnorm.py:83\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     80\u001b[0m du \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39munique()\n\u001b[0;32m     82\u001b[0m X_normalized \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mempty_like(X)\n\u001b[1;32m---> 83\u001b[0m res \u001b[39m=\u001b[39m [(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_domain_(X[d\u001b[39m==\u001b[39;49mdomain], domain),torch\u001b[39m.\u001b[39mnonzero(d\u001b[39m==\u001b[39mdomain))\n\u001b[0;32m     84\u001b[0m         \u001b[39mfor\u001b[39;00m domain \u001b[39min\u001b[39;00m du]\n\u001b[0;32m     85\u001b[0m X_out, ixs \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mres)\n\u001b[0;32m     86\u001b[0m X_out, ixs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(X_out), torch\u001b[39m.\u001b[39mcat(ixs)\u001b[39m.\u001b[39mflatten()\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\batchnorm.py:76\u001b[0m, in \u001b[0;36mBaseDomainBatchNorm.forward_domain_\u001b[1;34m(self, X, domain)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_domain_\u001b[39m(\u001b[39mself\u001b[39m, X, domain):\n\u001b[1;32m---> 76\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatchnorm[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain_to_key(domain)](X)\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\batchnorm.py:162\u001b[0m, in \u001b[0;36mSPDBatchNormImpl.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    159\u001b[0m         batch_mean \u001b[39m=\u001b[39m bm_sq \u001b[39m@\u001b[39m functionals\u001b[39m.\u001b[39msym_expm\u001b[39m.\u001b[39mapply(GT) \u001b[39m@\u001b[39m bm_sq\n\u001b[0;32m    161\u001b[0m \u001b[39m# update the running mean\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m rm \u001b[39m=\u001b[39m functionals\u001b[39m.\u001b[39;49mspd_2point_interpolation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean, batch_mean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meta)\n\u001b[0;32m    164\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispersion \u001b[39mis\u001b[39;00m BatchNormDispersion\u001b[39m.\u001b[39mSCALAR:\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mshape[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchdim] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\spdnets\\functionals.py:27\u001b[0m, in \u001b[0;36mspd_2point_interpolation\u001b[1;34m(A, B, t)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mspd_2point_interpolation\u001b[39m(A : Tensor, B : Tensor, t : Number) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m     26\u001b[0m     rm_sq, rm_invsq \u001b[39m=\u001b[39m sym_invsqrtm2\u001b[39m.\u001b[39mapply(A)\n\u001b[1;32m---> 27\u001b[0m     \u001b[39mreturn\u001b[39;00m rm_sq \u001b[39m@\u001b[39m sym_powm\u001b[39m.\u001b[39mapply(rm_invsq \u001b[39m@\u001b[39;49m B \u001b[39m@\u001b[39m rm_invsq, torch\u001b[39m.\u001b[39mtensor(t)) \u001b[39m@\u001b[39m rm_sq\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_bmm)"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "if 'inter-session' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([[s] for s in moabb_ds.subject_list])\n",
    "    groupvarname = 'session'\n",
    "elif 'inter-subject' in cfg_spd['evaluation']:\n",
    "    subset_iter = iter([None])\n",
    "    groupvarname = 'subject'\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "fit_records = []\n",
    "\n",
    "# iterate over subject groups\n",
    "for ix_subset, subjects in enumerate(subset_iter):\n",
    "\n",
    "    # get the data from the MOABB paradigm/dataset\n",
    "    # X, labels, metadata = moabb_paradigm.get_data(moabb_ds, subjects=subjects, return_epochs=False)\n",
    "    X = np.concatenate(X_parent)\n",
    "    labels = np.concatenate(labels_parent)\n",
    "    domains = np.concatenate(domains_parent)\n",
    "\n",
    "    X_std = X.std(axis=0)\n",
    "    X /= X_std + 1e-8\n",
    "\n",
    "    # xdawncov = XdawnCovariances(estimator=\"lwf\",xdawn_estimator=\"lwf\",nfilter=8)\n",
    "    # X = xdawncov.fit_transform(X,labels)\n",
    "\n",
    "    # extract domains = subject/session\n",
    "    metadata['label'] = labels\n",
    "    metadata['domain'] = metadata.apply(lambda row: f'{row.subject}/{row.session}',  axis=1)\n",
    "    domain = sklearn.preprocessing.LabelEncoder().fit_transform(metadata['domain'])\n",
    "\n",
    "    # convert to torch tensors\n",
    "    domain = torch.from_numpy(domain)\n",
    "    X = torch.from_numpy(X)\n",
    "    y = sklearn.preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    # leave one subject or session out\n",
    "    cv_outer = sklearn.model_selection.LeaveOneGroupOut()\n",
    "    cv_outer_group = metadata[groupvarname]\n",
    "\n",
    "    # train/validation split stratified across domains and labels\n",
    "    cv_inner_group = metadata.apply(lambda row: f'{row.domain}/{row.label}',  axis=1)\n",
    "    cv_inner_group = sklearn.preprocessing.LabelEncoder().fit_transform(cv_inner_group)\n",
    "\n",
    "    # add datadependen model kwargs\n",
    "    mdl_kwargs = deepcopy(cfg_spd['mdl_kwargs'])\n",
    "    mdl_kwargs['nclasses'] = n_classes\n",
    "    mdl_kwargs['nchannels'] = X.shape[1]\n",
    "    mdl_kwargs['nsamples'] = X.shape[2]\n",
    "    mdl_kwargs['domains'] = domain.unique()\n",
    "\n",
    "    # perform outer CV\n",
    "    for ix_fold, (fit, test) in enumerate(cv_outer.split(X, y, cv_outer_group)):\n",
    "\n",
    "        # Balancing the data\n",
    "        rus = RandomUnderSampler()\n",
    "        counter=np.array(range(0,len(y[fit]))).reshape(-1,1)\n",
    "        index,_ = rus.fit_resample(counter,y[fit][:])\n",
    "        index = np.sort(index,axis=0)\n",
    "        X_fit = np.squeeze(X[fit][index,:,:], axis=1)\n",
    "        y_fit = np.squeeze(y[fit][index])\n",
    "        domain_fit = np.squeeze(domain[fit][index])\n",
    "        metadata_fit = metadata.loc[fit].iloc[np.concatenate(index)]\n",
    "\n",
    "        # split fitting data into train and validation \n",
    "        cv_inner = sklearn.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=cfg_spd['validation_size'])\n",
    "        train, val = next(cv_inner.split(X_fit, y_fit, np.squeeze(cv_inner_group[fit][index])))\n",
    "\n",
    "        # adjust number of \n",
    "        du = domain_fit[train].unique()\n",
    "        if cfg_spd['domains_per_batch'] > len(du):\n",
    "            domains_per_batch = len(du)\n",
    "        else:\n",
    "            domains_per_batch = cfg_spd['domains_per_batch']\n",
    "\n",
    "        # split entire dataset into train/validation/test\n",
    "        ds_train = DomainDataset(X_fit[train], y_fit[train], domain_fit[train], metadata_fit.iloc[train,:])\n",
    "        ds_val = DomainDataset(X_fit[val], y_fit[val], domain_fit[val], metadata_fit.iloc[val,:])\n",
    "        ds_test = DomainDataset(X[test], y[test], domain[test], metadata.iloc[test,:])\n",
    "\n",
    "        # create dataloaders\n",
    "        # for training use specific loader/sampler so taht \n",
    "        # batches contain a specific number of domains with equal observations per domain\n",
    "        # and stratified labels\n",
    "        loader_train = StratifiedDomainDataLoader(ds_train, cfg_spd['batch_size_train'], domains_per_batch=domains_per_batch, shuffle=True)\n",
    "        loader_val = torch.utils.data.DataLoader(ds_val, batch_size=len(ds_val))\n",
    "        loader_test = torch.utils.data.DataLoader(ds_test, batch_size=len(ds_test))\n",
    "\n",
    "        # extract domains in the test dataset\n",
    "        test_domain = metadata['domain'].iloc[test].unique()\n",
    "\n",
    "        # create the model\n",
    "        net = SPDSMNet(**mdl_kwargs).to(device=device, dtype=cfg_spd['dtype'])\n",
    "\n",
    "        # create the momentum scheduler\n",
    "        bn_sched = MomentumBatchNormScheduler(\n",
    "            epochs=cfg_spd['epochs']-10,\n",
    "            bs0=cfg_spd['batch_size_train'],\n",
    "            bs=cfg_spd['batch_size_train']/cfg_spd['domains_per_batch'], \n",
    "            tau0=0.85\n",
    "        )\n",
    "\n",
    "        es = EarlyStopping(metric='val_loss', higher_is_better=False, patience=15, verbose=False)\n",
    "        \n",
    "        # create the trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=cfg_spd['epochs'],\n",
    "            min_epochs=cfg_spd['epochs'],\n",
    "            callbacks=[bn_sched, es],\n",
    "            loss=torch.nn.CrossEntropyLoss(),\n",
    "            device=device, \n",
    "            dtype=cfg_spd['dtype']\n",
    "        )\n",
    "\n",
    "        # fit the model\n",
    "        print(f\"test domain={test_domain}\")\n",
    "        trainer.fit(net, train_dataloader=loader_train, val_dataloader=loader_val)\n",
    "\n",
    "        print(f'ES best epoch={es.best_epoch}')\n",
    "\n",
    "        fit_df = pd.DataFrame(trainer.records)\n",
    "        fit_df['fold'] = ix_fold\n",
    "        fit_df['subset'] = ix_subset\n",
    "        fit_records.append(fit_df)\n",
    "\n",
    "        # evaluation\n",
    "        print(\"evaluate the estimator\")\n",
    "        res = trainer.test(net, dataloader=loader_train)\n",
    "        records.append(dict(mode='train', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_val)\n",
    "        records.append(dict(mode='validation', domain=test_domain, **res))\n",
    "        res = trainer.test(net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(noUDA)', domain=test_domain, **res))\n",
    "\n",
    "\n",
    "        y_pred = trainer.pred(net,dataloader=loader_test)\n",
    "        labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "            y_pred[0].cpu(), codes, min_len=30, sfreq=60, consecutive=50, window_size=0.25\n",
    "        )\n",
    "        accuracy_code = np.round(balanced_accuracy_score(labels_codes[ix_fold][labels_pred_accumul!=-1], labels_pred_accumul[labels_pred_accumul!=-1]), 2)\n",
    "        print(\" accuracy score of the participant\",accuracy_code)\n",
    "        records.append(dict(mode='test(noUDA)_code', domain=test_domain, score=accuracy_code, loss=None))\n",
    "\n",
    "\n",
    "        # extract model parameters\n",
    "        state_dict = deepcopy(net.state_dict())\n",
    "\n",
    "        # create new model and perform offline SF UDA\n",
    "        print(\"Perform UDA offline\")\n",
    "        sfuda_offline_net = SPDSMNet(**mdl_kwargs).to(device=device)\n",
    "        sfuda_offline_net.load_state_dict(state_dict)\n",
    "        sfuda_offline(ds_test, sfuda_offline_net)\n",
    "        res = trainer.test(sfuda_offline_net, dataloader=loader_test)\n",
    "        records.append(dict(mode='test(SFUDA)', domain=test_domain, **res))\n",
    "\n",
    "        # create a new model and perform online SF UDA\n",
    "        # sfuda_online_net = SPDSMNet(**mdl_kwargs)\n",
    "        # sfuda_online_net.load_state_dict(state_dict)\n",
    "        # loss, score = sfuda_online(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        # records.append(dict(mode='test(onlineSFUDA)', domain=test_domain, loss=loss, score=score))\n",
    "\n",
    "        # # create a new model and perform online SF UDA\n",
    "        # sfuda_online_net = SPDSMNet(**mdl_kwargs)\n",
    "        # sfuda_online_net.load_state_dict(state_dict)\n",
    "        # loss, score = sfuda_online_simulate(ds_test, sfuda_online_net, cfg_spd, trainer.loss_fn)\n",
    "        # records.append(dict(mode='test(online_sim_SFUDA)', domain=test_domain, loss=loss, score=score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\TSMNet\\minimal_tsmnet.ipynb Cell 34\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# report the results\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m resdf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(records)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/TSMNet/minimal_tsmnet.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m resdf\u001b[39m.\u001b[39;49mgroupby([\u001b[39m'\u001b[39;49m\u001b[39mmode\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39magg([\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstd\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mround(\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:8402\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   8399\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to supply one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mby\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   8400\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8402\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   8403\u001b[0m     obj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   8404\u001b[0m     keys\u001b[39m=\u001b[39;49mby,\n\u001b[0;32m   8405\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   8406\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   8407\u001b[0m     as_index\u001b[39m=\u001b[39;49mas_index,\n\u001b[0;32m   8408\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m   8409\u001b[0m     group_keys\u001b[39m=\u001b[39;49mgroup_keys,\n\u001b[0;32m   8410\u001b[0m     squeeze\u001b[39m=\u001b[39;49msqueeze,\n\u001b[0;32m   8411\u001b[0m     observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[0;32m   8412\u001b[0m     dropna\u001b[39m=\u001b[39;49mdropna,\n\u001b[0;32m   8413\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:965\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[39mif\u001b[39;00m grouper \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgroupby\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgrouper\u001b[39;00m \u001b[39mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 965\u001b[0m     grouper, exclusions, obj \u001b[39m=\u001b[39m get_grouper(\n\u001b[0;32m    966\u001b[0m         obj,\n\u001b[0;32m    967\u001b[0m         keys,\n\u001b[0;32m    968\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    969\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m    970\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    971\u001b[0m         observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[0;32m    972\u001b[0m         mutated\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmutated,\n\u001b[0;32m    973\u001b[0m         dropna\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropna,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    976\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m obj\n\u001b[0;32m    977\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:888\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    886\u001b[0m         in_axis, level, gpr \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, gpr, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    889\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(gpr, Grouper) \u001b[39mand\u001b[39;00m gpr\u001b[39m.\u001b[39mkey \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[39m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     exclusions\u001b[39m.\u001b[39madd(gpr\u001b[39m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'mode'"
     ]
    }
   ],
   "source": [
    "# report the results\n",
    "resdf = pd.DataFrame(records)\n",
    "\n",
    "resdf.groupby(['mode']).agg(['mean', 'std']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf.to_csv(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/SPDSMNet_code.csv\")\n",
    "# resdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/SPDSMNet_code.csv\")\n",
    "print()\n",
    "df_score_code = np.array(df[\"score\"])[df[\"mode\"]==\"test(noUDA)_code\"]\n",
    "df_score = np.array(df[\"score\"])[df[\"mode\"]==\"test(noUDA)\"]\n",
    "\n",
    "np.save(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/SPDSMNet_score_code\",df_score_code)\n",
    "np.save(\"C:/Users/s.velut/Documents/These/Protheus_PHD/results/results/Score_TF/TSMNet/SPDSMNet_score\",df_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x23432e9ee10>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_df = pd.concat(fit_records)\n",
    "fit_df = fit_df.groupby(['subset', 'fold', 'epoch']).mean()\n",
    "fit_df.columns = fit_df.columns.str.split('_', expand=True)\n",
    "fit_df.columns.names = ['set', 'metric']\n",
    "fit_df = fit_df.stack('set').reset_index()\n",
    "\n",
    "\n",
    "sns.relplot(data=fit_df, x='epoch', y='loss', hue='set', col='subset', col_wrap=5, kind='line', n_boot=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
