import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split

# Define your neural network model
class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, personalized_param):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.fc_personalized = nn.Linear(input_size, personalized_param)  # Personalized parameter layer
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc_personalized(x)  # Using personalized parameter layer
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

# Define your dataset
X = ...  # Your data
y = ...  # Your labels
input_size = X.shape[1]
output_size = len(torch.unique(y))  # Number of classes
hidden_size = 64  # Number of neurons in the hidden layer
personalized_param = 32  # Number of neurons in the personalized parameter layer

# Split data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Convert data into PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.long)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# Create DataLoader for train, validation, and test sets
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# Instantiate your model
model = SimpleNet(input_size, hidden_size, output_size, personalized_param)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train the model
num_epochs = 10
for epoch in range(num_epochs):
    running_loss = 0.0
    for inputs, labels in train_dataloader:
        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch {epoch+1}, Loss: {running_loss / len(train_dataloader)}")

print("Training finished!")

# Validation
model.eval()
val_correct = 0
with torch.no_grad():
    for inputs, labels in val_dataloader:
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)
        val_correct += (predicted == labels).sum().item()

val_accuracy = val_correct / len(X_val)
print(f"Validation Accuracy: {val_accuracy}")

# Testing
test_correct = 0
with torch.no_grad():
    for inputs, labels in test_dataloader:
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)
        test_correct += (predicted == labels).sum().item()

test_accuracy = test_correct / len(X_test)
print(f"Test Accuracy: {test_accuracy}")
