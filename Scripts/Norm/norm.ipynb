{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.15.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moabb\\pipelines\\__init__.py:26: ModuleNotFoundError: Tensorflow is not installed. You won't be able to use these MOABB pipelines if you attempt to do so.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from EEG2CodeKeras import (basearchi, basearchi_patchembedding,\n",
    "                           basearchi_patchembeddingdilation, vanilliaEEG2Code,\n",
    "                           trueVanilliaEEG2Code)\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import os\n",
    "import mne\n",
    "from collections import OrderedDict\n",
    "from itertools import product\n",
    "from keras.layers import Dense, Conv2D\n",
    "from numba import cuda\n",
    "\n",
    "from _utils import make_preds_accumul_aggresive, train_split\n",
    "sys.path.insert(0,\"C:\\\\Users\\\\s.velut\\\\Documents\\\\These\\\\Protheus_PHD\\\\Scripts\")\n",
    "from utils import prepare_data,get_BVEP_data,balance,get_y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices())\n",
    "# path = \"D:/SET/SET/\"\n",
    "path = \"~/Documents/These/Protheus_PHD/Class4\"\n",
    "n_cal_pretrain = 7\n",
    "n_cal_finetuning = 5\n",
    "learning_rate_pretrain = 1e-5\n",
    "learning_rate_finetuning = 1e-3\n",
    "batchsize = 64\n",
    "# pre_train_list = np.array([1, 3, 5])\n",
    "pre_train_list = np.array([1])\n",
    "# , vanilliaEEG2Code2, basearchi_patchembeddingdilation\n",
    "# models = {\"BaseArchi\": basearchi,\n",
    "#           \"PatchEmbedding\": basearchi_patchembedding,\n",
    "#           \"EEG2Code\": vanilliaEEG2Code,\n",
    "#           \"TrueEEG2Code\": trueVanilliaEEG2Code}\n",
    "n_class = 4\n",
    "window_size = 0.25\n",
    "fps = 60\n",
    "pretraining_epochs = 20\n",
    "finetuning_epochs = 20\n",
    "\n",
    "min_len_possibilities = [5, 10, 20, 30, 40]\n",
    "# consecutive_possibilities = [15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70]\n",
    "consecutive_possibilities = [10, 20, 30, 40, 50, 60]\n",
    "LEARNING_RATE = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    raw = mne.io.read_raw_eeglab(filepath, preload=True, verbose=False)\n",
    "    # Average re-referencing\n",
    "    # keep = [\"O1\", \"O2\", \"Oz\", \"P7\", \"P3\", \"P4\", \"P8\", \"Pz\"]\n",
    "    # raw = raw.drop_channels([i for i in raw.ch_names if i not in keep])\n",
    "\n",
    "    # to_drop = ['10', '21', 'ACC_X', 'ACC_Y', 'ACC_Z']\n",
    "    to_drop = [\"P9\", \"P10\", \"TP9\", \"TP10\", \"10\", \"21\", 'ACC_X', 'ACC_Y', 'ACC_Z']\n",
    "    raw = raw.drop_channels([ch for ch in raw.ch_names if ch in to_drop])\n",
    "\n",
    "    raw = raw.filter(l_freq=50.1, h_freq=49.9, method=\"iir\", verbose=False)\n",
    "    mne.set_eeg_reference(raw, 'average', copy=False, verbose=False)\n",
    "    # raw = raw.filter(l_freq=5, h_freq=45, method=\"fir\", verbose=False)\n",
    "    n_channels = len(raw.ch_names)\n",
    "    # print(\"Channels :\", n_channels)\n",
    "\n",
    "    # Strip the annotations that were script to make them easier to process\n",
    "    events, event_id = mne.events_from_annotations(\n",
    "        raw, event_id='auto', verbose=False)\n",
    "    to_remove = []\n",
    "    for idx in range(len(raw.annotations.description)):\n",
    "        if (('collects' in raw.annotations.description[idx]) or\n",
    "            ('iti' in raw.annotations.description[idx]) or\n",
    "                (raw.annotations.description[idx] == '[]')):\n",
    "            to_remove.append(idx)\n",
    "        else:\n",
    "            code = raw.annotations.description[idx].split('_')[0]\n",
    "            lab = raw.annotations.description[idx].split('_')[1]\n",
    "            code = code.replace('\\n', '')\n",
    "            code = code.replace('[', '')\n",
    "            code = code.replace(']', '')\n",
    "            code = code.replace(' ', '')\n",
    "            raw.annotations.description[idx] = code + '_' + lab\n",
    "\n",
    "    to_remove = np.array(to_remove)\n",
    "    if len(to_remove) > 0:\n",
    "        raw.annotations.delete(to_remove)\n",
    "    # Get the events\n",
    "    events, event_id = mne.events_from_annotations(\n",
    "        raw, event_id='auto', verbose=False)\n",
    "    shift = 0.0\n",
    "    # Epoch the data following event\n",
    "    epochs = mne.Epochs(raw, events, event_id=event_id, tmin=shift,\n",
    "                        tmax=2.2+shift, baseline=(None, None), preload=False, verbose=False)\n",
    "    labels = epochs.events[..., -1]\n",
    "    labels -= np.min(labels)\n",
    "    data = epochs.get_data()\n",
    "\n",
    "    codes = OrderedDict()\n",
    "    for k, v in event_id.items():\n",
    "        code = k.split('_')[0]\n",
    "        code = code.replace('.', '').replace('2', '')\n",
    "        idx = k.split('_')[1]\n",
    "        codes[v-1] = np.array(list(map(int, code)))\n",
    "\n",
    "    return data, labels, codes, int(epochs.info['sfreq']), n_channels\n",
    "\n",
    "\n",
    "def test_split(data, labels, n_class, n_training_cal):\n",
    "    data_train = data[:n_class*n_training_cal]\n",
    "    labels_train = labels[:n_class*n_training_cal]\n",
    "    data_test = data[n_class*n_training_cal:]\n",
    "    labels_test = labels[n_class*n_training_cal:]\n",
    "\n",
    "    return data_train, labels_train, data_test, labels_test\n",
    "\n",
    "\n",
    "def to_window(data, labels, n_channels, w_size, codes, sfreq, fps):\n",
    "    length = int((2.2-w_size)*sfreq)\n",
    "    n_samples_windows = int(w_size*sfreq)\n",
    "    X = np.empty(shape=((length)*data.shape[0], n_channels, n_samples_windows))\n",
    "    Y = np.empty(shape=((length)*data.shape[0]), dtype=int)\n",
    "    count = 0\n",
    "    for trial_nb, trial in enumerate(data):\n",
    "        lab = labels[trial_nb]\n",
    "        c = codes[lab]\n",
    "        code_pos = 0\n",
    "        for idx in range(length):\n",
    "            X[count] = trial[:, idx:idx+n_samples_windows]\n",
    "            if idx/sfreq >= (code_pos+1)/fps:\n",
    "                code_pos += 1\n",
    "            Y[count] = int(c[code_pos])\n",
    "            count += 1\n",
    "\n",
    "    X = np.expand_dims(X, 1)\n",
    "    X = X.astype(np.float32)\n",
    "    Y = np.vstack((Y, np.abs(1-Y))).T\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def code2array(code):\n",
    "    tmp = []\n",
    "    for idx, c in enumerate(code[:-2]):\n",
    "        if c == '5' or c == '.':\n",
    "            continue\n",
    "        elif c == '0':\n",
    "            if code[idx+2] == '5':\n",
    "                tmp.append(0.5)\n",
    "            else:\n",
    "                tmp.append(0)\n",
    "        else:\n",
    "            tmp.append(1)\n",
    "    if code[-1] == '.':\n",
    "        if code[-2] == '0':\n",
    "            tmp.append(0)\n",
    "        else:\n",
    "            tmp.append(1)\n",
    "    return np.array(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:277: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    InputLayer,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    BatchNormalization,\n",
    "    Normalization,\n",
    "    Dropout,\n",
    "    LeakyReLU\n",
    ")\n",
    "\n",
    "\n",
    "def base_norm(windows_size, n_channel_input, normlayer=None):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(1, n_channel_input, windows_size)))\n",
    "    if normlayer is not None:\n",
    "        model.add(normlayer)\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            16,\n",
    "            kernel_size=(n_channel_input, 1),\n",
    "            padding=\"valid\",\n",
    "            strides=(1, 1),\n",
    "            data_format=\"channels_first\",\n",
    "            kernel_initializer=\"he_uniform\",\n",
    "            activation=None,\n",
    "        )\n",
    "    )\n",
    "    model.add(BatchNormalization(axis=1, scale=True, center=False))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),\n",
    "              padding=\"same\", data_format=\"channels_first\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            8,\n",
    "            kernel_size=(1, 32),\n",
    "            dilation_rate=(1, 2),\n",
    "            data_format=\"channels_first\",\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=\"he_uniform\",\n",
    "            activation=None,\n",
    "        )\n",
    "    )\n",
    "    model.add(BatchNormalization(axis=1, scale=True, center=False))\n",
    "    model.add(LeakyReLU(alpha=0.3))\n",
    "    model.add(MaxPooling2D(pool_size=(1, 2), strides=(1, 2), padding=\"same\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            4,\n",
    "            kernel_size=(5, 5),\n",
    "            dilation_rate=(2, 2),\n",
    "            data_format=\"channels_first\",\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=\"he_uniform\",\n",
    "            activation=None,\n",
    "        )\n",
    "    )\n",
    "    model.add(BatchNormalization(axis=1, scale=True, center=False))\n",
    "    model.add(LeakyReLU(alpha=0.3))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),\n",
    "              data_format=\"channels_first\", padding=\"same\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(int(256), activation=None))\n",
    "    model.add(LeakyReLU(alpha=0.3))\n",
    "    model.add(Dense(2, name=\"preds\", activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preds_accumul_aggresive(y_pred, codes, min_len=30, sfreq=500, consecutive=30, window_size=0.25):\n",
    "    length = int((2.2-window_size)*sfreq)\n",
    "    y_pred = np.array(y_pred)\n",
    "    rez_acc = []\n",
    "\n",
    "    code_buffer = []\n",
    "    labels_pred = []\n",
    "    code_pos = 0\n",
    "    y_tmp = [] \n",
    "    mean_long = []\n",
    "\n",
    "\n",
    "    for trial in range(int(len(y_pred)/length)):   \n",
    "        # Retrieve a trial\n",
    "        tmp_code = y_pred[trial*length:(trial+1)*length]\n",
    "        code_pos = 0\n",
    "\n",
    "        # Do an average over the prdata, codes, labels, sfreq\n",
    "        code_buffer = []\n",
    "        for idx in range(len(tmp_code)):\n",
    "            y_tmp.append(tmp_code[idx])\n",
    "            if idx/sfreq >= (code_pos+1)/60:\n",
    "                code_pred = np.mean(y_tmp) \n",
    "                code_pred = int(np.rint(code_pred))\n",
    "                code_buffer.append(code_pred) \n",
    "                y_tmp = []\n",
    "                code_pos += 1\n",
    "        # Find the code that correlate the most\n",
    "        pred_lab = -1\n",
    "        out = 0\n",
    "        for long in np.arange(min_len, len(code_buffer) -1 , step=1):\n",
    "            dtw_values = []\n",
    "            for key, values in codes.items():\n",
    "                dtw_values.append(np.corrcoef(code_buffer[:long], values[:long])[0,1])\n",
    "            dtw_values = np.array(dtw_values)\n",
    "            max_dtw = list(codes.keys())[np.argmax(dtw_values)] \n",
    "            if (max_dtw == pred_lab):\n",
    "                out += 1\n",
    "            else:\n",
    "                pred_lab = max_dtw\n",
    "                out = 0\n",
    "            if out == consecutive:\n",
    "                mean_long.append((long)/60)\n",
    "                break\n",
    "        labels_pred.append(pred_lab)\n",
    "    labels_pred = np.array(labels_pred)\n",
    "    return labels_pred, code_buffer, mean_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sklearn.metrics\n",
    "from itertools import chain\n",
    "\n",
    "def basearchi_StdFromXTrain(parti, X_train, Y_train, X_test, Y_test, n_samples_windows, n_channels, labels_test, codes, sfreq, ctype):\n",
    "    X_std = X_train.std(axis=0)\n",
    "    X_train_final = X_train /(X_std + 1e-8)\n",
    "    X_test_final = X_test / (X_std + 1e-8)\n",
    "    clf = basearchi(windows_size=n_samples_windows, n_channel_input=n_channels)\n",
    "    \n",
    "    return predict(parti, \"BaseArchi_XTestNormByXTrain\", clf, X_train_final, Y_train, X_test_final, Y_test, labels_test, codes, sfreq, ctype)\n",
    "\n",
    "def basearchi_StdFromXTest(parti, X_train, Y_train, X_test, Y_test, n_samples_windows, n_channels, labels_test, codes, sfreq, ctype):\n",
    "    X_std = X_train.std(axis=0)\n",
    "    X_train_final = X_train /(X_std + 1e-8)\n",
    "    X_std = X_test.std(axis=0)\n",
    "    X_test_final = X_test / (X_std + 1e-8)\n",
    "    clf = basearchi(windows_size=n_samples_windows, n_channel_input=n_channels)\n",
    "\n",
    "    return predict(parti, \"BaseArchi_XTestNormByXTest\", clf, X_train_final, Y_train, X_test_final, Y_test, labels_test, codes, sfreq, ctype)\n",
    "\n",
    "def basearchi_GlobalNorm(parti, X_train, Y_train, X_test, Y_test, n_samples_windows, n_channels, labels_test, codes, sfreq, ctype):\n",
    "    normlayer = Normalization(axis=None)\n",
    "    normlayer.adapt(X_train)\n",
    "    clf = base_norm(windows_size=n_samples_windows,\n",
    "                    n_channel_input=n_channels, normlayer=normlayer)\n",
    "\n",
    "    return predict(parti, \"Global Norm\", clf, X_train, Y_train, X_test, Y_test, labels_test, codes, sfreq, ctype)\n",
    "\n",
    "def basearchi_SampleNorm(parti, X_train, Y_train, X_test, Y_test, n_samples_windows, n_channels, labels_test, codes, sfreq, ctype):\n",
    "    normlayer = Normalization(axis=0)\n",
    "    normlayer.adapt(X_train)\n",
    "    clf = base_norm(windows_size=n_samples_windows,\n",
    "                    n_channel_input=n_channels, normlayer=normlayer)\n",
    "\n",
    "    return predict(parti, \"Sample Norm\", clf, X_train, Y_train, X_test, Y_test, labels_test, codes, sfreq, ctype)\n",
    "\n",
    "def basearchi_Norm_ElectrodeNorm(parti, X_train, Y_train, X_test, Y_test, domains_train, nb_seconds,\n",
    "                             n_samples_windows, n_channels, labels_test, codes, sfreq, ctype):\n",
    "    \n",
    "    for d in np.unique(domains_train):\n",
    "        normlayer = Normalization(axis=-2)\n",
    "        normlayer.adapt(X_train[domains_train==d])\n",
    "        X_train[domains_train==d] = normlayer(X_train[domains_train==d])\n",
    "    \n",
    "    normlayer = Normalization(axis=-2)\n",
    "    normlayer.adapt(X_test[:nb_seconds*60])\n",
    "    X_test = normlayer(X_test)\n",
    "\n",
    "    clf = base_norm(windows_size=n_samples_windows,\n",
    "                    n_channel_input=n_channels, normlayer=None)\n",
    "\n",
    "    return predict(parti, \"Normed Electrode Norm\", clf, X_train, Y_train, X_test, Y_test, labels_test, codes, sfreq, ctype)\n",
    "\n",
    "def basearchi_ElectrodeNorm(parti, X_train, Y_train, X_test, Y_test, n_samples_windows, n_channels, labels_test, codes, sfreq, ctype):\n",
    "    normlayer = Normalization(axis=-2)\n",
    "    normlayer.adapt(X_train)\n",
    "    clf = base_norm(windows_size=n_samples_windows,\n",
    "                    n_channel_input=n_channels, normlayer=normlayer)\n",
    "\n",
    "    return predict(parti, \"Electrode Norm\", clf, X_train, Y_train, X_test, Y_test, labels_test, codes, sfreq, ctype)\n",
    "\n",
    "def basearchi_TemporalNorm(parti, X_train, Y_train, X_test, Y_test, n_samples_windows, n_channels, labels_test, codes, sfreq, ctype):\n",
    "    normlayer = Normalization(axis=-1)\n",
    "    normlayer.adapt(X_train)\n",
    "    clf = base_norm(windows_size=n_samples_windows,\n",
    "                    n_channel_input=n_channels, normlayer=normlayer)\n",
    "\n",
    "    return predict(parti, \"Temporal Norm\",clf, X_train, Y_train, X_test, Y_test, labels_test, codes, sfreq, ctype)\n",
    "\n",
    "\n",
    "def predict(parti, model_str, model, X_train, Y_train, X_test, Y_test, labels_test, codes, sfreq, ctype):\n",
    "    X_train_tr, X_val, Y_train_tr, Y_val = train_test_split(\n",
    "        X_train, Y_train, test_size=0.1, random_state=42, shuffle=True)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE, amsgrad=True)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    training_time = time.time()\n",
    "    print(\"    * Training model\")\n",
    "    print(np.array(X_train_tr.shape))\n",
    "    print(Y_train_tr.shape)\n",
    "    history = model.fit(\n",
    "        np.array(X_train_tr),\n",
    "        Y_train_tr,\n",
    "        batch_size=128,\n",
    "        epochs=35,\n",
    "        validation_data=(np.array(X_val), Y_val),\n",
    "        verbose=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    training_time = time.time()-training_time\n",
    "\n",
    "    print(\"    * Prediciting model\")\n",
    "    y_pred = model.predict(X_test, batch_size=24, verbose=0)[:, 0]\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred_norm = np.array([1 if (y >= 0.5) else 0 for y in y_pred])\n",
    "    y_test_norm = np.array([0 if y[0] == 0 else 1 for y in Y_test])\n",
    "    tn, fp, fn, tp = sklearn.metrics.confusion_matrix(\n",
    "        y_test_norm, y_pred_norm).ravel()\n",
    "    test_acc = (tp+tn)/len(y_test_norm)\n",
    "    test_sensitivity = tp/(tp+fn)\n",
    "    test_precision = tp/(tp+fp)\n",
    "    test_fowlkes_mallows = math.sqrt(test_sensitivity*test_precision)\n",
    "\n",
    "    acc_best = 0\n",
    "    acc_best_c = 0\n",
    "    acc_best_m = 0\n",
    "    itr_best = 0\n",
    "    itr_best_c = 0\n",
    "    itr_best_m = 0\n",
    "    itr_best_acc = 0\n",
    "    file_name = f\"./results/test_norm/user{parti}_{model_str}_{ctype}.csv\"\n",
    "    class_results = pd.DataFrame(columns=list(chain(*map(lambda i: [\"Acc\"+i, \"Time\"+i, \"ITR\"+i], map(str, min_len_possibilities)))))\n",
    "    \n",
    "    print(\"    * Computing windows scores\")\n",
    "    for c in consecutive_possibilities:\n",
    "        temp_res = []\n",
    "        for m in min_len_possibilities:\n",
    "            labels_pred_accumul, _, mean_long_accumul = make_preds_accumul_aggresive(\n",
    "                y_pred_norm, codes, min_len=m, sfreq=sfreq, consecutive=c, window_size=window_size\n",
    "            )\n",
    "            accuracy = round(accuracy_score(\n",
    "                labels_test[labels_pred_accumul != -1], labels_pred_accumul[labels_pred_accumul != -1]), 2)\n",
    "            if accuracy == 0:\n",
    "                itr = 0\n",
    "            elif accuracy == 1.0:\n",
    "                itr = (math.log2(n_class)+accuracy*math.log2(accuracy))*60/np.mean(mean_long_accumul)\n",
    "            else: \n",
    "                itr = round((math.log2(n_class)+accuracy*math.log2(accuracy)+(1-accuracy)*math.log2((1-accuracy)/(n_class-1)))*60/np.mean(mean_long_accumul), 0)\n",
    "            if accuracy > acc_best:\n",
    "                acc_best_m = m\n",
    "                acc_best_c = c\n",
    "                acc_best = accuracy\n",
    "                acc_best_itr = itr\n",
    "\n",
    "            if itr > itr_best:\n",
    "                itr_best_m = m\n",
    "                itr_best_c = c\n",
    "                itr_best_acc = accuracy\n",
    "                itr_best = itr\n",
    "            temp_res.extend([accuracy, np.mean(mean_long_accumul), itr])\n",
    "        class_results.loc[len(class_results)] = temp_res\n",
    "        class_results = class_results.rename(index={class_results.index[-1]: str()})\n",
    "\n",
    "    class_results.to_csv(file_name)\n",
    "    del class_results, model, temp_res\n",
    "    keras.backend.clear_session()\n",
    "    keras.backend.clear_session()\n",
    "    keras.backend.clear_session()\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    return [parti, ctype, model_str, history.history[\"accuracy\"][-1], history.history[\"val_accuracy\"][-1],\n",
    "            training_time, tp, fp, tn, fn, test_acc, test_sensitivity, test_precision,\n",
    "            test_fowlkes_mallows, itr_best_m, itr_best_c, itr_best_acc, itr_best, acc_best_m,\n",
    "            acc_best_c, acc_best, acc_best_itr, file_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on normalise train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "================= TYPE burst100 =================\n",
      "==================================================\n",
      "================= USER 1 =================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      " * BaseArchi_XTestNormByXTrain\n",
      "WARNING:tensorflow:From c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "    * Training model\n",
      "[8416    1   30  125]\n",
      "(8416, 2)\n",
      "WARNING:tensorflow:From c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\Norm\\norm.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m n_channels \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m * BaseArchi_XTestNormByXTrain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m results\u001b[39m.\u001b[39mloc[\u001b[39mlen\u001b[39m(results)] \u001b[39m=\u001b[39m basearchi_StdFromXTrain(sub, X_train_ru, Y_train_ru, X_test, Y_test, n_samples_windows, n_channels, labels_test, codes, sfreq, ctype)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m results\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39m./results/final_results.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m * BaseArchi_XTestNormByXTest\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\Norm\\norm.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m X_test_final \u001b[39m=\u001b[39m X_test \u001b[39m/\u001b[39m (X_std \u001b[39m+\u001b[39m \u001b[39m1e-8\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m clf \u001b[39m=\u001b[39m basearchi(windows_size\u001b[39m=\u001b[39mn_samples_windows, n_channel_input\u001b[39m=\u001b[39mn_channels)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m predict(parti, \u001b[39m\"\u001b[39;49m\u001b[39mBaseArchi_XTestNormByXTrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, clf, X_train_final, Y_train, X_test_final, Y_test, labels_test, codes, sfreq, ctype)\n",
      "\u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\Norm\\norm.ipynb Cell 8\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39marray(X_train_tr\u001b[39m.\u001b[39mshape))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mprint\u001b[39m(Y_train_tr\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     np\u001b[39m.\u001b[39;49marray(X_train_tr),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     Y_train_tr,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m35\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(np\u001b[39m.\u001b[39;49marray(X_val), Y_val),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m training_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mtraining_time\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X10sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m    * Prediciting model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1808\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    869\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[0;32m    870\u001b[0m   )\n\u001b[0;32m    871\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_preflattened\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_flat(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    217\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    253\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "PARTICIPANT = \"Participant\"\n",
    "CODE_TYPE = \"CodeType\"\n",
    "MODEL_TYPE = \"ModelType\"\n",
    "CNN_TRAINING_ACC = \"CnnTrainingAccuracy\"\n",
    "CNN_TRAINING_VALACC = \"CnnTrainingVALAccuracy\"\n",
    "CNN_TRAINING_TIME = \"CnnTrainingTime\"\n",
    "CNN_TRUE_POSITIVES = \"CnnTestTruePositive\"\n",
    "CNN_FALSE_POSITIVES = \"CnnTestFalsePositive\"\n",
    "CNN_TRUE_NEGATIVES = \"CnnTestTrueNegatives\"\n",
    "CNN_FALSE_NEGATIVES = \"CnnTestFalseNegatives\"\n",
    "CNN_ACCURACY = \"CnnTestAccuracy\"\n",
    "CNN_SENSITIVITY = \"CnnTestSensitivity\"\n",
    "CNN_PRECISION = \"CnnTestPrecision\"\n",
    "CNN_FOWLKES_MALLOWS = \"CnnTestFowlkesMallows\"\n",
    "CLASS_RESULTS_FILE = \"ClassWindowResultFile\"\n",
    "CLASS_RESULTS_BEST_MIN_LEN_BY_ITR = \"ClassWindowResultBestMinLenByITR\"\n",
    "CLASS_RESULTS_BEST_CONSECUTIVE_BY_ITR = \"ClassWindowResultBestConsecutiveByITR\"\n",
    "CLASS_RESULTS_BEST_ACC_BY_ITR = \"ClassWindowResultBestAccByITR\"\n",
    "CLASS_RESULTS_BEST_ITR_BY_ITR = \"ClassWindowResultBestITRByITR\"\n",
    "CLASS_RESULTS_BEST_MIN_LEN_BY_ACC = \"ClassWindowResultBestMinLenByAccuracy\"\n",
    "CLASS_RESULTS_BEST_CONSECUTIVE_BY_ACC = \"ClassWindowResultBestConsecutiveByAccuracy\"\n",
    "CLASS_RESULTS_BEST_ACC_BY_ACC = \"ClassWindowResultBestAccByAccuracy\"\n",
    "CLASS_RESULTS_BEST_ITR_BY_ACC = \"ClassWindowResultBestITRByAccuracy\"\n",
    "\n",
    "results = pd.DataFrame(columns=[PARTICIPANT, CODE_TYPE, MODEL_TYPE, CNN_TRAINING_ACC, CNN_TRAINING_VALACC, CNN_TRAINING_TIME,\n",
    "                                CNN_TRUE_POSITIVES, CNN_FALSE_POSITIVES, CNN_TRUE_NEGATIVES,\n",
    "                                CNN_FALSE_NEGATIVES, CNN_ACCURACY, CNN_SENSITIVITY, CNN_PRECISION,\n",
    "                                CNN_FOWLKES_MALLOWS, CLASS_RESULTS_BEST_MIN_LEN_BY_ITR,\n",
    "                                CLASS_RESULTS_BEST_CONSECUTIVE_BY_ITR, CLASS_RESULTS_BEST_ACC_BY_ITR,\n",
    "                                CLASS_RESULTS_BEST_ITR_BY_ITR, CLASS_RESULTS_BEST_MIN_LEN_BY_ACC,\n",
    "                                CLASS_RESULTS_BEST_CONSECUTIVE_BY_ACC, CLASS_RESULTS_BEST_ACC_BY_ACC,\n",
    "                                CLASS_RESULTS_BEST_ITR_BY_ACC, CLASS_RESULTS_FILE])\n",
    "\n",
    "sfreq = 500\n",
    "\n",
    "#for ctype in [\"burst100\", \"burst40\", \"mseq100\", \"mseq40\"]:\n",
    "for ctype in [\"burst100\"]:\n",
    "    print(\"==================================================\")\n",
    "    print(\"================= TYPE\", ctype, \"=================\")\n",
    "    print(\"==================================================\")\n",
    "    for sub in range(1,14):\n",
    "        print(\"================= USER\", sub, \"=================\")\n",
    "        data, labels, codes, sfreq, n_channels = load_data(\n",
    "            os.path.join(path, f\"P{sub}\", f\"P{sub}_{ctype}.set\"))\n",
    "        data_train, labels_train, data_test, labels_test = test_split(\n",
    "            data, labels, n_class, n_cal_pretrain)\n",
    "        X_train, Y_train = to_window(\n",
    "            data_train, labels_train, n_channels, window_size, codes, sfreq, fps)\n",
    "        X_test, Y_test = to_window(data_test, labels_test,\n",
    "                                n_channels, window_size, codes, sfreq, fps)\n",
    "        rus = RandomUnderSampler()\n",
    "        counter = np.array(range(0, len(Y_train))).reshape(-1, 1)\n",
    "        index, _ = rus.fit_resample(counter, Y_train[:, 0])\n",
    "        X_train_ru = np.squeeze(X_train[index, :, :, :], axis=1)\n",
    "        Y_train_ru = np.squeeze(Y_train[index])\n",
    "\n",
    "        n_samples_windows = X_train.shape[-1]\n",
    "        n_channels = X_train.shape[-2]\n",
    "        print(\" * BaseArchi_XTestNormByXTrain\")\n",
    "        results.loc[len(results)] = basearchi_StdFromXTrain(sub, X_train_ru, Y_train_ru, X_test, Y_test, n_samples_windows, n_channels, labels_test, codes, sfreq, ctype)\n",
    "        results.to_csv(\"./results/final_results.csv\")\n",
    "        print(\" * BaseArchi_XTestNormByXTest\")\n",
    "        results.loc[len(results)] = basearchi_StdFromXTest(sub, X_train_ru, Y_train_ru, X_test, Y_test, n_samples_windows, n_channels, labels_test, codes, sfreq, ctype)\n",
    "        results.to_csv(\"./results/final_results.csv\")\n",
    "        print(\" * Global Norm\")\n",
    "        results.loc[len(results)] = basearchi_GlobalNorm(sub, X_train_ru, Y_train_ru, X_test, Y_test, n_samples_windows, n_channels, labels_test, codes, sfreq, ctype)\n",
    "        results.to_csv(\"./results/final_results.csv\")\n",
    "        print(\" * Electrode Norm\")\n",
    "        results.loc[len(results)] = basearchi_ElectrodeNorm(sub, X_train_ru, Y_train_ru, X_test, Y_test, n_samples_windows, n_channels, labels_test, codes, sfreq, ctype)\n",
    "        results.to_csv(\"./results/final_results.csv\")\n",
    "        print(\" * Normed Electrode Norm\")\n",
    "        results.loc[len(results)] = basearchi_Norm_ElectrodeNorm(sub, X_train_ru, Y_train_ru, X_test, Y_test, [\"train\"], 5,\n",
    "                                                                  n_samples_windows, n_channels, labels_test, codes, sfreq, ctype)\n",
    "        results.to_csv(\"./results/final_results.csv\")\n",
    "        print(\" * Temporal Norm\")\n",
    "        results.loc[len(results)] = basearchi_TemporalNorm(sub, X_train_ru, Y_train_ru,X_test, Y_test, n_samples_windows, n_channels, labels_test, codes, sfreq, ctype)\n",
    "        results.to_csv(\"./results/final_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalised by domain LOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 50 - 50 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandstop zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 49.90, 50.10 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Using data from preloaded Raw for 60 events and 1101 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "subjects = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "to_drop = [\"P9\", \"P10\", \"TP9\", \"TP10\", \"10\", \"21\", 'ACC_X', 'ACC_Y', 'ACC_Z']\n",
    "on_frame = False\n",
    "raw_data,nlabels,codes,labels_codes = get_BVEP_data(subjects,on_frame,to_drop)\n",
    "data, labels, domains = prepare_data(subjects,raw_data,nlabels,on_frame,False,codes)\n",
    "# data = []\n",
    "# labels = []\n",
    "# domains = []\n",
    "\n",
    "# for s in subjects:\n",
    "#     print(s)\n",
    "#     temp_data,temp_labels,temp_domains = prepare_data([s],raw_data,nlabels,on_frame,False,codes)\n",
    "#     data.append(temp_data[0])\n",
    "#     labels.append(temp_labels[0])\n",
    "#     domains.append(temp_domains[0])\n",
    "# domains = np.array(domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "================= TYPE burst100 =================\n",
      "==================================================\n",
      "================= USER 0 =================\n",
      " * Global Norm\n",
      "WARNING:tensorflow:From c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "WARNING:tensorflow:From c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\s.velut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Normed Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Temporal Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      "================= USER 1 =================\n",
      " * Global Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Normed Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Temporal Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      "================= USER 2 =================\n",
      " * Global Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Normed Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Temporal Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      "================= USER 3 =================\n",
      " * Global Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Normed Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Temporal Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      "================= USER 4 =================\n",
      " * Global Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Normed Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Temporal Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      "================= USER 5 =================\n",
      " * Global Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Normed Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Temporal Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      "================= USER 6 =================\n",
      " * Global Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Normed Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Temporal Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      "================= USER 7 =================\n",
      " * Global Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Normed Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Temporal Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      "================= USER 8 =================\n",
      " * Global Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Normed Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Temporal Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      "================= USER 9 =================\n",
      " * Global Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Normed Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Temporal Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      "================= USER 10 =================\n",
      " * Global Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Normed Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Temporal Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      "================= USER 11 =================\n",
      " * Global Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Normed Electrode Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n",
      " * Temporal Norm\n",
      "    * Training model\n",
      "[198396      1     30    125]\n",
      "(198396, 2)\n",
      "    * Prediciting model\n",
      "    * Computing windows scores\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "PARTICIPANT = \"Participant\"\n",
    "CODE_TYPE = \"CodeType\"\n",
    "MODEL_TYPE = \"ModelType\"\n",
    "CNN_TRAINING_ACC = \"CnnTrainingAccuracy\"\n",
    "CNN_TRAINING_VALACC = \"CnnTrainingVALAccuracy\"\n",
    "CNN_TRAINING_TIME = \"CnnTrainingTime\"\n",
    "CNN_TRUE_POSITIVES = \"CnnTestTruePositive\"\n",
    "CNN_FALSE_POSITIVES = \"CnnTestFalsePositive\"\n",
    "CNN_TRUE_NEGATIVES = \"CnnTestTrueNegatives\"\n",
    "CNN_FALSE_NEGATIVES = \"CnnTestFalseNegatives\"\n",
    "CNN_ACCURACY = \"CnnTestAccuracy\"\n",
    "CNN_SENSITIVITY = \"CnnTestSensitivity\"\n",
    "CNN_PRECISION = \"CnnTestPrecision\"\n",
    "CNN_FOWLKES_MALLOWS = \"CnnTestFowlkesMallows\"\n",
    "CLASS_RESULTS_FILE = \"ClassWindowResultFile\"\n",
    "CLASS_RESULTS_BEST_MIN_LEN_BY_ITR = \"ClassWindowResultBestMinLenByITR\"\n",
    "CLASS_RESULTS_BEST_CONSECUTIVE_BY_ITR = \"ClassWindowResultBestConsecutiveByITR\"\n",
    "CLASS_RESULTS_BEST_ACC_BY_ITR = \"ClassWindowResultBestAccByITR\"\n",
    "CLASS_RESULTS_BEST_ITR_BY_ITR = \"ClassWindowResultBestITRByITR\"\n",
    "CLASS_RESULTS_BEST_MIN_LEN_BY_ACC = \"ClassWindowResultBestMinLenByAccuracy\"\n",
    "CLASS_RESULTS_BEST_CONSECUTIVE_BY_ACC = \"ClassWindowResultBestConsecutiveByAccuracy\"\n",
    "CLASS_RESULTS_BEST_ACC_BY_ACC = \"ClassWindowResultBestAccByAccuracy\"\n",
    "CLASS_RESULTS_BEST_ITR_BY_ACC = \"ClassWindowResultBestITRByAccuracy\"\n",
    "\n",
    "results = pd.DataFrame(columns=[PARTICIPANT, CODE_TYPE, MODEL_TYPE, CNN_TRAINING_ACC, CNN_TRAINING_VALACC, CNN_TRAINING_TIME,\n",
    "                                CNN_TRUE_POSITIVES, CNN_FALSE_POSITIVES, CNN_TRUE_NEGATIVES,\n",
    "                                CNN_FALSE_NEGATIVES, CNN_ACCURACY, CNN_SENSITIVITY, CNN_PRECISION,\n",
    "                                CNN_FOWLKES_MALLOWS, CLASS_RESULTS_BEST_MIN_LEN_BY_ITR,\n",
    "                                CLASS_RESULTS_BEST_CONSECUTIVE_BY_ITR, CLASS_RESULTS_BEST_ACC_BY_ITR,\n",
    "                                CLASS_RESULTS_BEST_ITR_BY_ITR, CLASS_RESULTS_BEST_MIN_LEN_BY_ACC,\n",
    "                                CLASS_RESULTS_BEST_CONSECUTIVE_BY_ACC, CLASS_RESULTS_BEST_ACC_BY_ACC,\n",
    "                                CLASS_RESULTS_BEST_ITR_BY_ACC, CLASS_RESULTS_FILE])\n",
    "sfreq=500\n",
    "#for ctype in [\"burst100\", \"burst40\", \"mseq100\", \"mseq40\"]:\n",
    "for ctype in [\"burst100\"]:\n",
    "    print(\"==================================================\")\n",
    "    print(\"================= TYPE\", ctype, \"=================\")\n",
    "    print(\"==================================================\")\n",
    "    for sub in range(0,12):\n",
    "        print(\"================= USER\", sub, \"=================\")\n",
    "        ind2take = [j for j in range(12) if j!=sub]\n",
    "        # X_train = data[ind2take[0]]\n",
    "        # Y_train = labels[ind2take[0]]\n",
    "        # for k in ind2take[1:]:\n",
    "        #     X_train += data[k]\n",
    "        #     print(len(X_train))\n",
    "        #     Y_train += labels[k]\n",
    "        # print(len(Y_train))\n",
    "\n",
    "        # X_train = np.expand_dims(X_train,1)\n",
    "        X_train = np.expand_dims(np.concatenate(data[ind2take]).reshape(-1,data.shape[-2],data.shape[-1]),1)\n",
    "        Y_train = np.concatenate(labels[ind2take]).reshape(-1)\n",
    "        Y_train = np.vstack((Y_train,np.abs(1-Y_train))).T\n",
    "        domains_train = np.concatenate(domains[ind2take]).reshape(-1)\n",
    "        X_test = np.expand_dims(data[sub],1)\n",
    "        Y_test = np.vstack((labels[sub],np.abs(1-labels[sub]))).T\n",
    "        domains_test = domains[sub]\n",
    "        \n",
    "        X_train_ru, Y_train_ru,domain_train_ru = balance(X_train,Y_train,domains_train)\n",
    "        \n",
    "        n_samples_windows = X_train.shape[-1]\n",
    "        n_channels = X_train.shape[-2]\n",
    "        print(\" * Global Norm\")\n",
    "        results.loc[len(results)] = basearchi_GlobalNorm(sub, X_train_ru, Y_train_ru, X_test, Y_test, n_samples_windows, n_channels, labels_codes[sub], codes, sfreq, ctype)\n",
    "        results.to_csv(\"./results/moabb_final_results_sample.csv\")\n",
    "        print(\" * Electrode Norm\")\n",
    "        results.loc[len(results)] = basearchi_ElectrodeNorm(sub, X_train_ru, Y_train_ru, X_test, Y_test, n_samples_windows, n_channels, labels_codes[sub], codes, sfreq, ctype)\n",
    "        results.to_csv(\"./results/moabb_final_results_sample.csv\")\n",
    "        print(\" * Normed Electrode Norm\")\n",
    "        results.loc[len(results)] = basearchi_Norm_ElectrodeNorm(sub, X_train_ru, Y_train_ru, X_test, Y_test, domain_train_ru, 25,\n",
    "                                                                  n_samples_windows, n_channels, labels_codes[sub], codes, sfreq, ctype)\n",
    "        results.to_csv(\"./results/moabb_final_results_sample.csv\")\n",
    "        print(\" * Temporal Norm\")\n",
    "        results.loc[len(results)] = basearchi_StdFromXTest(sub, X_train_ru, Y_train_ru,X_test, Y_test, n_samples_windows, n_channels, labels_codes[sub], codes, sfreq, ctype)\n",
    "        results.to_csv(\"./results/moabb_final_results_sample.csv\")\n",
    "        # print(results)\n",
    "\n",
    "        del X_train\n",
    "        del X_test\n",
    "        del Y_train\n",
    "        del Y_test\n",
    "        del domains_train\n",
    "        del domains_test\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "================= TYPE burst100 =================\n",
      "==================================================\n",
      "================= USER 0 =================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\s.velut\\Documents\\These\\Protheus_PHD\\Scripts\\Norm\\norm.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m================= USER\u001b[39m\u001b[39m\"\u001b[39m, sub, \u001b[39m\"\u001b[39m\u001b[39m=================\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X13sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m ind2take \u001b[39m=\u001b[39m [j \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m12\u001b[39m) \u001b[39mif\u001b[39;00m j\u001b[39m!=\u001b[39msub]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X13sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m X_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(np\u001b[39m.\u001b[39mconcatenate(data[ind2take])\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,data\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m],data\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]),\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X13sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m Y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(labels[ind2take])\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/s.velut/Documents/These/Protheus_PHD/Scripts/Norm/norm.ipynb#X13sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m Y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack((Y_train,np\u001b[39m.\u001b[39mabs(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mY_train)))\u001b[39m.\u001b[39mT\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "PARTICIPANT = \"Participant\"\n",
    "CODE_TYPE = \"CodeType\"\n",
    "MODEL_TYPE = \"ModelType\"\n",
    "CNN_TRAINING_ACC = \"CnnTrainingAccuracy\"\n",
    "CNN_TRAINING_VALACC = \"CnnTrainingVALAccuracy\"\n",
    "CNN_TRAINING_TIME = \"CnnTrainingTime\"\n",
    "CNN_TRUE_POSITIVES = \"CnnTestTruePositive\"\n",
    "CNN_FALSE_POSITIVES = \"CnnTestFalsePositive\"\n",
    "CNN_TRUE_NEGATIVES = \"CnnTestTrueNegatives\"\n",
    "CNN_FALSE_NEGATIVES = \"CnnTestFalseNegatives\"\n",
    "CNN_ACCURACY = \"CnnTestAccuracy\"\n",
    "CNN_SENSITIVITY = \"CnnTestSensitivity\"\n",
    "CNN_PRECISION = \"CnnTestPrecision\"\n",
    "CNN_FOWLKES_MALLOWS = \"CnnTestFowlkesMallows\"\n",
    "CLASS_RESULTS_FILE = \"ClassWindowResultFile\"\n",
    "CLASS_RESULTS_BEST_MIN_LEN_BY_ITR = \"ClassWindowResultBestMinLenByITR\"\n",
    "CLASS_RESULTS_BEST_CONSECUTIVE_BY_ITR = \"ClassWindowResultBestConsecutiveByITR\"\n",
    "CLASS_RESULTS_BEST_ACC_BY_ITR = \"ClassWindowResultBestAccByITR\"\n",
    "CLASS_RESULTS_BEST_ITR_BY_ITR = \"ClassWindowResultBestITRByITR\"\n",
    "CLASS_RESULTS_BEST_MIN_LEN_BY_ACC = \"ClassWindowResultBestMinLenByAccuracy\"\n",
    "CLASS_RESULTS_BEST_CONSECUTIVE_BY_ACC = \"ClassWindowResultBestConsecutiveByAccuracy\"\n",
    "CLASS_RESULTS_BEST_ACC_BY_ACC = \"ClassWindowResultBestAccByAccuracy\"\n",
    "CLASS_RESULTS_BEST_ITR_BY_ACC = \"ClassWindowResultBestITRByAccuracy\"\n",
    "\n",
    "results = pd.DataFrame(columns=[PARTICIPANT, CODE_TYPE, MODEL_TYPE, CNN_TRAINING_ACC, CNN_TRAINING_VALACC, CNN_TRAINING_TIME,\n",
    "                                CNN_TRUE_POSITIVES, CNN_FALSE_POSITIVES, CNN_TRUE_NEGATIVES,\n",
    "                                CNN_FALSE_NEGATIVES, CNN_ACCURACY, CNN_SENSITIVITY, CNN_PRECISION,\n",
    "                                CNN_FOWLKES_MALLOWS, CLASS_RESULTS_BEST_MIN_LEN_BY_ITR,\n",
    "                                CLASS_RESULTS_BEST_CONSECUTIVE_BY_ITR, CLASS_RESULTS_BEST_ACC_BY_ITR,\n",
    "                                CLASS_RESULTS_BEST_ITR_BY_ITR, CLASS_RESULTS_BEST_MIN_LEN_BY_ACC,\n",
    "                                CLASS_RESULTS_BEST_CONSECUTIVE_BY_ACC, CLASS_RESULTS_BEST_ACC_BY_ACC,\n",
    "                                CLASS_RESULTS_BEST_ITR_BY_ACC, CLASS_RESULTS_FILE])\n",
    "sfreq=500\n",
    "#for ctype in [\"burst100\", \"burst40\", \"mseq100\", \"mseq40\"]:\n",
    "for ctype in [\"burst100\"]:\n",
    "    print(\"==================================================\")\n",
    "    print(\"================= TYPE\", ctype, \"=================\")\n",
    "    print(\"==================================================\")\n",
    "    for sub in range(0,12):\n",
    "        print(\"================= USER\", sub, \"=================\")\n",
    "        ind2take = [j for j in range(12) if j!=sub]\n",
    "        X_train = np.expand_dims(np.concatenate(data[ind2take]).reshape(-1,data.shape[-2],data.shape[-1]),1)\n",
    "        Y_train = np.concatenate(labels[ind2take]).reshape(-1)\n",
    "        Y_train = np.vstack((Y_train,np.abs(1-Y_train))).T\n",
    "        domains_train = np.concatenate(domains[ind2take]).reshape(-1)\n",
    "        X_test = np.expand_dims(data[sub],1)\n",
    "        Y_test = np.vstack((labels[sub],np.abs(1-labels[sub]))).T\n",
    "        domains_test = domains[sub]\n",
    "        \n",
    "        X_train_ru, Y_train_ru,domain_train_ru = balance(X_train,Y_train,domains_train)\n",
    "        \n",
    "        n_samples_windows = X_train.shape[-1]\n",
    "        n_channels = X_train.shape[-2]\n",
    "        print(\" * Global Norm\")\n",
    "        results.loc[len(results)] = basearchi_GlobalNorm(sub, X_train_ru, Y_train_ru, X_test, Y_test, n_samples_windows, n_channels, labels_codes[sub], codes, 60, ctype)\n",
    "        results.to_csv(\"./results/moabb_final_results_frame.csv\")\n",
    "        print(\" * Electrode Norm\")\n",
    "        results.loc[len(results)] = basearchi_ElectrodeNorm(sub, X_train_ru, Y_train_ru, X_test, Y_test, n_samples_windows, n_channels, labels_codes[sub], codes, 60, ctype)\n",
    "        results.to_csv(\"./results/moabb_final_results_frame.csv\")\n",
    "        print(\" * Normed Electrode Norm\")\n",
    "        results.loc[len(results)] = basearchi_Norm_ElectrodeNorm(sub, X_train_ru, Y_train_ru, X_test, Y_test, domain_train_ru, 5,\n",
    "                                                                  n_samples_windows, n_channels, labels_codes[sub], codes, 60, ctype)\n",
    "        results.to_csv(\"./results/moabb_final_results_frame.csv\")\n",
    "        print(\" * Temporal Norm\")\n",
    "        results.loc[len(results)] = basearchi_TemporalNorm(sub, X_train_ru, Y_train_ru,X_test, Y_test, n_samples_windows, n_channels, labels_codes[sub], codes, 60, ctype)\n",
    "        results.to_csv(\"./results/moabb_final_results_frame.csv\")\n",
    "        # print(results)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./results/moabb_final_results2.csv\")\n",
    "globres = df.loc[df[\"ModelType\"]==\"Global Norm\"][[\"Participant\",\"CnnTrainingAccuracy\",\"CnnTrainingVALAccuracy\",\"CnnTestAccuracy\",\"ClassWindowResultBestAccByAccuracy\"]]\n",
    "electres = df.loc[df[\"ModelType\"]==\"Electrode Norm\"][[\"Participant\",\"CnnTrainingAccuracy\",\"CnnTrainingVALAccuracy\",\"CnnTestAccuracy\",\"ClassWindowResultBestAccByAccuracy\"]]\n",
    "normelectres = df.loc[df[\"ModelType\"]==\"Normed Electrode Norm\"][[\"Participant\",\"CnnTrainingAccuracy\",\"CnnTrainingVALAccuracy\",\"CnnTestAccuracy\",\"ClassWindowResultBestAccByAccuracy\"]]\n",
    "tempres = df.loc[df[\"ModelType\"]==\"Temporal Norm\"][[\"Participant\",\"CnnTrainingAccuracy\",\"CnnTrainingVALAccuracy\",\"CnnTestAccuracy\",\"ClassWindowResultBestAccByAccuracy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8075000000000001\n",
      "0.8041666666666667\n",
      "0.8474999999999998\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(globres[\"ClassWindowResultBestAccByAccuracy\"]))\n",
    "print(np.mean(electres[\"ClassWindowResultBestAccByAccuracy\"]))\n",
    "print(np.mean(normelectres[\"ClassWindowResultBestAccByAccuracy\"]))\n",
    "print(np.mean(tempres[\"ClassWindowResultBestAccByAccuracy\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./results/moabb_final_results.csv\",sep=\";\")\n",
    "globres = df.loc[df[\"ModelType\"]==\"Global Norm\"][[\"Participant\",\"CnnTrainingAccuracy\",\"CnnTrainingVALAccuracy\",\"CnnTestAccuracy\",\"ClassWindowResultBestAccByAccuracy\"]]\n",
    "electres = df.loc[df[\"ModelType\"]==\"Electrode Norm\"][[\"Participant\",\"CnnTrainingAccuracy\",\"CnnTrainingVALAccuracy\",\"CnnTestAccuracy\",\"ClassWindowResultBestAccByAccuracy\"]]\n",
    "normelectres = df.loc[df[\"ModelType\"]==\"Normed Electrode Norm\"][[\"Participant\",\"CnnTrainingAccuracy\",\"CnnTrainingVALAccuracy\",\"CnnTestAccuracy\",\"ClassWindowResultBestAccByAccuracy\"]]\n",
    "tempres = df.loc[df[\"ModelType\"]==\"Temporal Norm\"][[\"Participant\",\"CnnTrainingAccuracy\",\"CnnTrainingVALAccuracy\",\"CnnTestAccuracy\",\"ClassWindowResultBestAccByAccuracy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8391666666666667\n",
      "0.8283333333333333\n",
      "0.8358333333333334\n",
      "0.8391666666666667\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(globres[\"ClassWindowResultBestAccByAccuracy\"]))\n",
    "print(np.mean(electres[\"ClassWindowResultBestAccByAccuracy\"]))\n",
    "print(np.mean(normelectres[\"ClassWindowResultBestAccByAccuracy\"]))\n",
    "print(np.mean(tempres[\"ClassWindowResultBestAccByAccuracy\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
